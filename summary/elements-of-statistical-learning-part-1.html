<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="author" content="Nitish Puri" />
    <meta name="robots" content="index, follow"/>

    <meta property="og:title" content="Elements Of Statistical Learning, Part 1"/>
    <meta property="og:url" content="/summary/elements-of-statistical-learning-part-1.html"/>
    <meta property="og:site_name" content="nitishpuri.github.io"/>
    <meta property="og:type" content="article"/>

    <link rel="canonical" href="/summary/elements-of-statistical-learning-part-1.html" />

    <title>Elements Of Statistical Learning, Part 1 | nitishpuri.github.io</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" />
    <link rel="stylesheet" type="text/css" href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.min.css" />

    <link rel="stylesheet" type="text/css" href="/theme/css/main.css" />

    <script type="text/javascript">var switchTo5x=true;</script>
    <script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
    <script type="text/javascript">
        stLight.options({
            publisher: "59baff2dc1263e001291a4b3",
            doNotHash: false,
            doNotCopy: false,
            hashAddressBar: false
        });
    </script>
</head>

<body id="index">
<a class="hidden-phone" href="nitishpuri">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="http://s3.amazonaws.com/github/ribbons/forkme_right_gray_6d6d6d.png" alt="Fork me on GitHub" />
</a>
    <div class="row-fluid">
        <div class="span10 offset1">
            <header id="banner" >
                <h1>
                    <a href="/">nitishpuri.github.io </a>
                </h1>
                <nav class="navbar">
                    <div class="navbar-inner">
                        <ul class="nav">
                            <li ><a href="/archives.html">Archives</a></li>
                            <li ><a href="/pages/bio.html">About</a></li>
                        </ul>

<script>
  (function() {
    var cx = '011138406956770016801:3ffzupagszg';
    var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
    gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') +
        '//www.google.com/cse/cse.js?cx=' + cx;
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
  })();
</script>
<div id="google-custom-search" class="nav">
    <gcse:search></gcse:search>
</div>
                    </div>
                </nav>
            </header><!-- /#banner -->
        </div>
    </div>

    <div class="row-fluid">
        <div class="span10 offset1">
            <div class="row-fluid">
<div class="span10 offset1">
  <section>
    <article>
      <header>
        <h1 class="entry-title">
          <a href="/summary/elements-of-statistical-learning-part-1.html" rel="bookmark"
             title="Permalink to Elements Of Statistical Learning, Part 1">Elements Of Statistical Learning, Part 1</a></h1>
      </header>
      <div class="entry-content">
<footer class="post-info">
    <address class="vcard author">
        by <a class="url fn" href="/author/trevor-hastie-robert-tibshirani-jerome-friedman.html">Trevor Hastie, Robert Tibshirani, Jerome Friedman</a>
    </address>

    in <a href="/category/summary.html">Summary</a>

    on 2017-08-09

        |
        tags:         <a href="/tag/data-science.html">data-science</a>
        <a href="/tag/notes.html">notes</a>


        |
        <a href="/summary/elements-of-statistical-learning-part-1.html#disqus_thread">comments</a>

    
</footer><!-- /.post-info -->
        <div class="sharethis-top">
          <span class='st_fblike_hcount' displayText='Facebook Like'></span>
          <span class='st_plusone_hcount' displayText='Google +1'></span>
          <span class='st_twitter_hcount' displayText='Tweet'></span>
          <span class='st_email_hcount' displayText='Email'></span>
        </div>

        <p>This post is part 1 of the "Elements Of Statistical Learning" series:</p>
        <ol class="parts">
                <li class="active">
                    <a href='/summary/elements-of-statistical-learning-part-1.html'>Elements Of Statistical Learning, Part 1</a>
                </li>
                <li >
                    <a href='/summary/elements-of-statistical-learning-part-2.html'>Elements Of Statistical Learning, Part 2</a>
                </li>
        </ol>
        <hr>

        <p>Contents:</p>
        <nav class="toc">
          <div id="toc"><ul><li><a class="toc-href" href="#chapter-1-introduction" title="Chapter 1: Introduction">Chapter 1: Introduction</a></li><li><a class="toc-href" href="#chapter-2-overview-of-supervised-learning" title="Chapter 2: Overview of Supervised Learning">Chapter 2: Overview of Supervised Learning</a></li><li><a class="toc-href" href="#chapter-3-linear-methods-of-regression" title="Chapter 3: Linear Methods Of Regression">Chapter 3: Linear Methods Of Regression</a></li><li><a class="toc-href" href="#chapter-4-linear-methods-of-classification" title="Chapter 4: Linear Methods of Classification">Chapter 4: Linear Methods of Classification</a></li><li><a class="toc-href" href="#chapter-5-basis-expansions-and-regularization" title="Chapter 5: Basis Expansions and Regularization">Chapter 5: Basis Expansions and Regularization</a></li><li><a class="toc-href" href="#chapter-6-kernel-smoothing-methods" title="Chapter 6: Kernel Smoothing Methods">Chapter 6: Kernel Smoothing Methods</a></li></ul></div>
        </nav>
        <hr>
    
        <h2 id="chapter-1-introduction">Chapter 1: Introduction</h2>
<ul>
<li>Motivation towards statistical learning and belief in data.</li>
<li>What's next.</li>
</ul>
<h2 id="chapter-2-overview-of-supervised-learning">Chapter 2: Overview of Supervised Learning</h2>
<ul>
<li>Variable types and terminology<ul>
<li>Quantitative vs Qualitative output.</li>
<li>Regression and Classification</li>
</ul>
</li>
<li>Simple approaches : Least Squares and Nearest Neighbors<ul>
<li>Linear Models and Least Squares <br/>
<span class="math">\(\hat Y = \hat \beta_0 + \sum_{j=1}^pX_j\hat\beta_j\)</span> <ul>
<li>Least squares by solving <em>normal</em> equations.</li>
</ul>
</li>
<li>Nearest Neighbor Methods  <ul>
<li><em>Voronoi tessellation</em></li>
</ul>
</li>
<li>From Least Squares to Nearest Neighbors</li>
</ul>
</li>
<li>Statistical Decision Theory       </li>
<li>Local Methods in High Dimensions<ul>
<li><strong>The curse of Dimensionality,<em>Bellman</em> </strong></li>
</ul>
</li>
<li>Statistical Models, Supervised Learning and Function Approximation<ul>
<li>A Statistical Model for the Joint Distribution Pr(X, Y )</li>
<li>Supervised Learning</li>
<li>Function Approximation</li>
</ul>
</li>
<li>Structured Regression Models<ul>
<li>Difficulty of the Problem</li>
</ul>
</li>
<li>Classes of Restricted Estimators<ul>
<li>Roughness Penalty and Bayesian Methods  <ul>
<li><em>regularization</em></li>
</ul>
</li>
<li>Kernel Methods and Local Regression</li>
<li>Basis Functions and Dictionary Methods</li>
</ul>
</li>
<li>Model Selection and the Bias&ndash;Variance Tradeoff <br/>
<img alt="Bias-Var" src="/images/esl/bias-var.png"/> </li>
</ul>
<h2 id="chapter-3-linear-methods-of-regression">Chapter 3: Linear Methods Of Regression</h2>
<ul>
<li>Introduction</li>
<li>Linear Regression Models and Least Squares<ul>
<li>Solution from <em>normal</em> form</li>
<li>F statistic</li>
<li>Example : prostrate cancer</li>
<li>The Gauss-Markov Theorem<ul>
<li>Proof that the Least Squares estimate for the parameters, <span class="math">\(\beta\)</span> has the least variance.    </li>
</ul>
</li>
<li>Multiple Regression from Simple Univariate Regression
<img alt="Alg 3.1" src="/images/esl/alg3_1.png"/></li>
<li>Multiple Outputs</li>
</ul>
</li>
<li>Subset Selection<ul>
<li>Best-Subset Selection</li>
<li>Forward and Backward-Stepwise Selection</li>
<li>Forward-Stagewise Selection
<img alt="alt" src="/images/esl/subset-sel.png"/></li>
<li>Example : Prostrate Cancer (Continued)</li>
</ul>
</li>
<li>Shrinkage Methods<ul>
<li>Ridge Regression : L2 regularization</li>
<li>The Lasso : L1 regularization</li>
<li>Discussion : Subset Selection, Ridge Regression and the Lasso</li>
<li>Least Angle Regression</li>
</ul>
</li>
<li>Methods Using Derived Input Directions<ul>
<li>Principal Components Regression</li>
<li>Partial Least Squares</li>
</ul>
</li>
<li>Discussion : A Comparison of Selection and Shrinkage Methods</li>
<li>Multiple Outcomes Shrinkage and Selection ☠</li>
<li>More on Lasso and Related Path Algorithms ☠<ul>
<li>Incremental Forward Stagewise Regression</li>
<li>Piecewise-Linear Path Algorithms</li>
<li>The Dantzig selector</li>
<li>The Grouped Lasso</li>
<li>Further Properties of Lasso</li>
<li>Pathwise Coordinate Optimization</li>
</ul>
</li>
<li>Computational Considerations  <ul>
<li>Fitting is usually done using <em>Cholesky decomposition</em> of matrix <span class="math">\(X^TX\)</span>.</li>
</ul>
</li>
</ul>
<h2 id="chapter-4-linear-methods-of-classification">Chapter 4: Linear Methods of Classification</h2>
<ul>
<li>Introduction</li>
<li>Linear Regression of an Indicator Matrix</li>
<li>Linear Discriminant Analysis<ul>
<li>Regularized Discriminant Analysis</li>
<li>Computations for LDA</li>
<li>Reduced-Rank Linear Discriminant Analysis</li>
</ul>
</li>
<li>Logistic Regression<ul>
<li>Fitting Logistic Regression Models</li>
<li>Example : South African Heart Disease</li>
<li>Quadratic Approximations and Inference</li>
<li><span class="math">\(L_1\)</span> Regularized Logistic Regression</li>
<li>Logistic Regression or LDA ?</li>
</ul>
</li>
<li>Separating Hyperplanes<ul>
<li>Rosenblatt&rsquo;s Perceptron Learning Algorithm</li>
<li>Optimal Separating Hyperplanes ☠</li>
</ul>
</li>
</ul>
<h2 id="chapter-5-basis-expansions-and-regularization">Chapter 5: Basis Expansions and Regularization</h2>
<ul>
<li>Introduction</li>
<li>Piecewise Polynomials and Splines
<img alt="alt" src="/images/esl/5_piecewise_1.png"/><ul>
<li>Natural Cubic Splines</li>
<li>Example: South African Heart Disease (Continued)</li>
<li>Example: Phoneme Recognition</li>
</ul>
</li>
<li>Filtering and Feature Extraction</li>
<li>Smoothing Splines<ul>
<li>Degrees of Freedom and Smoother Matrices</li>
</ul>
</li>
<li>Automatic Selection of the Smoothing Parameters<ul>
<li>Fixing the Degrees of Freedom</li>
<li>The Bias&ndash;Variance Tradeoff</li>
</ul>
</li>
<li>Nonparametric Logistic Regression</li>
<li>Multidimensional Splines</li>
<li>Regularization and Reproducing Kernel Hilbert Spaces ☠<ul>
<li>Spaces of Functions Generated by Kernels</li>
<li>Examples of RKHS</li>
<li>Penalized Polynomial Regression<ul>
<li>Gaussian Radial Basis Functions</li>
<li>Support Vector Classifiers</li>
</ul>
</li>
</ul>
</li>
<li>Wavelet Smoothing ☠<ul>
<li>Wavelet Smoothing and the Wavelet Transform</li>
<li>Adaptive Wavelet Filtering</li>
</ul>
</li>
</ul>
<h2 id="chapter-6-kernel-smoothing-methods">Chapter 6: Kernel Smoothing Methods</h2>
<ul>
<li>One-Dimensional Kernel Smoothers<ul>
<li>Local Linear Regression</li>
<li>Local Polynomial Regression</li>
</ul>
</li>
<li>Selecting the Width of the Kernel</li>
<li>Local Regression in <span class="math">\({\mathbb R}^p\)</span></li>
<li>Structured Local Regression Models in <span class="math">\({\mathbb R}^p\)</span><ul>
<li>Structured Kernels</li>
<li>Structured Regression Functions</li>
</ul>
</li>
<li>Kernel Density Estimation and Classification<ul>
<li>Kernel Density Estimation</li>
<li>Kernel Density Classification</li>
<li>The Naive Bayes Classifier</li>
</ul>
</li>
<li>Radial Basis Functions and Kernels</li>
<li>Mixture Models for Density Estimation and Classification</li>
<li>Computational Considerations</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}</script>

        <div class="sharethis-bottom">
          <span class='st_fblike_hcount' displayText='Facebook Like'></span>
          <span class='st_plusone_hcount' displayText='Google +1'></span>
          <span class='st_twitter_hcount' displayText='Tweet'></span>
          <span class='st_email_hcount' displayText='Email'></span>
        </div>
      </div><!-- /.entry-content -->
      <div class="comments">
        <h2>Comments</h2>
        <div id="disqus_thread"></div>
        <script type="text/javascript">
          var disqus_identifier = "summary/elements-of-statistical-learning-part-1.html";
          (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//nitishpuri.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
          })();
        </script>
      </div>

    </article>
  </section>
</div>
            </div>
        </div>
    </div>

    <footer id="site-footer">
        <div class="row-fluid">
            <div class="span10 offset1">
                <address>
                    <p>
                        This blog is proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                    </p>
                    <p>
                        <a href="http://github.com/jsliang/pelican-fresh/">Fresh</a> is a responsive theme designed by <a href="http://jsliang.com/">jsliang</a> and <a href="https://github.com/jsliang/pelican-fresh/graphs/contributors">contributors</a>.
                        Special thanks to <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a> and <a href="http://getbootstrap.com/">Twitter Bootstrap</a>.
                    </p>
                </address>
            </div>
        </div>
    </footer>

    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-103032011-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
<script type="text/javascript">
    var disqus_shortname = 'nitishpuri';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
    <script src="//code.jquery.com/jquery.min.js"></script>
    <script src="//netdna.bootstrapcdn.com/bootstrap/2.3.2/js/bootstrap.min.js"></script>
</body>
</html>