<!DOCTYPE html>
<html lang="english" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Neural Networks for Computer Vision - nitishpuri.github.io</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="/posts/machine-intelligence/neural-networks-for-computer-vision/">

        <meta name="author" content="Nitish" />
        <meta name="keywords" content="notes,deep-learning" />
        <meta name="description" content="Minimal notes on some papers or articles that I recently read. Mainly for logging." />

        <meta property="og:site_name" content="nitishpuri.github.io" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Neural Networks for Computer Vision"/>
        <meta property="og:url" content="/posts/machine-intelligence/neural-networks-for-computer-vision/"/>
        <meta property="og:description" content="Minimal notes on some papers or articles that I recently read. Mainly for logging."/>
        <meta property="article:published_time" content="2017-09-05" />
            <meta property="article:section" content="machine-intelligence" />
            <meta property="article:tag" content="notes" />
            <meta property="article:tag" content="deep-learning" />
            <meta property="article:author" content="Nitish" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.cosmo.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/native.css" rel="stylesheet">
    <link href="/theme/tipuesearch/tipuesearch.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>
        <link href="/static/custom.css" rel="stylesheet" type="text/css" />





</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
nitishpuri.github.io            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/pages/bio/">Bio</a></li>
                    <li><a href="/pages/gallery/">Gallery</a></li>
                    <li><a href="/pages/books/">Books</a></li>
                    <li><a href="https://nitishpuri.github.io/ProcessingExperiments/">Demos</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><span>
                <form class="navbar-search" action="/search.html">
                  <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input" required>
                </form></span>
              </li>
              <li><a href="/archives/"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/posts/machine-intelligence/neural-networks-for-computer-vision/"
                       rel="bookmark"
                       title="Permalink to Neural Networks for Computer Vision">
                        Neural Networks for Computer Vision
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-09-05T00:00:00+05:30"> Tue 05 September 2017</time>
    </span>


            <span class="label label-default">By</span>
            <a href="/author/nitish.html"><i class="fa fa-user"></i> Nitish</a>

        <span class="label label-default">Category</span>
        <a href="/category/machine-intelligence/">machine-intelligence</a>

        



<span class="label label-default">Tags</span>
	<a href="/tag/notes/">notes</a>
        /
	<a href="/tag/deep-learning/">deep-learning</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>

    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
<section class="well" id="related-posts">
    <h4>Part 2 of the Deep Learning series</h4>
       <h5>Previous articles</h5>
       <ul>
           <li><a href="/posts/machine-intelligence/research-notes-deep-learning/">Research Notes:: Deep Learning</a></li>
       </ul>
       <h5>Next articles</h5>
       <ul>
           <li><a href="/posts/machine-intelligence/style-transfer-part-1/">Style Transfer, Part 1</a></li>
           <li><a href="/posts/machine-intelligence/style-transfer-part-2/">Style Transfer, Part 2</a></li>
           <li><a href="/posts/machine-intelligence/neural-network-architectures/">Neural Network Architectures</a></li>
           <li><a href="/posts/machine-intelligence/object-detection-and-image-segmentation/">Object Detection and Image Segmentation</a></li>
           <li><a href="/posts/machine-intelligence/object-detection-and-image-segmentation-part-2/">Object Detection and Image Segmentation, Part 2</a></li>
       </ul>
</section>
                
                <p>Contents:</p>
                <nav class="toc">
                  <div id="toc"><ul><li><a class="toc-href" href="#image-recognition" title="Image Recognition">Image Recognition</a><ul><li><a class="toc-href" href="#very-deep-convolutional-networks-for-large-scale-image-recognition" title="  Very Deep Convolutional Networks For Large-Scale Image Recognition">  Very Deep Convolutional Networks For Large-Scale Image Recognition</a></li><li><a class="toc-href" href="#going-deeper-with-convolutions" title="  Going Deeper with Convolutions">  Going Deeper with Convolutions</a></li><li><a class="toc-href" href="#deep-residual-learning-for-image-recognition" title="  Deep Residual Learning for Image Recognition">  Deep Residual Learning for Image Recognition</a></li><li><a class="toc-href" href="#rethinking-the-inception-architecture-for-computer-vision" title="  Rethinking the Inception Architecture for Computer Vision">  Rethinking the Inception Architecture for Computer Vision</a></li><li><a class="toc-href" href="#inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning" title="  Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning">  Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></li><li><a class="toc-href" href="#xception-deep-learning-with-depthwise-separable-convolutions" title="  Xception: Deep Learning with Depthwise Separable Convolutions">  Xception: Deep Learning with Depthwise Separable Convolutions</a></li></ul></li><li><a class="toc-href" href="#deep-visualization_1" title="Deep Visualization">Deep Visualization</a><ul><li><a class="toc-href" href="#visualizing-and-understanding-convolutional-networks" title="  Visualizing and Understanding Convolutional Networks">  Visualizing and Understanding Convolutional Networks</a></li><li><a class="toc-href" href="#multifaceted-feature-visualization-uncovering-the-different-types-of-features-learned-by-each-neuron-in-deep-neural-networks" title="  Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks">  Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks</a></li><li><a class="toc-href" href="#how-transferable-are-features-in-deep-neural-networks" title="  How transferable are features in deep neural networks?">  How transferable are features in deep neural networks?</a></li></ul></li></ul></div>
                </nav>
                <hr>
                
                <p>Very minimal notes on some papers or articles that I recently read. Mainly for logging purposes.</p>
<h2 id="image-recognition">Image Recognition</h2>
<h3 id="very-deep-convolutional-networks-for-large-scale-image-recognition"><a name="vgg1"> </a> Very Deep Convolutional Networks For Large-Scale Image Recognition</h3>
<p><em>Karen Simonyan, Andrew Zisserman : Apr 2015</em> <br/>
<a href="https://arxiv.org/abs/1409.1556"><em>Source</em></a> <br/>
<a href="https://github.com/KaimingHe/resnet-1k-layers"><em>Implementation</em></a> </p>
<ul>
<li>Introduces the <em><strong>VGG network</strong> that won ImageNet in 2014</em>.</li>
<li>Deeper ConvNets. Takes input as (224 X 224) RGB and mean image subtracted as preprocessing. Two final FC hidden layers, followed by one FC layer with 1000 outputs. Number of total trainable parameters turn out to be 144 million for VGG-19. </li>
<li>All the hidden layers use <em>ReLU</em> activations.</li>
<li>Deeper networks with small filters result in more regularization and less parameters.</li>
<li>Optimise multinomial logistic regression objective using mini-batch gradient descent with momentum.</li>
<li>At the end introduces ensemble models by averaging softmax predictions from multiple models.</li>
</ul>
<h3 id="going-deeper-with-convolutions"><a name="inception1"> </a> Going Deeper with Convolutions</h3>
<p><em>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich : Sep 2014</em> <br/>
<em>Google DeepMind</em> <br/>
<a href="https://arxiv.org/abs/1409.4842"><em>Source</em></a></p>
<ul>
<li>Introduces <strong>"Inception"</strong> with improved utilization of computing resources.</li>
<li>"We need to go deeper" : But <em>deeper</em> networks come with a cost of large number of parameters, which makes the model prone to overfitting, and dramatically increased use of computational resources.</li>
<li>Fundamental idea : sparsely connected architectures, even inside the convolutions.</li>
<li>However, the computing infrastructure is very inefficient when it comes to numerical calculations on sparse data structures. And non-uniform sparse structures require careful engineering!</li>
<li><strong>Architecture</strong>
<img alt="" src="/images/papers/inception1.jpg"/> </li>
<li><strong>GoogLeNet</strong></li>
<li>22 trainable Layers(100 total layers), low memory footprint.</li>
<li>Auxillary classifiers are used to allow for efficient gradient propagation. These are used only at training time.</li>
</ul>
<h3 id="deep-residual-learning-for-image-recognition"><a name="resnet1"> </a> Deep Residual Learning for Image Recognition</h3>
<p><em>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun : Dec 2015</em> <br/>
<em>Microsoft Research</em> <br/>
<a href="https://arxiv.org/abs/1512.03385"><em>Source</em></a></p>
<ul>
<li>Presents residual learning framework(<strong>ResNet</strong>) to ease the training of networks that are substantially deeper(152 layers!) than those used previously. <em>How to win ImageNet in 2015.</em></li>
<li>Problem with deeper networks : <em>Vanishing Gradients</em> : Addressed by intermediate normalization.</li>
<li>Problem with deeper networks : <em>Degradation</em>, not caused by overfitting..
<img alt="" src="/images/papers/resNet1.jpg"/></li>
<li>Introduces residual learning framework by using shortcut connections that can perform identity mapping.</li>
<li>Using Identity mapping as precondition allows the network to easily learn the identity, if it is a desired mapping. This helps in <em>simplifying</em> networks.</li>
<li><em>Plain Network</em> architecture, mainly based on VGG nets.</li>
<li><em>Residual Network</em> architecture, insert shortcuts to the plain network.</li>
<li>The model shows <em>no optimization difficulty</em> even with &gt; 1000 layers..!!</li>
<li>Finally discusses improvements for detection and localization tasks.</li>
</ul>
<h3 id="rethinking-the-inception-architecture-for-computer-vision"><a name="inception2"> </a> Rethinking the Inception Architecture for Computer Vision</h3>
<p><em>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna : Dec 2015</em> <br/>
<em>Google DeepMind</em> <br/>
<a href="https://arxiv.org/abs/1512.00567"><em>Source</em></a> </p>
<ul>
<li>Improving upon <strong>Inception</strong> module and GoogLeNet.</li>
<li>General guiding principles<ul>
<li>Avoid representational bottlenecks, especially early in the network.</li>
<li>Higher dimensional representations are easier to process locally within a network.</li>
<li>Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. </li>
<li>Balance the width and depth of the network.</li>
</ul>
</li>
<li>Factorizing Convolutions with Large Filter Size<ul>
<li>Factorize into smaller convolutions. <ul>
<li>This results in reduced parameter count.</li>
<li>Does this replacement result in any loss of expressiveness?      <br/>
<img alt="" src="/images/papers/reInception1.jpg"/></li>
</ul>
</li>
</ul>
</li>
<li>Spatial Factorization into Asymmetric Convolutions
<img alt="" src="/images/papers/reInception2.jpg"/>
<img alt="" src="/images/papers/reInception3.jpg"/></li>
</ul>
<h3 id="inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning"><a name="inception4"> </a> Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</h3>
<p><em>Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi : Aug 2016</em> <br/>
<em>Google DeepMind</em> <br/>
<a href="https://arxiv.org/abs/1602.07261"><em>Source</em></a> </p>
<ul>
<li>Combining <strong>Residual networks with Inception architecture</strong>.</li>
<li>Uniform Inception-v4 blocks are introduced for cleaner architecture. </li>
</ul>
<h3 id="xception-deep-learning-with-depthwise-separable-convolutions"><a name="xception"> </a> Xception: Deep Learning with Depthwise Separable Convolutions</h3>
<p><em>Fran&ccedil;ois Chollet : Apr 2017</em> <br/>
<a href="https://arxiv.org/abs/1610.02357"><em>Source</em></a> <br/>
<em>Google Inc.</em> </p>
<ul>
<li>Building on top of the <em>inception</em> modules.</li>
<li>An attempt to make things efficient by decoupling operations for cross-channel correlations and spatial correlations.</li>
<li>This introduces the <em>Depthwise separable convolutions</em>.</li>
<li>The <em>Xception</em> architecture then takes these layers and builds a complete network for ImageNet task, with better reported performane than <em>Inception v3</em>.</li>
</ul>
<h2 id="deep-visualization_1">Deep Visualization</h2>
<h3 id="visualizing-and-understanding-convolutional-networks"><a name="vis1"> </a> Visualizing and Understanding Convolutional Networks</h3>
<p><em>Matthew D Zeiler, Rob Fergus L : Nov 2013</em> <br/>
<a href="https://arxiv.org/abs/1311.2901"><em>Source</em></a> </p>
<ul>
<li>Understanding why CNNs perform well on Image Classification tasks.</li>
<li>Visualizing with a Deconvnet
<img alt="" src="/images/papers/visnet1.jpg"/></li>
<li>Feature Visualization
<img alt="" src="/images/papers/visnet2.jpg"/>
<img alt="" src="/images/papers/visnet3.jpg"/>
<img alt="" src="/images/papers/visnet4.jpg"/>
<img alt="" src="/images/papers/visnet5.jpg"/></li>
<li>Feature Evolution during training
<img alt="" src="/images/papers/visnet6.jpg"/></li>
<li>Feature Invariance</li>
<li>Occlusion Sensitivity
<img alt="" src="/images/papers/visnet7.jpg"/>
<img alt="" src="/images/papers/visnet8.jpg"/></li>
<li>Correspondence Analysis  </li>
</ul>
<h3 id="multifaceted-feature-visualization-uncovering-the-different-types-of-features-learned-by-each-neuron-in-deep-neural-networks"><a name="vis2"> </a> Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks</h3>
<p><em>Anh Nguyen, Jason Yosinski, Jeff Clune : May 2016</em> <br/>
<a href="https://arxiv.org/abs/1602.03616"><em>Source</em></a> </p>
<ul>
<li>Researchers have been using <em>activation maximization</em> techniques until now. This assumes that each neuron detects only one type of feature.</li>
<li>But, we know neurons can be <em>multifaceted</em>. Here <em>multifaceted feature visualization</em> (MFV) is introduced.<ul>
<li>Systematically visualize all facets of a neuron.</li>
<li>Improve image quality of synthesized images with natural and globally consistent colors.
<img alt="" src="/images/papers/multiVis1.jpg"/></li>
</ul>
</li>
<li><em>Center biased regularization</em> is used so that the synthesized images dont have many repeated object fragments.<ul>
<li>This is done by first producing a blurry image, then updating the center pixels more than the edge ones, producing a final image that is sharp and has a centrally-located object.</li>
<li>This image would have far fewer duplicated fragments.
<img alt="" src="/images/papers/multiVis2.jpg"/></li>
</ul>
</li>
<li>Visualizing the multifaceted nature of hidden neurons
<img alt="" src="/images/papers/multiVis3.jpg"/>
<img alt="" src="/images/papers/multiVis4.jpg"/></li>
<li>Discusses various optimization techniques to produce better images in detail : center biased regularization, mean image initialization.</li>
</ul>
<h3 id="how-transferable-are-features-in-deep-neural-networks"><a name="transfer1"> </a> How transferable are features in deep neural networks?</h3>
<p><em>Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson : Nov 2014</em> <br/>
<a href="https://arxiv.org/abs/1411.1792"><em>Source</em></a></p>
<ul>
<li>First-layer features always resembles either <em>Gabor filters</em> or color blobs.</li>
<li>We also often use the initial layers of a network to initialize other networks(for a  different task) in <em>transfer learning</em>. This raises a few questions,
    *Can we quantify the degree to which a particular layer is general or specific?<ul>
<li>Does the transition occur suddenly at a single layer, or is it spread out over several layers?</li>
<li>Where does this transition take place: near the first, middle, or last layer of the network? <br/>
<img alt="" src="/images/papers/transfer1.png"/> <br/>
Figure 1: Overview of the experimental treatments and controls. <em>Top two rows</em>: The base networks
are trained using standard supervised backprop on only half of the ImageNet dataset (first row: A
half, second row: B half). The labeled rectangles (e.g. <span class="math">\(W_{A1}\)</span>) represent the weight vector learned for that layer, with the color indicating which dataset the layer was originally trained on. The vertical, ellipsoidal bars between weight vectors represent the activations of the network at each layer. <em>Third row</em>: In the <em>selffer</em> network control, the first <span class="math">\(n\)</span> weight layers of the network (in this example, <span class="math">\(n = 3\)</span>) are copied from a base network (e.g. one trained on dataset B), the upper <span class="math">\(8 &minus; n\)</span> layers are randomly initialized, and then the entire network is trained on that same dataset (in this example, dataset B).
The first n layers are either locked during training (&ldquo;frozen&rdquo; selffer treatment <span class="math">\(B3B\)</span>) or allowed to learn (&ldquo;fine-tuned&rdquo; selffer treatment <span class="math">\(B3B^+\)</span>). This treatment reveals the occurrence of <em>fragile coadaptation</em>, when neurons on neighboring layers co-adapt during training in such a way that cannot be rediscovered when one layer is frozen. <em>Fourth row</em>: The <em>transfer</em> network experimental treatment is the same as the selffer treatment, except that the first n layers are copied from a network trained on one dataset (e.g. A) and then the entire network is trained on the other dataset (e.g. B). This treatment tests the extent to which the features on layer n are general or specific.</li>
</ul>
</li>
</ul>
<p><img alt="" src="/images/papers/transfer2.png"/> <br/>
<img alt="" src="/images/papers/transfer3.png"/> <br/>
Figure 2: The results from this paper&rsquo;s main experiment. <em>Top</em>: Each marker in the figure represents the average accuracy over the validation set for a trained network. The white circles 
above <span class="math">\(n = 0\)</span> represent the accuracy of baseB. There are eight points, because we tested on four separate random A/B splits. Each dark blue dot represents a BnB network. Light blue points 
represent BnB+ networks, or fine-tuned versions of BnB. Dark red diamonds are AnB networks, and 
light red diamonds are the fine-tuned AnB+ versions. Points are shifted slightly left or right 
for visual clarity. <em>Bottom</em>: Lines connecting the means of each treatment. </p>
<ul>
<li>Conclusively, <em>transfer</em> learning can be very effective for lower layers for general dissimilar objectives, and for higher layers as well in case of similar objectives.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "true";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

            
            </div>
            <!-- /.entry-content -->
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.facebook.com/npuri1903"><i class="fa fa-facebook-square fa-lg"></i> Facebook</a></li>
    <li class="list-group-item"><a href="https://github.com/nitishpuri"><i class="fa fa-github-square fa-lg"></i> Github</a></li>
    <li class="list-group-item"><a href="https://www.linkedin.com/in/nitishpuri/"><i class="fa fa-linkedin-square fa-lg"></i> Linkedin</a></li>
    <li class="list-group-item"><a href="https://www.instagram.com/purinitish/"><i class="fa fa-instagram fa-lg"></i> Instagram</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="/category/articles/"><i class="fa fa-folder-open fa-lg"></i>articles</a>
    </li>
    <li class="list-group-item">
      <a href="/category/books/"><i class="fa fa-folder-open fa-lg"></i>books</a>
    </li>
    <li class="list-group-item">
      <a href="/category/gallery/"><i class="fa fa-folder-open fa-lg"></i>gallery</a>
    </li>
    <li class="list-group-item">
      <a href="/category/machine-intelligence/"><i class="fa fa-folder-open fa-lg"></i>machine-intelligence</a>
    </li>
    <li class="list-group-item">
      <a href="/category/robotics/"><i class="fa fa-folder-open fa-lg"></i>robotics</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-1">
      <a href="/tag/algorithms/">algorithms</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/blogging/">blogging</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/book/">book</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/data-science/">data-science</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/deep-learning/">deep-learning</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/design/">design</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/gallery/">gallery</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/generative/">generative</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/graphics/">graphics</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/image-segmentation/">image-segmentation</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/jekyll/">jekyll</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/notes/">notes</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/opengl/">opengl</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/philosophy/">philosophy</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/programming/">programming</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/projects/">projects</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/robotics/">robotics</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/sketch/">sketch</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/style-transfer/">style-transfer</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/udacity/">udacity</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Series -->
<li class="list-group-item">
  <h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Series</span></h4>
  <ul class="list-group">
    <li class="list-group-item">
      <h5></i>Previous article</h5>
      <a href="/posts/machine-intelligence/research-notes-deep-learning/">Research Notes:: Deep Learning</a>
    </li>
    <li class="list-group-item">
      <h5>Next article</h5>
      <a href="/posts/machine-intelligence/style-transfer-part-1/">Style Transfer, Part 1</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Series -->


<!-- Sidebar/Github -->
<li class="list-group-item">
  <h4><i class="fa fa-github fa-lg"></i><span class="icon-label">GitHub Repos</span></h4>
  <div id="gh_repos">
    <p class="list-group-item">Status updating...</p>
  </div>
</li>
<!-- End Sidebar/Github -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2017 Nitish Puri
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>


<!-- GitHub JS Code -->
<script type="text/javascript">
$(document).ready(function () {
  if (!window.jXHR) {
    var jxhr = document.createElement('script');
    jxhr.type = 'text/javascript';
    jxhr.src = '/theme/js/jXHR.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(jxhr, s);
  }

  github.showRepos({
    user: 'nitishpuri',
    count: 5,
    skip_forks: true,
    target: '#gh_repos'
  });
});
</script>
<script src="/theme/js/github.js" type="text/javascript"></script>
<!-- End GitHub JS Code -->
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-103032011-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->


    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59bc69ad3aa13e47"></script>
</body>
</html>