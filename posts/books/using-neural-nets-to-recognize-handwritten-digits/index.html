<!DOCTYPE html>
<html lang="english" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Using neural nets to recognize handwritten digits - nitishpuri.github.io</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="/posts/books/using-neural-nets-to-recognize-handwritten-digits/">

        <meta name="author" content="Micheal Nelson" />
        <meta name="keywords" content="deep-learning,notes" />
        <meta name="description" content="Neural Networks and Deep Learning, Chapter 1" />

        <meta property="og:site_name" content="nitishpuri.github.io" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Using neural nets to recognize handwritten digits"/>
        <meta property="og:url" content="/posts/books/using-neural-nets-to-recognize-handwritten-digits/"/>
        <meta property="og:description" content="Neural Networks and Deep Learning, Chapter 1"/>
        <meta property="article:published_time" content="2017-09-21" />
            <meta property="article:section" content="books" />
                <meta property="article:subcategory" content="books/neuralNets" />
            <meta property="article:tag" content="deep-learning" />
            <meta property="article:tag" content="notes" />
            <meta property="article:author" content="Micheal Nelson" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.cosmo.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/native.css" rel="stylesheet">
    <link href="/theme/tipuesearch/tipuesearch.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>
        <link href="/static/custom.css" rel="stylesheet" type="text/css" />





</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
nitishpuri.github.io            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/pages/bio/">Bio</a></li>
                    <li><a href="/pages/gallery/">Gallery</a></li>
                    <li><a href="/pages/books/">Books</a></li>
                    <li><a href="https://nitishpuri.github.io/ProcessingExperiments/">Demos</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><span>
                <form class="navbar-search" action="/search.html">
                  <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input" required>
                </form></span>
              </li>
              <li><a href="/archives/"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/posts/books/using-neural-nets-to-recognize-handwritten-digits/"
                       rel="bookmark"
                       title="Permalink to Using neural nets to recognize handwritten digits">
                        Using neural nets to recognize handwritten digits
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-09-21T00:00:00+05:30"> Thu 21 September 2017</time>
    </span>


            <span class="label label-default">By</span>
            <a href="/author/micheal-nelson.html"><i class="fa fa-user"></i> Micheal Nelson</a>

        <span class="label label-default">Category</span>
        <a href="/category/books/">books</a>

            <a href="/category/books/neuralnets/">/neuralNets</a>
        



<span class="label label-default">Tags</span>
	<a href="/tag/deep-learning/">deep-learning</a>
        /
	<a href="/tag/notes/">notes</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>

    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
<section class="well" id="related-posts">
    <h4>Part 2 of the Neural Networks and Deep Learning series</h4>
       <h5>Previous articles</h5>
       <ul>
           <li><a href="/posts/books/neural-networks-and-deep-learning/">Neural Networks and Deep Learning</a></li>
       </ul>
       <h5>Next articles</h5>
       <ul>
           <li><a href="/posts/books/how-the-backpropogation-algorithm-works/">How the backpropogation algorithm works</a></li>
           <li><a href="/posts/books/improving-the-way-neural-networks-learn/">Improving the way neural networks learn</a></li>
           <li><a href="/posts/books/a-visual-proof-that-neural-networks-can-compute-any-function/">A visual proof that neural networks can compute any function</a></li>
           <li><a href="/posts/books/why-are-deep-neural-networks-hard-to-train/">Why are deep neural networks hard to train?</a></li>
           <li><a href="/posts/books/deep-learning/">Deep Learning</a></li>
           <li><a href="/posts/books/is-there-a-simple-algorithm-for-intelligence/">Is there a simple algorithm for intelligence?</a></li>
       </ul>
</section>
                
                <p>Contents:</p>
                <nav class="toc">
                  <div id="toc"><ul><li><a class="toc-href" href="#chapter-1-using-neural-nets-to-recognize-handwritten-digits" title="Chapter 1: Using neural nets to recognize handwritten digits">Chapter 1: Using neural nets to recognize handwritten digits</a><ul><li><a class="toc-href" href="#perceptrons" title="Perceptrons">Perceptrons</a><ul><li><a class="toc-href" href="#network-of-perceptrons" title="Network of perceptrons">Network of perceptrons</a></li><li><a class="toc-href" href="#neural-nets-as-logic-gates" title="Neural Nets as Logic Gates">Neural Nets as Logic Gates</a></li></ul></li><li><a class="toc-href" href="#sigmoid-neurons_1" title="Sigmoid Neurons">Sigmoid Neurons</a></li><li><a class="toc-href" href="#the-architecture-of-neural-networks" title="The architecture of neural networks">The architecture of neural networks</a></li><li><a class="toc-href" href="#a-simple-network-to-classify-handwritten-digits" title="A simple network to classify handwritten digits">A simple network to classify handwritten digits</a></li><li><a class="toc-href" href="#learning-with-gradient-descent" title="Learning with gradient descent">Learning with gradient descent</a></li><li><a class="toc-href" href="#implementing-our-network-to-classify-digits" title="Implementing our network to classify digits">Implementing our network to classify digits</a><ul><li><a class="toc-href" href="#the-network-class" title="The Network class">The Network class</a></li></ul></li><li><a class="toc-href" href="#towards-deep-learning_1" title="Towards Deep Learning">Towards Deep Learning</a></li></ul></li></ul></div>
                </nav>
                <hr>
                
                <p>Notes for the <a href="http://neuralnetworksanddeeplearning.com/index.html">book</a>. <br/>
<a href="https://github.com/mnielsen/neural-networks-and-deep-learning">Source code</a> for the book.</p>
<h2 id="chapter-1-using-neural-nets-to-recognize-handwritten-digits">Chapter 1: Using neural nets to recognize handwritten digits</h2>
<h3 id="perceptrons">Perceptrons</h3>
<p><img alt="" src="/images/nnfordl/perceptron1.png"/> <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \mbox{output} &amp; = &amp; \left\{ \begin{array}{ll}
      0 &amp; \mbox{if } \sum_j w_j x_j \leq \mbox{ threshold} \\
      1 &amp; \mbox{if } \sum_j w_j x_j &gt; \mbox{ threshold}
      \end{array} \right.
\tag{1}\end{eqnarray}$$</div>
<h4 id="network-of-perceptrons">Network of perceptrons</h4>
<p><img alt="" src="/images/nnfordl/perceptron2.png"/> </p>
<p>First, simplify notation, <span class="math">\(w \cdot x \equiv \sum_j w_j x_j\)</span> <br/>
Move, <em>threshold</em> into the network as <em>bias</em>, <span class="math">\(b \equiv -\mbox{threshold}\)</span> <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \mbox{output} = \left\{ 
    \begin{array}{ll} 
      0 &amp; \mbox{if } w\cdot x + b \leq 0 \\
      1 &amp; \mbox{if } w\cdot x + b &gt; 0
    \end{array}
  \right.
\tag{2}\end{eqnarray}$$</div>
<h4 id="neural-nets-as-logic-gates">Neural Nets as Logic Gates</h4>
<p>This network represent a <em>NAND</em> Gate <br/>
<img alt="" src="/images/nnfordl/perceptron3.png"/> </p>
<p><em>NAND</em> gates can do arbitrary computations, <br/>
<img alt="" src="/images/nnfordl/perceptron4.png"/> </p>
<p>And so can the perceptrons, <br/>
<img alt="" src="/images/nnfordl/perceptron5.png"/> </p>
<p>This is reassuring, as this shows that perceptron can be as powerful as any computing device. Bit is disappointing as it makes the perceptrons just another type of <em>NAND</em>.   </p>
<p>However, these <em>perceptrons</em> can <em>learn</em>.</p>
<h3 id="sigmoid-neurons_1">Sigmoid Neurons</h3>
<p>We want our network to do this, <br/>
<img alt="" src="/images/nnfordl/perceptron6.png"/> </p>
<p>we would be able to <em>learn</em> if we repeat this process gradually <em>improving</em> our weights.   </p>
<p>However, perceptrons don't behave that way, changing the weights or bias may only completely flip the output, say 0 to 1. This makes it difficult to do gradual improvements to our network.   </p>
<p>Enter <em>sigmoid</em>, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \sigma(z) \equiv \frac{1}{1+e^{-z}}.
\tag{3}\end{eqnarray}$$</div>
<p>and, <span class="math">\(\sigma(z) \in [0,1]\)</span> </p>
<p>Output of sigmoid neuron <span class="math">\( = \sigma(w \cdot x+b)\)</span> </p>
<p><img alt="" src="/images/nnfordl/perceptron7.png"/> </p>
<p>Which is a smoothed out version of the <em>step</em> function represented by <em>perceptrons</em>.   </p>
<p>Using come calculus, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \Delta \mbox{output} \approx \sum_j \frac{\partial \, \mbox{output}}{\partial w_j}
  \Delta w_j + \frac{\partial \, \mbox{output}}{\partial b} \Delta b,
\tag{5}\end{eqnarray}$$</div>
<p><span class="math">\(\Delta \mbox{output}\)</span> is a <em>linear</em> function with respect to <span class="math">\(\Delta w_j\)</span> and <span class="math">\(\Delta b\)</span>. <br/>
This makes it easier to figure out how to change the weights to achieve some output.   </p>
<h3 id="the-architecture-of-neural-networks">The architecture of neural networks</h3>
<p><em>Feedforward</em> neural network <br/>
<img alt="" src="/images/nnfordl/perceptron7.png"/> </p>
<h3 id="a-simple-network-to-classify-handwritten-digits">A simple network to classify handwritten digits</h3>
<p><img alt="" src="/images/nnfordl/mnist1.png"/> </p>
<h3 id="learning-with-gradient-descent">Learning with gradient descent</h3>
<p>The <a href="http://yann.lecun.com/exdb/mnist/">dataset</a>.</p>
<p>We use the following <em>quadratic cost function</em>, also called the <em>mean squared error</em> or just <em>MSE</em>, <br/>
</p>
<div class="math">$$\begin{eqnarray}  C(w,b) \equiv
  \frac{1}{2n} \sum_x \| y(x) - a\|^2.
\tag{6}\end{eqnarray}$$</div>
<p>To minimize this function, we use <em>gradient descent</em>,   </p>
<p><em>Gradient descent with two variables,</em> <br/>
<img alt="" src="/images/nnfordl/mnist2.png"/> </p>
<p>We could use calculus to find the minima analytically, but it would become a nightmare as soon as the number of variables go up.   </p>
<p>So, let's start rolling a ball down the valley,..    </p>
<div class="math">$$\begin{eqnarray} 
  \Delta C \approx \frac{\partial C}{\partial v_1} \Delta v_1 +
  \frac{\partial C}{\partial v_2} \Delta v_2.
\tag{7}\end{eqnarray}$$</div>
<p>We also define a <em>gradient</em> vector <span class="math">\(\nabla C\)</span>, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \nabla C \equiv \left( \frac{\partial C}{\partial v_1}, 
  \frac{\partial C}{\partial v_2} \right)^T.
\tag{8}\end{eqnarray}$$</div>
<p>With this defined, we can rewrite, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \Delta C \approx \nabla C \cdot \Delta v.
\tag{9}\end{eqnarray}$$</div>
<p>This equation lets us choose <span class="math">\(\Delta v\)</span> so as to make <span class="math">\(\Delta C\)</span> negative. <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \Delta v = -\eta \nabla C,
\tag{10}\end{eqnarray}$$</div>
<p>where <span class="math">\(\eta\)</span> is a small, positive parameter (known as the <em>learning rate</em>).   </p>
<p>Then, we can combine <span class="math">\((9)\)</span> and <span class="math">\((10)\)</span> to give, <span class="math">\(\Delta C \approx -\eta \nabla C \cdot \nabla C = -\eta \|\nabla C\|^2\)</span>. </p>
<p>So, we will use <span class="math">\((10)\)</span> to compute <span class="math">\(\Delta v\)</span>, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  v \rightarrow v' = v -\eta \nabla C.
\tag{11}\end{eqnarray}$$</div>
<p>doing this until - we hope - we reach a global minimum.</p>
<p>This can also be written as, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  w_k &amp; \rightarrow &amp; w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \tag{16}\\
  b_l &amp; \rightarrow &amp; b_l' = b_l-\eta \frac{\partial C}{\partial b_l}.
\tag{17}\end{eqnarray}$$</div>
<p>Calculating all the gradients can become slow,    </p>
<p>So we do that in batches, using <em>stochastic gradient descent</em>, <br/>
We randomly choose a <em>mini-batch</em> of samples from the training inputs, <span class="math">\(X_1, X_2, \ldots,X_m\)</span> <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \frac{\sum_{j=1}^m \nabla C_{X_{j}}}{m} \approx \frac{\sum_x \nabla C_x}{n} = \nabla C,
\tag{18}\end{eqnarray}$$</div>
<p>So now our update rule becomes, <br/>
$$\begin{eqnarray} 
  w_k &amp; \rightarrow &amp; w_k' = w_k-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial w_k} \tag{20}\</p>
<p>b_l &amp; \rightarrow &amp; b_l' = b_l-\frac{\eta}{m}
  \sum_j \frac{\partial C_{X_j}}{\partial b_l},
\tag{21}\end{eqnarray}$$</p>
<p>We do this update for all the training data, dividing it into batches. This completes a single <em>epoch</em>.   </p>
<h3 id="implementing-our-network-to-classify-digits">Implementing our network to classify digits</h3>
<p>Clone the repo,   </p>
<div class="highlight"><pre><span></span>git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git
</pre></div>
<p>We will be using 10,000 images from the test set as our <em>validation set</em>. <br/>
This will be used for <em>hyper-parameter tuning</em>.   </p>
<h4 id="the-network-class">The <em>Network</em> class</h4>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>    <span class="c1"># Number of layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sizes</span> <span class="o">=</span> <span class="n">sizes</span>              <span class="c1"># Number of neurons in the respective layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> 
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
</pre></div>
<p>To create a Nueral Net with 2 neurons in the first layer, 3 neurons in the second layer, and 1 neuron in the final layer,   </p>
<div class="highlight"><pre><span></span><span class="n">net</span> <span class="o">=</span> <span class="n">Network</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</pre></div>
<p>It then generates random initial <em>bias</em> and <em>weights</em> for the layers.   </p>
<p>We can calculate the activations of a given layer by, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  a' = \sigma(w a + b).
\tag{22}\end{eqnarray}$$</div>
<p>Here we are <em>vectorizing</em> out operations to denote(and compute) them compactly.
So, we can define the <em>sigmoid</em> activation in our code,   </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
<p>and the <em>feedforward</em> function in the <code>Network</code> class,   </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">a</span><span class="p">):</span>
    <span class="sd">"""Return the output of the network, if `a` is the input"""</span>
    <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">a</span>
</pre></div>
<p>Now, the main thing that we want our network to do is to learn, so we'll define an <code>SGD</code> method   </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">SGD</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainig_data</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">"""Train the neural network using mini-batch stochastic gradient descent. </span>
<span class="sd">    The "training_data" is a list of tuples "(x,y)" representing the training inputs</span>
<span class="sd">    and the desired outputs. If "test_data" is provided then the network will be evaluated </span>
<span class="sd">    against then test data after each epoch. This is good for tracking the progress, </span>
<span class="sd">    but slows things down"""</span>

    <span class="k">if</span> <span class="n">test_data</span><span class="p">:</span> <span class="n">n_test</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainig_data</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">trainig_data</span><span class="p">)</span>
        <span class="n">mini_batches</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">trainig_data</span><span class="p">[</span><span class="n">k</span><span class="p">:</span><span class="n">k</span><span class="o">+</span><span class="n">mini_batch_size</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n</span><span class="p">,</span><span class="n">mini_batch_size</span><span class="p">)]</span>

        <span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">mini_batches</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update_mini_batch</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">,</span> <span class="n">eta</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">test_data</span><span class="p">:</span>
            <span class="k">print</span> <span class="p">(</span><span class="s2">"Epoch {0}:{1}/{2}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">test_data</span><span class="p">),</span> <span class="n">n_test</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">"Epocj {0} complete."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>
</pre></div>
<p>In the above code the training data is randomly divided into mini batches and for each batch the gradient step is applied using <code>self.update_mini_batch(mini_batch, eta)</code>.   </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">update_mini_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mini_batch</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
    <span class="sd">"""Update the network's weights and biases by applying</span>
<span class="sd">    gradient descent using backpropogation to a single  mini batch.</span>
<span class="sd">    The "mini_batch" is a list of tuples "(x,y)", and "eta" is </span>
<span class="sd">    the learning_rate."""</span> 

    <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">]</span>
    <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">mini_batch</span><span class="p">:</span>
        <span class="n">delta_nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">nb</span> <span class="o">+</span> <span class="n">dnb</span> <span class="k">for</span> <span class="n">nb</span><span class="p">,</span> <span class="n">dnb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_b</span><span class="p">)]</span>
        <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">nw</span> <span class="o">+</span> <span class="n">dnw</span> <span class="k">for</span> <span class="n">nw</span><span class="p">,</span> <span class="n">dnw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">nabla_w</span><span class="p">,</span> <span class="n">delta_nabla_w</span><span class="p">)]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nw</span> 
        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">nw</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="o">-</span><span class="p">(</span><span class="n">eta</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">))</span><span class="o">*</span><span class="n">nb</span> 
        <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">nb</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="n">nabla_b</span><span class="p">)]</span>
</pre></div>
<p>Here, most of the work is done by the line,(which is explained in the next article in the series)   </p>
<div class="highlight"><pre><span></span>       <span class="n">delta_nabla_b</span><span class="p">,</span> <span class="n">delta_nabla_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
<p>We can now load some data using the helper scripts in the repo,   </p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">mnist_loader</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">training_data</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> \
<span class="o">...</span> <span class="n">mnist_loader</span><span class="o">.</span><span class="n">load_data_wrapper</span><span class="p">()</span>
</pre></div>
<p>And then create a network,   </p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">network</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">net</span> <span class="o">=</span> <span class="n">network</span><span class="o">.</span><span class="n">Network</span><span class="p">([</span><span class="mi">784</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
<p>Finally, we can use <code>SGD</code> to learn from the MNIST data,   </p>
<div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="n">net</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">trainig_data</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">mini_batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">test_data</span><span class="o">=</span><span class="n">validation_data</span><span class="p">)</span>
</pre></div>
<p>Here is the output you should expect,   </p>
<div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">0</span><span class="p">:</span> <span class="mi">9129</span> <span class="o">/</span> <span class="mi">10000</span>
<span class="n">Epoch</span> <span class="mi">1</span><span class="p">:</span> <span class="mi">9295</span> <span class="o">/</span> <span class="mi">10000</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="p">:</span> <span class="mi">9348</span> <span class="o">/</span> <span class="mi">10000</span>
<span class="o">...</span>
<span class="n">Epoch</span> <span class="mi">27</span><span class="p">:</span> <span class="mi">9528</span> <span class="o">/</span> <span class="mi">10000</span>
<span class="n">Epoch</span> <span class="mi">28</span><span class="p">:</span> <span class="mi">9542</span> <span class="o">/</span> <span class="mi">10000</span>
<span class="n">Epoch</span> <span class="mi">29</span><span class="p">:</span> <span class="mi">9534</span> <span class="o">/</span> <span class="mi">10000</span>
</pre></div>
<p>That is <em>95.42</em> percent accuracy in <em>28 epochs</em>.</p>
<p><em>Tuning the hyperparameters</em> </p>
<p>This can be challenging. The art of debugging is required here.</p>
<p>The current state-of-the-art for this dataset is <em>99.79</em> percent.</p>
<h3 id="towards-deep-learning_1">Towards Deep Learning</h3>
<p>How does the network does what it does?</p>
<p>Possibly by, <em>breaking down the problem into subproblems and finding those answers</em>.</p>
<p>But, this <em>breaking down</em> is done by the network <em>automatically</em> while learning, 
and we don't really have a say in it.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "true";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

            
            </div>
            <!-- /.entry-content -->
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.facebook.com/npuri1903"><i class="fa fa-facebook-square fa-lg"></i> Facebook</a></li>
    <li class="list-group-item"><a href="https://github.com/nitishpuri"><i class="fa fa-github-square fa-lg"></i> Github</a></li>
    <li class="list-group-item"><a href="https://www.linkedin.com/in/nitishpuri/"><i class="fa fa-linkedin-square fa-lg"></i> Linkedin</a></li>
    <li class="list-group-item"><a href="https://www.instagram.com/purinitish/"><i class="fa fa-instagram fa-lg"></i> Instagram</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="/category/articles/"><i class="fa fa-folder-open fa-lg"></i>articles</a>
    </li>
    <li class="list-group-item">
      <a href="/category/books/"><i class="fa fa-folder-open fa-lg"></i>books</a>
    </li>
    <li class="list-group-item">
      <a href="/category/gallery/"><i class="fa fa-folder-open fa-lg"></i>gallery</a>
    </li>
    <li class="list-group-item">
      <a href="/category/machine-intelligence/"><i class="fa fa-folder-open fa-lg"></i>machine-intelligence</a>
    </li>
    <li class="list-group-item">
      <a href="/category/robotics/"><i class="fa fa-folder-open fa-lg"></i>robotics</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-1">
      <a href="/tag/algorithms/">algorithms</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/architecture/">architecture</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/blogging/">blogging</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/book/">book</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/data-science/">data-science</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/deep-dream/">deep dream</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/deep-learning/">deep-learning</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/design/">design</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/gallery/">gallery</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/game-engine/">game engine</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/generative/">generative</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/graphics/">graphics</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/image-segmentation/">image-segmentation</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/imagenet/">imagenet</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/jekyll/">jekyll</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/lisp/">lisp</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/notes/">notes</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/opengl/">opengl</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/philosophy/">philosophy</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/prgogramming/">prgogramming</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/programming/">programming</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/projects/">projects</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/robotics/">robotics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/scheme/">scheme</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/sketch/">sketch</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/style-transfer/">style-transfer</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/udacity/">udacity</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Series -->
<li class="list-group-item">
  <h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Series</span></h4>
  <ul class="list-group">
    <li class="list-group-item">
      <h5></i>Previous article</h5>
      <a href="/posts/books/neural-networks-and-deep-learning/">Neural Networks and Deep Learning</a>
    </li>
    <li class="list-group-item">
      <h5>Next article</h5>
      <a href="/posts/books/how-the-backpropogation-algorithm-works/">How the backpropogation algorithm works</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Series -->


<!-- Sidebar/Github -->
<li class="list-group-item">
  <h4><i class="fa fa-github fa-lg"></i><span class="icon-label">GitHub Repos</span></h4>
  <div id="gh_repos">
    <p class="list-group-item">Status updating...</p>
  </div>
</li>
<!-- End Sidebar/Github -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2018 Nitish Puri
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>


<!-- GitHub JS Code -->
<script type="text/javascript">
$(document).ready(function () {
  if (!window.jXHR) {
    var jxhr = document.createElement('script');
    jxhr.type = 'text/javascript';
    jxhr.src = '/theme/js/jXHR.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(jxhr, s);
  }

  github.showRepos({
    user: 'nitishpuri',
    count: 5,
    skip_forks: true,
    target: '#gh_repos'
  });
});
</script>
<script src="/theme/js/github.js" type="text/javascript"></script>
<!-- End GitHub JS Code -->
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-103032011-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->


    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59bc69ad3aa13e47"></script>
</body>
</html>