<!DOCTYPE html>
<html lang="english" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>A visual proof that neural networks can compute any function - nitishpuri.github.io</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="/posts/books/a-visual-proof-that-neural-networks-can-compute-any-function/">

        <meta name="author" content="Micheal Nelson" />
        <meta name="keywords" content="deep-learning,notes" />
        <meta name="description" content="Neural Networks and Deep Learning, Chapter 4" />

        <meta property="og:site_name" content="nitishpuri.github.io" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="A visual proof that neural networks can compute any function"/>
        <meta property="og:url" content="/posts/books/a-visual-proof-that-neural-networks-can-compute-any-function/"/>
        <meta property="og:description" content="Neural Networks and Deep Learning, Chapter 4"/>
        <meta property="article:published_time" content="2017-11-23" />
            <meta property="article:section" content="books" />
                <meta property="article:subcategory" content="books/neuralNets" />
            <meta property="article:tag" content="deep-learning" />
            <meta property="article:tag" content="notes" />
            <meta property="article:author" content="Micheal Nelson" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.cosmo.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/native.css" rel="stylesheet">
    <link href="/theme/tipuesearch/tipuesearch.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>
        <link href="/static/custom.css" rel="stylesheet" type="text/css" />





</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
nitishpuri.github.io            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/pages/bio/">Bio</a></li>
                    <li><a href="/pages/gallery/">Gallery</a></li>
                    <li><a href="/pages/books/">Books</a></li>
                    <li><a href="https://nitishpuri.github.io/ProcessingExperiments/">Demos</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><span>
                <form class="navbar-search" action="/search.html">
                  <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input" required>
                </form></span>
              </li>
              <li><a href="/archives/"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/posts/books/a-visual-proof-that-neural-networks-can-compute-any-function/"
                       rel="bookmark"
                       title="Permalink to A visual proof that neural networks can compute any function">
                        A visual proof that neural networks can compute any function
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-11-23T00:00:00+05:30"> Thu 23 November 2017</time>
    </span>


            <span class="label label-default">By</span>
            <a href="/author/micheal-nelson.html"><i class="fa fa-user"></i> Micheal Nelson</a>

        <span class="label label-default">Category</span>
        <a href="/category/books/">books</a>

            <a href="/category/books/neuralnets/">/neuralNets</a>
        



<span class="label label-default">Tags</span>
	<a href="/tag/deep-learning/">deep-learning</a>
        /
	<a href="/tag/notes/">notes</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>

    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
<section class="well" id="related-posts">
    <h4>Part 5 of the Neural Networks and Deep Learning series</h4>
       <h5>Previous articles</h5>
       <ul>
           <li><a href="/posts/books/neural-networks-and-deep-learning/">Neural Networks and Deep Learning</a></li>
           <li><a href="/posts/books/using-neural-nets-to-recognize-handwritten-digits/">Using neural nets to recognize handwritten digits</a></li>
           <li><a href="/posts/books/how-the-backpropogation-algorithm-works/">How the backpropogation algorithm works</a></li>
           <li><a href="/posts/books/improving-the-way-neural-networks-learn/">Improving the way neural networks learn</a></li>
       </ul>
       <h5>Next articles</h5>
       <ul>
           <li><a href="/posts/books/why-are-deep-neural-networks-hard-to-train/">Why are deep neural networks hard to train?</a></li>
           <li><a href="/posts/books/deep-learning/">Deep Learning</a></li>
           <li><a href="/posts/books/is-there-a-simple-algorithm-for-intelligence/">Is there a simple algorithm for intelligence?</a></li>
       </ul>
</section>
                
                <p>Contents:</p>
                <nav class="toc">
                  <div id="toc"><ul><li><a class="toc-href" href="#chapter-4-a-visual-proof-that-neural-networks-can-compute-any-function" title="Chapter 4: A visual proof that neural networks can compute any function">Chapter 4: A visual proof that neural networks can compute any function</a><ul><li><a class="toc-href" href="#two-caveats" title="Two Caveats">Two Caveats</a></li><li><a class="toc-href" href="#universality-with-one-input-and-one-output" title="Universality with one input and one output">Universality with one input and one output</a></li><li><a class="toc-href" href="#many-input-variables" title="Many input variables">Many input variables</a></li><li><a class="toc-href" href="#extension-beyond-sigmoid-neurons" title="Extension beyond sigmoid neurons">Extension beyond sigmoid neurons</a></li><li><a class="toc-href" href="#fixing-up-the-step-function" title="Fixing up the step function">Fixing up the step function</a></li><li><a class="toc-href" href="#conclusion" title="Conclusion">Conclusion</a></li></ul></li></ul></div>
                </nav>
                <hr>
                
                <p>Notes for the <a href="http://neuralnetworksanddeeplearning.com/index.html">book</a>. <br/>
<a href="https://github.com/mnielsen/neural-networks-and-deep-learning">Source code</a> for the book.</p>
<h2 id="chapter-4-a-visual-proof-that-neural-networks-can-compute-any-function"><strong>Chapter 4: A visual proof that neural networks can compute any function</strong></h2>
<p>One of the most striking facts about neural networks is that they can compute any function at all. <br/>
That is, suppose someone hands you some complicated, wiggly function, <span class="math">\(f(x)\)</span>: <br/>
<img alt="" src="/images/nnfordl/4_fun1.png"/> </p>
<p>No matter what the function, there is guaranteed to be a neural network so that for every possible input, <span class="math">\(x\)</span>, the value <span class="math">\(f(x)\)</span> (or some close approximation) is output from the network. <br/>
This result holds even if the function has many inputs, <br/>
<img alt="" src="/images/nnfordl/4_fun2.png"/> </p>
<p>This result tells us that neural networks have a kind of universality. No matter what function we want to compute, we know that there is a neural network which can do the job.   </p>
<p>What's more, this universality theorem holds even if we restrict our networks to have just a single layer intermediate between the input and the output neurons - a so-called single hidden layer. So even very simple network architectures can be extremely powerful.   </p>
<p>However, the proofs for the universality of neural networks is not widely understood and the explanations are quite technical. Here we attempt to simplify the underlying intuition.   </p>
<p>Any process that we can do or imagine can be thought of as a function computation. Universality means that neural networks can do all these things, and many more.   </p>
<p>So, we know neural networks can compute things, but to find those networks we need learning algorithms!!   </p>
<h3 id="two-caveats">Two Caveats</h3>
<p>to the statement "a neural network can compute any function".   </p>
<ul>
<li>Firstly, This does not mean that the network can <em>exactly</em> compute any function. We can get an <em>approximation</em> that is as good as we want by increasing the number of hidden neurons.</li>
<li>Second, the class of functions that can be approximated in this way are the <em>continous</em> functions. However, the network can still compute a <em>continous</em> approximation of the underlying <em>discontinous</em> function.   </li>
</ul>
<h3 id="universality-with-one-input-and-one-output">Universality with one input and one output</h3>
<p>It turns out that this is the core of the problem of universality. Once we've understood this special case it's actually pretty easy to extend to functions with many inputs and many outputs.   </p>
<p>Here is an output of a single hidden neuron,  <br/>
<img alt="" src="/images/nnfordl/4_fun3.png"/> </p>
<p>Now, this single neuron can be treated as a step function with given parameters. <br/>
<img alt="" src="/images/nnfordl/4_fun4.png"/> </p>
<p>Working with this would actually be easier than a general sigmoid function. </p>
<p>With a little bit of work, we can see that the value at which the step occurs is <em>proportional</em> to <span class="math">\(b\)</span> and <em>inversely proportional</em> to <span class="math">\(w\)</span>. <br/>
In fact, the step is at position <span class="math">\(s = -b/w\)</span>.
<img alt="" src="/images/nnfordl/4_fun5.png"/> </p>
<p>Now, it will greatly simplify our lives if we describe the hidden neuron by a single parameter <span class="math">\(s = -b/w\)</span>. <br/>
<img alt="" src="/images/nnfordl/4_fun6.png"/> </p>
<p>Up to now we've been focusing on the output from just the top hidden neuron. Let's take a look at the behavior of the entire network. In particular, we'll suppose the hidden neurons are computing step functions parameterized by step points <span class="math">\(s_1\)</span> (top neuron) and <span class="math">\(s_2\)</span> (bottom neuron). And they'll have respective output weights <span class="math">\(w_1\)</span> and <span class="math">\(w_2\)</span>. <br/>
<img alt="" src="/images/nnfordl/4_fun7.png"/> </p>
<p><span class="math">\(weighted\,output = w_1a_1 + w_2a_2, $ where $a\)</span>s are the <em>activations</em>.   </p>
<p>Finally, let's on playing with the values, we can get a <em>bump</em> function, which starts at <span class="math">\(s_1\)</span>, ends at <span class="math">\(s_2\)</span>. <br/>
<img alt="" src="/images/nnfordl/4_fun8.png"/> </p>
<p>You'll notice, by the way, that we're using our neurons in a way that can be thought of not just in graphical terms, but in more conventional programming terms, as a kind of if-then-else statement, e.g.:   </p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="nb">input</span> <span class="o">&gt;=</span> <span class="n">step</span> <span class="n">point</span><span class="p">:</span>
        <span class="n">add</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">the</span> <span class="n">weighted</span> <span class="n">output</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">add</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">the</span> <span class="n">weighted</span> <span class="n">output</span>
</pre></div>
<p>Furthur, we can use our <em>bump-making</em> trick to get two bumps, by gluing together pairs of hidden neurons together, <br/>
<img alt="" src="/images/nnfordl/4_fun9.png"/> </p>
<p>More, generally, we can have <span class="math">\(N\)</span> peaks/bumps by using <span class="math">\(N\)</span> pairs of hidden neurons.
<img alt="" src="/images/nnfordl/4_fun10.png"/> </p>
<p>You may be able to see now where this is going, we are now <em>designing</em> a function by choosing various height values for the intervals.   </p>
<p>Going back to the function we saw earlier, <br/>
<img alt="" src="/images/nnfordl/4_fun1.png"/> </p>
<p>is actually, <br/>
</p>
<div class="math">$$\begin{align}f(x) = 0.2 + 0.4x^2+0.3x\sin(15x) + 0.05\cos(50x)\end{align}$$</div>
<p>That's obviously not a trivial function,   </p>
<p>n our networks above we've been analyzing the weighted combination <span class="math">\(\sum_j w_ja_j\)</span> output from the hidden neurons. We now know how to get a lot of control over this quantity. But, as I noted earlier, this quantity is not what's output from the network. What's output from the network is <span class="math">\(\sigma(\sum_j w_j a_j + b)\)</span> where <span class="math">\(b\)</span> is the bias on the output neuron. Is there some way we can achieve control over the actual output from the network?   </p>
<p>The solution is to design a neural network whose hidden layer has a weighted output given by <span class="math">\(\sigma_{&minus;1}\circ\,f(x)\)</span>, where <span class="math">\(\sigma^{-1}\)</span> is just the inverse of the <span class="math">\(\sigma\)</span> function. That is, we want the weighted output from the hidden layer to be: <br/>
<img alt="" src="/images/nnfordl/4_fun11.png"/> </p>
<p>If we can compute this function, the output from the network would be a good approximation for <span class="math">\(f(x)\)</span>.   </p>
<p>With some work, we can get this, minimizing the <em>average deviation</em> between the goal and the network output.  <br/>
<img alt="" src="/images/nnfordl/4_fun12.png"/> </p>
<p>This is only a coarse approximation, but we can do much better simply by increasing the number of hidden neurons, allowing more bumps.   </p>
<p>And, it is possible to convert all the data we found into standard parameterization  used for neural networks.   </p>
<p>What's more, there was nothing special about our original goal function <span class="math">\(f(x) = 0.2 + 0.4x^2+0.3x\sin(15x) + 0.05\cos(50x)\)</span>.
We could have used the same procedure for any continuos function from <span class="math">\([0, 1]\)</span> to <span class="math">\([0, 1]\)</span>.   </p>
<h3 id="many-input-variables">Many input variables</h3>
<p>We'll start by considering what happens when we have two inputs to a neuron:   </p>
<p><img alt="" src="/images/nnfordl/4_fun13_1.png"/>
<img alt="" src="/images/nnfordl/4_fun13.png"/> </p>
<p>As we can see, with <span class="math">\(w_2 = 0\)</span>, the input <span class="math">\(y\)</span> makes no difference to the output. It's as though there was only one input <span class="math">\(x\)</span>.   </p>
<p>Now, if you can recall, we are going to create steps, <br/>
<img alt="" src="/images/nnfordl/4_fun14_1.png"/>
<img alt="" src="/images/nnfordl/4_fun14.png"/> </p>
<p>Changing to a simpler parameter,  <br/>
<img alt="" src="/images/nnfordl/4_fun15_1.png"/>
<img alt="" src="/images/nnfordl/4_fun15.png"/> </p>
<p>Ofcourse, it is possible to create this step in the <span class="math">\(y\)</span> direction, <br/>
<img alt="" src="/images/nnfordl/4_fun16_1.png"/>
<img alt="" src="/images/nnfordl/4_fun16.png"/> </p>
<p>Now we are going to create a 3D bump function, <br/>
<img alt="" src="/images/nnfordl/4_fun17.png"/>
<img alt="" src="/images/nnfordl/4_fun17_1.png"/> </p>
<p>And, the same can be made in <span class="math">\(y\)</span> direction, <br/>
<img alt="" src="/images/nnfordl/4_fun18.png"/>
<img alt="" src="/images/nnfordl/4_fun18_1.png"/> </p>
<p>Let's see what happens when we combine the above functions,  <br/>
<img alt="" src="/images/nnfordl/4_fun19.png"/>
<img alt="" src="/images/nnfordl/4_fun19_1.png"/> </p>
<p>This can be tuned to create a <em>tower</em> function, <br/>
<img alt="" src="/images/nnfordl/4_fun20_1.png"/>
<img alt="" src="/images/nnfordl/4_fun20.png"/> </p>
<p>And the <em>towers</em> can be combined to form, <br/>
<img alt="" src="/images/nnfordl/4_fun21.png"/> </p>
<p>This, can be expressed as ,   </p>
<div class="highlight"><pre><span></span> <span class="k">if</span> <span class="n">combined</span> <span class="n">output</span> <span class="kn">from</span> <span class="nn">hidden</span> <span class="nn">neurons</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="n">output</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output</span> <span class="mi">0</span>
</pre></div>
<p>The same approach can be extended to higher dimensions(networks with more input variables).   </p>
<p>Okay, so we now know how to use neural networks to approximate a real-valued function of many variables. What about vector-valued functions <span class="math">\(f(x_1, ... ,x_m) \in R^n\)</span>? Of course, such a function can be regarded as just <span class="math">\(n\)</span> separate real-valued functions, <span class="math">\(f_1(x_1, ... ,x_m),f_2(x_1,...,x_m)\)</span>, and so on. So we create a network approximating <span class="math">\(f_1\)</span>, another network for <span class="math">\(f_2\)</span>, and so on. And then we simply glue all the networks together. So that's also easy to cope with.   </p>
<h3 id="extension-beyond-sigmoid-neurons">Extension beyond sigmoid neurons</h3>
<p>We've proved that networks made up of sigmoid neurons can compute any function. Recall that in a sigmoid neuron the inputs <span class="math">\(x_1,x_2,...\)</span> result in the output <span class="math">\(\sigma (\sum_j w_j x_j + b)\)</span>, where <span class="math">\(w_j\)</span> are the weights, <span class="math">\(b\)</span> is the bias, and <span class="math">\(\sigma\)</span> is the sigmoid function:     </p>
<p><img alt="" src="/images/nnfordl/4_fun22.png"/> </p>
<p>What if we consider a different type of neuron, one using some other activation function <span class="math">\(s(x)\)</span>: <br/>
<img alt="" src="/images/nnfordl/4_fun23.png"/> </p>
<p>This can still produce a step function given appropriate weights, <br/>
<img alt="" src="/images/nnfordl/4_fun24.png"/> </p>
<p>So, what properties does <span class="math">\(s(z)\)</span> need to assume that this works. We need to assume that <span class="math">\(s(z)\)</span> is well-defined as <span class="math">\(z \to -\infty\)</span> and <span class="math">\(z \to \infty\)</span>. These two limits are the two values taken on by our step function. We also need to assume that these two limits are different from one another. If they weren't, there would be no step, simple a flat graph!   </p>
<h3 id="fixing-up-the-step-function">Fixing up the step function</h3>
<p>Upto this point, we have made some pretty good approximations of the function, but it is only an approximation. 
There would be a narrow window of failure. </p>
<p><img alt="" src="/images/nnfordl/4_fun25.png"/> </p>
<p>Now, these are not terrible failures, we can make the weights of input neurons big enough to make this window smaller.</p>
<p>Again, let's assume we want to approximate a function <span class="math">\(f\)</span>. We would try to design pur network so that the 
weighted output from our hidden layer of neurons is <span class="math">\(\sigma^{-1} \circ f(x)\)</span>:  <br/>
<img alt="" src="/images/nnfordl/4_fun26.png"/> </p>
<p>Using the technique described earlier, we would use hidden neurons to produce a sequence of bumps like this : <br/>
<img alt="" src="/images/nnfordl/4_fun27.png"/> </p>
<p>Now, suppose we build another set of hidden neurons to compute the approximation of <span class="math">\(\sigma^{-1} \circ f(x)\)</span>, but with the bases of the bumps <em>shifted</em> by half the width of the bump:
<img alt="" src="/images/nnfordl/4_fun28.png"/> </p>
<p>Now, if we combine the two approximations, the overall approximation will still have windows of failure, but these windows would be still smaller. Further, the approximation will be 2 times better in those windows.   </p>
<p>We can do even better by combining a large number of overlapping approximations.   </p>
<h3 id="conclusion">Conclusion</h3>
<p>The above discussion and proof of universality is certainly very crude, however it provides a good intuition.    </p>
<p>So, this takes off the table the question of whether any particular function is computable using a neural network. 
The answer is always <em>yes</em>. The right question to ask is, what's a <em>good</em> way to compute the function.   </p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "true";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

            
            </div>
            <!-- /.entry-content -->
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.facebook.com/npuri1903"><i class="fa fa-facebook-square fa-lg"></i> Facebook</a></li>
    <li class="list-group-item"><a href="https://github.com/nitishpuri"><i class="fa fa-github-square fa-lg"></i> Github</a></li>
    <li class="list-group-item"><a href="https://www.linkedin.com/in/nitishpuri/"><i class="fa fa-linkedin-square fa-lg"></i> Linkedin</a></li>
    <li class="list-group-item"><a href="https://www.instagram.com/purinitish/"><i class="fa fa-instagram fa-lg"></i> Instagram</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="/category/articles/"><i class="fa fa-folder-open fa-lg"></i>articles</a>
    </li>
    <li class="list-group-item">
      <a href="/category/books/"><i class="fa fa-folder-open fa-lg"></i>books</a>
    </li>
    <li class="list-group-item">
      <a href="/category/gallery/"><i class="fa fa-folder-open fa-lg"></i>gallery</a>
    </li>
    <li class="list-group-item">
      <a href="/category/machine-intelligence/"><i class="fa fa-folder-open fa-lg"></i>machine-intelligence</a>
    </li>
    <li class="list-group-item">
      <a href="/category/robotics/"><i class="fa fa-folder-open fa-lg"></i>robotics</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-1">
      <a href="/tag/algorithms/">algorithms</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/blogging/">blogging</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/book/">book</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/data-science/">data-science</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/deep-dream/">deep dream</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/deep-learning/">deep-learning</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/design/">design</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/gallery/">gallery</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/generative/">generative</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/graphics/">graphics</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/image-segmentation/">image-segmentation</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/imagenet/">imagenet</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/jekyll/">jekyll</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/notes/">notes</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/opengl/">opengl</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/philosophy/">philosophy</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/programming/">programming</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/projects/">projects</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/robotics/">robotics</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/sketch/">sketch</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/style-transfer/">style-transfer</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/udacity/">udacity</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Series -->
<li class="list-group-item">
  <h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Series</span></h4>
  <ul class="list-group">
    <li class="list-group-item">
      <h5></i>Previous article</h5>
      <a href="/posts/books/improving-the-way-neural-networks-learn/">Improving the way neural networks learn</a>
    </li>
    <li class="list-group-item">
      <h5>Next article</h5>
      <a href="/posts/books/why-are-deep-neural-networks-hard-to-train/">Why are deep neural networks hard to train?</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Series -->


<!-- Sidebar/Github -->
<li class="list-group-item">
  <h4><i class="fa fa-github fa-lg"></i><span class="icon-label">GitHub Repos</span></h4>
  <div id="gh_repos">
    <p class="list-group-item">Status updating...</p>
  </div>
</li>
<!-- End Sidebar/Github -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2018 Nitish Puri
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>


<!-- GitHub JS Code -->
<script type="text/javascript">
$(document).ready(function () {
  if (!window.jXHR) {
    var jxhr = document.createElement('script');
    jxhr.type = 'text/javascript';
    jxhr.src = '/theme/js/jXHR.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(jxhr, s);
  }

  github.showRepos({
    user: 'nitishpuri',
    count: 5,
    skip_forks: true,
    target: '#gh_repos'
  });
});
</script>
<script src="/theme/js/github.js" type="text/javascript"></script>
<!-- End GitHub JS Code -->
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-103032011-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->


    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59bc69ad3aa13e47"></script>
</body>
</html>