<!DOCTYPE html>
<html lang="english" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>How the backpropogation algorithm works - nitishpuri.github.io</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="/posts/books/how-the-backpropogation-algorithm-works/">

        <meta name="author" content="Micheal Nelson" />
        <meta name="keywords" content="deep-learning,notes" />
        <meta name="description" content="Neural Networks and Deep Learning, Chapter 2" />

        <meta property="og:site_name" content="nitishpuri.github.io" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="How the backpropogation algorithm works"/>
        <meta property="og:url" content="/posts/books/how-the-backpropogation-algorithm-works/"/>
        <meta property="og:description" content="Neural Networks and Deep Learning, Chapter 2"/>
        <meta property="article:published_time" content="2017-09-22" />
            <meta property="article:section" content="books" />
                <meta property="article:subcategory" content="books/neuralNets" />
            <meta property="article:tag" content="deep-learning" />
            <meta property="article:tag" content="notes" />
            <meta property="article:author" content="Micheal Nelson" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.cosmo.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/native.css" rel="stylesheet">
    <link href="/theme/tipuesearch/tipuesearch.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>
        <link href="/static/custom.css" rel="stylesheet" type="text/css" />





</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
nitishpuri.github.io            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/pages/bio/">Bio</a></li>
                    <li><a href="/pages/gallery/">Gallery</a></li>
                    <li><a href="/pages/books/">Books</a></li>
                    <li><a href="https://nitishpuri.github.io/ProcessingExperiments/">Demos</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><span>
                <form class="navbar-search" action="/search.html">
                  <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input" required>
                </form></span>
              </li>
              <li><a href="/archives/"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/posts/books/how-the-backpropogation-algorithm-works/"
                       rel="bookmark"
                       title="Permalink to How the backpropogation algorithm works">
                        How the backpropogation algorithm works
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-09-22T00:00:00+05:30"> Fri 22 September 2017</time>
    </span>


            <span class="label label-default">By</span>
            <a href="/author/micheal-nelson.html"><i class="fa fa-user"></i> Micheal Nelson</a>

        <span class="label label-default">Category</span>
        <a href="/category/books/">books</a>

            <a href="/category/books/neuralnets/">/neuralNets</a>
        



<span class="label label-default">Tags</span>
	<a href="/tag/deep-learning/">deep-learning</a>
        /
	<a href="/tag/notes/">notes</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>

    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
<section class="well" id="related-posts">
    <h4>Part 3 of the Neural Networks and Deep Learning series</h4>
       <h5>Previous articles</h5>
       <ul>
           <li><a href="/posts/books/neural-networks-and-deep-learning/">Neural Networks and Deep Learning</a></li>
           <li><a href="/posts/books/using-neural-nets-to-recognize-handwritten-digits/">Using neural nets to recognize handwritten digits</a></li>
       </ul>
       <h5>Next articles</h5>
       <ul>
           <li><a href="/posts/books/improving-the-way-neural-networks-learn/">Improving the way neural networks learn</a></li>
           <li><a href="/posts/books/a-visual-proof-that-neural-networks-can-compute-any-function/">A visual proof that neural networks can compute any function</a></li>
           <li><a href="/posts/books/why-are-deep-neural-networks-hard-to-train/">Why are deep neural networks hard to train?</a></li>
           <li><a href="/posts/books/deep-learning/">Deep Learning</a></li>
           <li><a href="/posts/books/is-there-a-simple-algorithm-for-intelligence/">Is there a simple algorithm for intelligence?</a></li>
       </ul>
</section>
                
                <p>Contents:</p>
                <nav class="toc">
                  <div id="toc"><ul><li><a class="toc-href" href="#chapter-2-how-the-backpropogation-algorithm-works" title="Chapter 2: How the backpropogation algorithm works">Chapter 2: How the backpropogation algorithm works</a><ul><li><a class="toc-href" href="#warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network" title="Warm up: a fast matrix-based approach to computing the output from a neural network">Warm up: a fast matrix-based approach to computing the output from a neural network</a></li><li><a class="toc-href" href="#the-two-assumptions-we-need-about-the-cost-function" title="The two assumptions we need about the cost function">The two assumptions we need about the cost function</a></li><li><a class="toc-href" href="#the-hadamard-product-s-odot-t" title="The Hadamard product, \(s \odot t\)">The Hadamard product, \(s \odot t\)</a></li><li><a class="toc-href" href="#the-four-fundamental-equations-behind-backpropagation" title="The four fundamental equations behind backpropagation">The four fundamental equations behind backpropagation</a></li><li><a class="toc-href" href="#proof-of-the-four-fundamental-equations" title="Proof of the four fundamental equations">Proof of the four fundamental equations</a></li><li><a class="toc-href" href="#the-backpropagation-algorithm" title="The backpropagation algorithm">The backpropagation algorithm</a></li><li><a class="toc-href" href="#the-code-for-backpropagation" title="The code for backpropagation">The code for backpropagation</a></li><li><a class="toc-href" href="#in-what-sense-is-backpropagation-a-fast-algorithm" title="In what sense is backpropagation a fast algorithm?">In what sense is backpropagation a fast algorithm?</a></li><li><a class="toc-href" href="#backpropagation-the-big-picture" title="Backpropagation: the big picture">Backpropagation: the big picture</a></li></ul></li></ul></div>
                </nav>
                <hr>
                
                <p>Notes for the <a href="http://neuralnetworksanddeeplearning.com/index.html">book</a>. <br/>
<a href="https://github.com/mnielsen/neural-networks-and-deep-learning">Source code</a> for the book.</p>
<h2 id="chapter-2-how-the-backpropogation-algorithm-works"><strong>Chapter 2: How the backpropogation algorithm works</strong></h2>
<p>Was introduced in the 70's, but came into light with <a href="https://www.nature.com/nature/journal/v323/n6088/pdf/323533a0.pdf">this paper</a>.   </p>
<p>Today, it is the workhorse of learning in neural networks.   </p>
<h3 id="warm-up-a-fast-matrix-based-approach-to-computing-the-output-from-a-neural-network"><strong>Warm up: a fast matrix-based approach to computing the output from a neural network</strong></h3>
<p>First, the notations, <br/>
For weights, <br/>
<img alt="" src="/images/nnfordl/backprop1.png"/> </p>
<p>For biases and activations, <br/>
<img alt="" src="/images/nnfordl/backprop2.png"/> </p>
<p>These are related, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  a^{l}_j = \sigma\left( \sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \right),
\tag{23}\end{eqnarray}$$</div>
<p>which can be rewritten in <em>vectorized</em> form as, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  a^{l} = \sigma(w^l a^{l-1}+b^l).
\tag{25}\end{eqnarray}$$</div>
<p>This form is more compact and practical as we will be using libraries that provide fast matrix multiplication and vectorization capabilities.   </p>
<h3 id="the-two-assumptions-we-need-about-the-cost-function"><strong>The two assumptions we need about the cost function</strong></h3>
<p>The goal of backpropogation is to calculate the partial derivatives <span class="math">\(\partial C / \partial w\)</span> and <span class="math">\(\partial C / \partial b\)</span>.   </p>
<p>Here is an example of cost function we will be using(there can and will be others). <br/>
</p>
<div class="math">$$\begin{eqnarray}
  C = \frac{1}{2n} \sum_x \|y(x)-a^L(x)\|^2,
\tag{26}\end{eqnarray}$$</div>
<p>Now, the assumptions,   </p>
<ol>
<li>The cost function can be written as an average <span class="math">\(C = \frac{1}{n} \sum_x C_x\)</span> over cost <span class="math">\(C_x\)</span> for individual training examples.   </li>
<li>The cost function can be written as  a function of the outputs from the neural network:
<img alt="" src="/images/nnfordl/backprop3.png"/> </li>
</ol>
<div class="math">$$\begin{eqnarray}
  C = \frac{1}{2} \|y-a^L\|^2 = \frac{1}{2} \sum_j (y_j-a^L_j)^2,
\tag{27}\end{eqnarray}$$</div>
<h3 id="the-hadamard-product-s-odot-t"><strong>The Hadamard product, <span class="math">\(s \odot t\)</span></strong></h3>
<p><span class="math">\(s \odot t\)</span> represents the <em>elementwise</em> product of two vectors. <br/>
</p>
<div class="math">$$\begin{eqnarray}
\left[\begin{array}{c} 1 \\ 2 \end{array}\right] 
  \odot \left[\begin{array}{c} 3 \\ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 \\ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 \\ 8 \end{array} \right].
\tag{28}\end{eqnarray}$$</div>
<h3 id="the-four-fundamental-equations-behind-backpropagation"><strong>The four fundamental equations behind backpropagation</strong></h3>
<p>First, we define the <em>error</em> in the <span class="math">\(j^{th}\)</span> neuron in the <span class="math">\(l^{th}\)</span> layer, <span class="math">\(\delta^l_j\)</span> <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \delta^l_j \equiv \frac{\partial C}{\partial z^l_j}.
\tag{29}\end{eqnarray}$$</div>
<p><strong>An equation for the error in the output layer,</strong> <span class="math">\(\delta^L\)</span>: <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j).
\tag{BP1}\end{eqnarray}$$</div>
<p>Which can again be rewritten in vectorized form, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \delta^L = \nabla_a C \odot \sigma'(z^L).
\tag{BP1a}\end{eqnarray}$$</div>
<p>where, in case of a quadratic cost function, we have <span class="math">\(\nabla_a C = (a^L-y)\)</span>.So,  <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \delta^L = (a^L-y) \odot \sigma'(z^L).
\tag{30}\end{eqnarray}$$</div>
<p><strong>An equation for the error <span class="math">\(\delta^l\)</span> in terms of the error in the next layer,</strong> <span class="math">\(\delta^{l+1}\)</span>: <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l),
\tag{BP2}\end{eqnarray}$$</div>
<p>Suppose we know the error <span class="math">\(\delta^{l+1}\)</span> at the <span class="math">\(l+q^{\rm th}\)</span> layer. When we apply the transpose weight matrix, <span class="math">\((w^{l+1})^T\)</span>, we can think intuitively of this as moving the error <em>backward</em> through the network, giving us some sort of measure of the error at the output of the <span class="math">\(l^{\rm th}\)</span> layer.   </p>
<p>By combining <span class="math">\((BP1)\)</span> and <span class="math">\((BP2)\)</span>, we can compute the error <span class="math">\(\delta^l\)</span> for any layer in the network.   </p>
<p><strong>An equation for the rate of change of the cost with respect to any bias in the network</strong> </p>
<div class="math">$$\begin{eqnarray}  \frac{\partial C}{\partial b^l_j} =
  \delta^l_j.
\tag{BP3}\end{eqnarray}$$</div>
<p>which can also be written as, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \frac{\partial C}{\partial b} = \delta,
\tag{31}\end{eqnarray}$$</div>
<p><strong>An equation for the rate of change of the cost with respect to any weight in the network:</strong> </p>
<div class="math">$$\begin{eqnarray}
  \frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j.
\tag{BP4}\end{eqnarray}$$</div>
<p>which can also be written as,   </p>
<div class="math">$$\begin{eqnarray}  \frac{\partial
    C}{\partial w} = a_{\rm in} \delta_{\rm out},
\tag{32}\end{eqnarray}$$</div>
<p>This can also be depicted as, <br/>
<img alt="" src="/images/nnfordl/backprop4.png"/> </p>
<p>Looking at this image, we can also say that a weight will learn slowly if either the input neuron is low-activation, or if the output neuron has <em>saturated</em>, i.e. it's gradient has become too small(when its either high- or low-activation in case of <em>sigmoid</em>).   </p>
<p><strong>Summary of the four equations of backpropagation</strong> <br/>
<img alt="" src="/images/nnfordl/backprop5.png"/> </p>
<h3 id="proof-of-the-four-fundamental-equations"><strong>Proof of the four fundamental equations</strong></h3>
<p>We start with the expression for <span class="math">\(\delta^L\)</span> <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial z^L_j}.
\tag{36}\end{eqnarray}$$</div>
<p>Applying chain rule, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \delta^L_j = \sum_k \frac{\partial C}{\partial a^L_k} \frac{\partial a^L_k}{\partial z^L_j},
\tag{37}\end{eqnarray}$$</div>
<p>But, the output activation <span class="math">\(a_k^L\)</span> of the <span class="math">\(k_{\rm th}\)</span> neuron only depends on the weighted input <span class="math">\(x_j^L\)</span> for the <span class="math">\(j^{\rm th}\)</span> neuron when <span class="math">\(k=j\)</span>. And so, <span class="math">\(\partial a^L_k / \partial z^L_j\)</span> vanishes when <span class="math">\(k \neq j\)</span>. So,   </p>
<div class="math">$$\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \frac{\partial a^L_j}{\partial z^L_j}.
\tag{38}\end{eqnarray}$$</div>
<p>Recalling that, <span class="math">\(a^L_j = \sigma(z^L_j)\)</span> <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \delta^L_j = \frac{\partial C}{\partial a^L_j} \sigma'(z^L_j),
\tag{39}\end{eqnarray}$$</div>
<p>which is <span class="math">\((BP1)\)</span> in component form.   </p>
<p>Next, we prove <span class="math">\((BP2)\)</span>, which gives an equation for the error <span class="math">\(\delta^L\)</span> in terms of the error in the next layer, <span class="math">\(\delta^{l+1}\)</span>. 
</p>
<div class="math">$$\begin{eqnarray}
  \delta^l_j &amp; = &amp; \frac{\partial C}{\partial z^l_j} \tag{40}\\
  &amp; = &amp; \sum_k \frac{\partial C}{\partial z^{l+1}_k} \frac{\partial z^{l+1}_k}{\partial z^l_j} \tag{41}\\ 
  &amp; = &amp; \sum_k \frac{\partial z^{l+1}_k}{\partial z^l_j} \delta^{l+1}_k,
\tag{42}\end{eqnarray}$$</div>
<p>Now, 
</p>
<div class="math">$$\begin{eqnarray}
  z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j +b^{l+1}_k = \sum_j w^{l+1}_{kj} \sigma(z^l_j) +b^{l+1}_k.
\tag{43}\end{eqnarray}$$</div>
<p>Differentiating,   </p>
<div class="math">$$\begin{eqnarray}
  \frac{\partial z^{l+1}_k}{\partial z^l_j} = w^{l+1}_{kj} \sigma'(z^l_j).
\tag{44}\end{eqnarray}$$</div>
<p>Substituting back in <span class="math">\((42)\)</span>, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \delta^l_j = \sum_k w^{l+1}_{kj}  \delta^{l+1}_k \sigma'(z^l_j).
\tag{45}\end{eqnarray}$$</div>
<p>which is <span class="math">\((BP2)\)</span> in component form.   </p>
<h3 id="the-backpropagation-algorithm"><strong>The backpropagation algorithm</strong></h3>
<ol>
<li><strong>Input <span class="math">\(x\)</span>:</strong> Set the corresponding activation <span class="math">\(a^1\)</span> for the input layer.</li>
<li><strong>Feedforward:</strong> For each <span class="math">\(l = 2, 3, \ldots, L\)</span> compute <span class="math">\(z^l = w^la^{l&minus;1} + b^l\)</span> and <span class="math">\(a^l = \sigma(z^l)\)</span>.</li>
<li><strong>Output error <span class="math">\(\delta^L\)</span>:</strong> Compute the vector <span class="math">\(\delta^{L} = \nabla_a C \odot \sigma'(z^L)\)</span>.</li>
<li><strong>Backpropagate the error:</strong> For each <span class="math">\(l = L&minus;1, L&minus;2,\ldots, 2\)</span> compute <span class="math">\(\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})\)</span>.</li>
<li><strong>Output:</strong> The gradient of the cost function is given by <span class="math">\(\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j\)</span> and <span class="math">\(\frac{\partial C}{\partial b^l_j} = \delta^l_j\)</span>.</li>
</ol>
<p>The above steps show <em>back</em>propagation applied w.r.t. a single training example. In practice, it is common to use some a learning algorithm like <em>stochastic gradient descent</em>, computing the gradients of many training examples. In particular, the following algorithm applies gradient descent learning step, based on mini-batches.</p>
<ol>
<li><strong>Input a set of training examples</strong></li>
<li><strong>For each training example <span class="math">\(x\)</span>:</strong> Set the corresponding input activation <span class="math">\(a^{x,1}\)</span> and perform the following steps:<ul>
<li><strong>Feedforward:</strong> For each <span class="math">\(l = 2, 3, \ldots, L\)</span> compute <span class="math">\(z^{x,l} = w^l a^{x,l-1}+b^l\)</span> and <span class="math">\(a^{x,l} = \sigma(z^{x,l})\)</span></li>
<li><strong>Output error <span class="math">\(\delta^{x,L}\)</span>:</strong> Compute the vector <span class="math">\(\delta^{x,L} = \nabla_a C_x \odot \sigma'(z^{x,L})\)</span>.</li>
<li><strong>Backpropagate the error:</strong> For each <span class="math">\(l = L-1, L-2, \ldots, 2\)</span> compute <span class="math">\(\delta^{x,l} = ((w^{l+1})^T \delta^{x,l+1}) \odot \sigma'(z^{x,l})\)</span></li>
</ul>
</li>
<li><strong>Gradient descent:</strong> For each <span class="math">\(l = L, L-1, \ldots, 2\)</span> update the weights according to the rule <span class="math">\(w^l \rightarrow w^l-\frac{\eta}{m} \sum_x \delta^{x,l} (a^{x,l-1})^T\)</span>, and biases according to the rule <span class="math">\(b^l \rightarrow b^l-\frac{\eta}{m} \sum_x \delta^{x,l}\)</span>.</li>
</ol>
<p>Of course, in practice, we would also need an outer loop, generating the mini-batches, and another outer loop, stepping through the multiple epochs, but they are just ommitted for simplicity.   </p>
<h3 id="the-code-for-backpropagation"><strong>The code for backpropagation</strong></h3>
<p>The code for backpropagation was is contained in two methods, <code>update_mini_batch</code>, described in the last post, and <code>backprop</code> method, discussed here,   </p>
<div class="highlight"><pre><span></span>  <span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
      <span class="sd">"""Return a tuple "(nabla_b, nabla_w)" representing the</span>
<span class="sd">      gradient for the cost function C_x.  "nabla_b" and</span>
<span class="sd">      "nabla_w" are layer-by-layer lists of numpy arrays, similar</span>
<span class="sd">      to "self.biases" and "self.weights"."""</span>
      <span class="n">nabla_b</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">]</span>
      <span class="n">nabla_w</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">]</span>
      <span class="c1"># feedforward</span>
      <span class="n">activation</span> <span class="o">=</span> <span class="n">x</span>
      <span class="n">activations</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="c1"># list to store all the activations, layer by layer</span>
      <span class="n">zs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># list to store all the z vectors, layer by layer</span>
      <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">biases</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">):</span>
          <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">activation</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
          <span class="n">zs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
          <span class="n">activation</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
          <span class="n">activations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
      <span class="c1"># backward pass</span>
      <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cost_derivative</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> \
          <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
      <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
      <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
      <span class="c1"># Note that the variable l in the loop below is used a little</span>
      <span class="c1"># differently to the notation in Chapter 2 of the book.  Here,</span>
      <span class="c1"># l = 1 means the last layer of neurons, l = 2 is the</span>
      <span class="c1"># second-last layer, and so on.  It's a renumbering of the</span>
      <span class="c1"># scheme in the book, used here to take advantage of the fact</span>
      <span class="c1"># that Python can use negative indices in lists.</span>
      <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
          <span class="n">z</span> <span class="o">=</span> <span class="n">zs</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span>
          <span class="n">sp</span> <span class="o">=</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
          <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span> <span class="n">delta</span><span class="p">)</span> <span class="o">*</span> <span class="n">sp</span>
          <span class="n">nabla_b</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">delta</span>
          <span class="n">nabla_w</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">activations</span><span class="p">[</span><span class="o">-</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
      <span class="k">return</span> <span class="p">(</span><span class="n">nabla_b</span><span class="p">,</span> <span class="n">nabla_w</span><span class="p">)</span>
</pre></div>
<h3 id="in-what-sense-is-backpropagation-a-fast-algorithm"><strong>In what sense is backpropagation a fast algorithm?</strong></h3>
<p>Let's consider another approach, using numerical gradients, <br/>
</p>
<div class="math">$$\begin{eqnarray}  \frac{\partial
    C}{\partial w_{j}} \approx \frac{C(w+\epsilon
    e_j)-C(w)}{\epsilon},
\tag{46}\end{eqnarray}$$</div>
<p>Gradients for the biases can be computed similarly.   </p>
<p>This looks promising. It's conceptually simple and easy to implement. <br/>
Certainly much more promising than the chain rule to compute the gradient!   </p>
<p>However, turns out it is extremely slow. That's because for each distinct weight <span class="math">\(w_j\)</span> we need to compute <span class="math">\(C(w+\epsilon e_j)\)</span> in order to compute <span class="math">\(\delta C/\delta w_j\)</span>.   </p>
<p>That means, if we have a million weights, to compute the gradient, we need to calculate the cost a million different times, requiring a million forward passes through the network. </p>
<p>Backpropagation enables us to simultaneously calculate <em>all</em> the partial derivatives using just one single forward pass through the network, followed by one backward pass. Which is roughly the same as just two forward passes through the network. And so even though backpropagation appears superficially complex, it is much, much faster.  </p>
<p>But even that speedup was not enough in the early days, and we need some more clever ideas to make deeper networks work.   </p>
<h3 id="backpropagation-the-big-picture"><strong>Backpropagation: the big picture</strong></h3>
<p>So, we have seen what backprop does, and the steps that it takes. But does that give an intuitive idea of how it does what it does? </p>
<p>To improve our intuition, <br/>
let's imagine that we've made a small change <span class="math">\(\Delta{w_{jk}^l}\)</span> to some weight in the network.   </p>
<p><img alt="" src="/images/nnfordl/backprop6.png"/> <br/>
That will cause a change in its output activation, <br/>
<img alt="" src="/images/nnfordl/backprop7.png"/> <br/>
which will cause a change in <em>all</em> the activations in the next layer. <br/>
<img alt="" src="/images/nnfordl/backprop8.png"/> <br/>
and it would propagate to the final cost function. <br/>
<img alt="" src="/images/nnfordl/backprop9.png"/> </p>
<p>This change can also be written mathematically as, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \Delta C \approx \frac{\partial C}{\partial w^l_{jk}} \Delta w^l_{jk}.
\tag{47}\end{eqnarray}$$</div>
<p>This suggests if we propagate the change <span class="math">\(\Delta w^l_{jk}\)</span> forward, we should be able to compute <span class="math">\(\delta{C}/\delta w^l_{jk}\)</span>.   </p>
<p>Let's try that out, <br/>
The change <span class="math">\(\Delta w^l_{jk}\)</span> causes a small change <span class="math">\(\Delta a_l_j\)</span> in the activation of the <span class="math">\(j^{th}\)</span> neuron in the <span class="math">\(l^{th}\)</span> layer. <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \Delta a^l_j \approx \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}.
\tag{48}\end{eqnarray}$$</div>
<p>This will cause change in <em>all</em> the activations in the next layer. We'll concentrate on just a single such activation. <br/>
<img alt="" src="/images/nnfordl/backprop10.png"/> </p>
<p>Which can be written as, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \Delta a^{l+1}_q \approx \frac{\partial a^{l+1}_q}{\partial a^l_j} \Delta a^l_j.
\tag{49}\end{eqnarray}$$</div>
<p>Substituting in <span class="math">\((48)\)</span>, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \Delta a^{l+1}_q \approx \frac{\partial a^{l+1}_q}{\partial a^l_j} \frac{\partial a^l_j}{\partial w^l_{jk}} \Delta w^l_{jk}.
\tag{50}\end{eqnarray}$$</div>
<p>And, following similar patterns, it propagates to the change in final cost <span class="math">\(\Delta C\)</span>. <br/>
<img alt="" src="/images/nnfordl/backprop11.png"/> </p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "true";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

            
            </div>
            <!-- /.entry-content -->
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.facebook.com/npuri1903"><i class="fa fa-facebook-square fa-lg"></i> Facebook</a></li>
    <li class="list-group-item"><a href="https://github.com/nitishpuri"><i class="fa fa-github-square fa-lg"></i> Github</a></li>
    <li class="list-group-item"><a href="https://www.linkedin.com/in/nitishpuri/"><i class="fa fa-linkedin-square fa-lg"></i> Linkedin</a></li>
    <li class="list-group-item"><a href="https://www.instagram.com/purinitish/"><i class="fa fa-instagram fa-lg"></i> Instagram</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="/category/articles/"><i class="fa fa-folder-open fa-lg"></i>articles</a>
    </li>
    <li class="list-group-item">
      <a href="/category/books/"><i class="fa fa-folder-open fa-lg"></i>books</a>
    </li>
    <li class="list-group-item">
      <a href="/category/gallery/"><i class="fa fa-folder-open fa-lg"></i>gallery</a>
    </li>
    <li class="list-group-item">
      <a href="/category/machine-intelligence/"><i class="fa fa-folder-open fa-lg"></i>machine-intelligence</a>
    </li>
    <li class="list-group-item">
      <a href="/category/robotics/"><i class="fa fa-folder-open fa-lg"></i>robotics</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-1">
      <a href="/tag/algorithms/">algorithms</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/architecture/">architecture</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/blogging/">blogging</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/book/">book</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/data-science/">data-science</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/deep-dream/">deep dream</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/deep-learning/">deep-learning</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/design/">design</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/gallery/">gallery</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/game-engine/">game engine</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/generative/">generative</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/graphics/">graphics</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/image-segmentation/">image-segmentation</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/imagenet/">imagenet</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/jekyll/">jekyll</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/notes/">notes</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/opengl/">opengl</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/philosophy/">philosophy</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/programming/">programming</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/projects/">projects</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/robotics/">robotics</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/sketch/">sketch</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/style-transfer/">style-transfer</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/udacity/">udacity</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Series -->
<li class="list-group-item">
  <h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Series</span></h4>
  <ul class="list-group">
    <li class="list-group-item">
      <h5></i>Previous article</h5>
      <a href="/posts/books/using-neural-nets-to-recognize-handwritten-digits/">Using neural nets to recognize handwritten digits</a>
    </li>
    <li class="list-group-item">
      <h5>Next article</h5>
      <a href="/posts/books/improving-the-way-neural-networks-learn/">Improving the way neural networks learn</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Series -->


<!-- Sidebar/Github -->
<li class="list-group-item">
  <h4><i class="fa fa-github fa-lg"></i><span class="icon-label">GitHub Repos</span></h4>
  <div id="gh_repos">
    <p class="list-group-item">Status updating...</p>
  </div>
</li>
<!-- End Sidebar/Github -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2018 Nitish Puri
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>


<!-- GitHub JS Code -->
<script type="text/javascript">
$(document).ready(function () {
  if (!window.jXHR) {
    var jxhr = document.createElement('script');
    jxhr.type = 'text/javascript';
    jxhr.src = '/theme/js/jXHR.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(jxhr, s);
  }

  github.showRepos({
    user: 'nitishpuri',
    count: 5,
    skip_forks: true,
    target: '#gh_repos'
  });
});
</script>
<script src="/theme/js/github.js" type="text/javascript"></script>
<!-- End GitHub JS Code -->
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-103032011-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->


    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59bc69ad3aa13e47"></script>
</body>
</html>