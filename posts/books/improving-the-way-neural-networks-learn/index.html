<!DOCTYPE html>
<html lang="english" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Improving the way neural networks learn - nitishpuri.github.io</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="/posts/books/improving-the-way-neural-networks-learn/">

        <meta name="author" content="Micheal Nelson" />
        <meta name="keywords" content="deep-learning,notes" />
        <meta name="description" content="Neural Networks and Deep Learning, Chapter 3" />

        <meta property="og:site_name" content="nitishpuri.github.io" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="Improving the way neural networks learn"/>
        <meta property="og:url" content="/posts/books/improving-the-way-neural-networks-learn/"/>
        <meta property="og:description" content="Neural Networks and Deep Learning, Chapter 3"/>
        <meta property="article:published_time" content="2017-09-23" />
            <meta property="article:section" content="books" />
                <meta property="article:subcategory" content="books/neuralNets" />
            <meta property="article:tag" content="deep-learning" />
            <meta property="article:tag" content="notes" />
            <meta property="article:author" content="Micheal Nelson" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="/theme/css/bootstrap.cosmo.min.css" type="text/css"/>
    <link href="/theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="/theme/css/pygments/native.css" rel="stylesheet">
    <link href="/theme/tipuesearch/tipuesearch.css" rel="stylesheet">
    <link rel="stylesheet" href="/theme/css/style.css" type="text/css"/>
        <link href="/static/custom.css" rel="stylesheet" type="text/css" />





</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
	<div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="/" class="navbar-brand">
nitishpuri.github.io            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                    <li><a href="/pages/bio/">Bio</a></li>
                    <li><a href="/pages/gallery/">Gallery</a></li>
                    <li><a href="/pages/books/">Books</a></li>
                    <li><a href="https://nitishpuri.github.io/ProcessingExperiments/">Demos</a></li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
              <li><span>
                <form class="navbar-search" action="/search.html">
                  <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input" required>
                </form></span>
              </li>
              <li><a href="/archives/"><i class="fa fa-th-list"></i><span class="icon-label">Archives</span></a></li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->
<!-- Banner -->
<!-- End Banner -->
<div class="container">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="/posts/books/improving-the-way-neural-networks-learn/"
                       rel="bookmark"
                       title="Permalink to Improving the way neural networks learn">
                        Improving the way neural networks learn
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2017-09-23T00:00:00+05:30"> Sat 23 September 2017</time>
    </span>


            <span class="label label-default">By</span>
            <a href="/author/micheal-nelson.html"><i class="fa fa-user"></i> Micheal Nelson</a>

        <span class="label label-default">Category</span>
        <a href="/category/books/">books</a>

            <a href="/category/books/neuralnets/">/neuralNets</a>
        



<span class="label label-default">Tags</span>
	<a href="/tag/deep-learning/">deep-learning</a>
        /
	<a href="/tag/notes/">notes</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>

    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
<section class="well" id="related-posts">
    <h4>Part 4 of the Neural Networks and Deep Learning series</h4>
       <h5>Previous articles</h5>
       <ul>
           <li><a href="/posts/books/neural-networks-and-deep-learning/">Neural Networks and Deep Learning</a></li>
           <li><a href="/posts/books/using-neural-nets-to-recognize-handwritten-digits/">Using neural nets to recognize handwritten digits</a></li>
           <li><a href="/posts/books/how-the-backpropogation-algorithm-works/">How the backpropogation algorithm works</a></li>
       </ul>
       <h5>Next articles</h5>
       <ul>
           <li><a href="/posts/books/a-visual-proof-that-neural-networks-can-compute-any-function/">A visual proof that neural networks can compute any function</a></li>
           <li><a href="/posts/books/why-are-deep-neural-networks-hard-to-train/">Why are deep neural networks hard to train?</a></li>
           <li><a href="/posts/books/deep-learning/">Deep Learning</a></li>
           <li><a href="/posts/books/is-there-a-simple-algorithm-for-intelligence/">Is there a simple algorithm for intelligence?</a></li>
       </ul>
</section>
                
                <p>Contents:</p>
                <nav class="toc">
                  <div id="toc"><ul><li><a class="toc-href" href="#chapter-3-improving-the-way-neural-networks-learn" title="Chapter 3: Improving the way neural networks learn">Chapter 3: Improving the way neural networks learn</a><ul><li><a class="toc-href" href="#the-cross-entropy-cost-function" title="The cross-entropy cost function">The cross-entropy cost function</a><ul><li><a class="toc-href" href="#introducing-the-cross-entropy-cost-function" title="Introducing the cross-entropy cost function">Introducing the cross-entropy cost function</a></li><li><a class="toc-href" href="#what-does-the-cross-entropy-mean-where-does-it-come-from" title="What does the cross-entropy mean? Where does it come from?">What does the cross-entropy mean? Where does it come from?</a></li><li><a class="toc-href" href="#softmax" title="Softmax">Softmax</a></li></ul></li><li><a class="toc-href" href="#overfitting-and-regularization_1" title="Overfitting and regularization">Overfitting and regularization</a><ul><li><a class="toc-href" href="#regularization" title="Regularization">Regularization</a></li><li><a class="toc-href" href="#why-does-regularization-help-reduce-overfitting" title="Why does regularization help reduce overfitting?">Why does regularization help reduce overfitting?</a></li><li><a class="toc-href" href="#other-techniques-for-regularization" title="Other techniques for regularization">Other techniques for regularization</a></li></ul></li><li><a class="toc-href" href="#weight-initialization_1" title="Weight initialization">Weight initialization</a></li><li><a class="toc-href" href="#handwriting-recognition-revisited-the-code" title="Handwriting recognition revisited: the code">Handwriting recognition revisited: the code</a></li><li><a class="toc-href" href="#how-to-choose-a-neural-networks-hyper-parameters" hyper-parameters?'="" s="" title="How to choose a neural network">How to choose a neural network's hyper-parameters?</a></li><li><a class="toc-href" href="#other-techniques" title="Other techniques">Other techniques</a><ul><li><a class="toc-href" href="#variations-on-stochastic-gradient-descent" title="Variations on stochastic gradient descent">Variations on stochastic gradient descent</a></li></ul></li><li><a class="toc-href" href="#other-models-of-artificial-neuron_1" title="Other models of artificial neuron">Other models of artificial neuron</a></li></ul></li></ul></div>
                </nav>
                <hr>
                
                <p>Notes for the <a href="http://neuralnetworksanddeeplearning.com/index.html">book</a>. <br/>
<a href="https://github.com/mnielsen/neural-networks-and-deep-learning">Source code</a> for the book.</p>
<h2 id="chapter-3-improving-the-way-neural-networks-learn"><strong>Chapter 3: Improving the way neural networks learn</strong></h2>
<p><em>Backpropagation</em> was the <em>basic swing</em>, the <em>fooundation</em> for learning in most work on neural networks. 
Now we will learn the <em>tricks</em>.   </p>
<p>These include,   </p>
<ul>
<li>A better cost function, known as <em>cross entropy</em></li>
<li>Four <em>regularization</em> methods.</li>
<li>A better method for <em>weight initialization</em>.</li>
<li>A set of heuristics to help <em>choose good hyper-parameters</em>.</li>
<li>And several other techniques,</li>
</ul>
<h3 id="the-cross-entropy-cost-function"><strong>The cross-entropy cost function</strong></h3>
<p>Yet while unpleasant, we learn quickly when we're decisively wrong. <br/>
By contrast, we learn slowly when our errors are less well defined.   </p>
<p>However, turns out this is not always the case with the neural networks we train. <br/>
If the results are very wrong, it might very well be the case that the activations are close to 0 or 1. And thus the gradients of our sigmoid activation would be very small. <br/>
This would cause the learning to progress very slowly.   </p>
<h4 id="introducing-the-cross-entropy-cost-function"><strong>Introducing the cross-entropy cost function</strong></h4>
<div class="math">$$\begin{eqnarray} 
  C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right],
\tag{57}\end{eqnarray}$$</div>
<p>Barring the explanation of what it does, <br/>
Here is the derivative,   </p>
<div class="math">$$\begin{eqnarray} 
  \frac{\partial C}{\partial w_j} =  \frac{1}{n} \sum_x x_j(\sigma(z)-y).
\tag{61}\end{eqnarray}$$</div>
<p>Now, this expression says that the change is directly proportional to the error, which is what we would intuitively expect our network to do.   </p>
<p>The cross-entropy function was specially chosen to have just this property.   </p>
<p>Ans, in a similar way, we can calculate the partial derivative for the bias. <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y).
\tag{62}\end{eqnarray}$$</div>
<p>When should we use this? Probably always,..   </p>
<h4 id="what-does-the-cross-entropy-mean-where-does-it-come-from"><strong>What does the cross-entropy mean? Where does it come from?</strong></h4>
<p>It comes naturally from our expected gradients, <br/>
If we start with, <br/>
$$\begin{eqnarray} 
  \frac{\partial C}{\partial w_j} &amp; = &amp; x_j(a-y) \tag{71}\</p>
<p>\frac{\partial C}{\partial b } &amp; = &amp; (a-y).
\tag{72}\end{eqnarray}$$</p>
<p>We can come up with this equation, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  C = -\frac{1}{n} \sum_x [y \ln a +(1-y) \ln(1-a)] + {\rm constant},
\tag{77}\end{eqnarray}$$</div>
<p>Cross-entropy also has significance in <em>information theory</em>, where it tells about <em>information gain</em>.</p>
<h4 id="softmax"><strong>Softmax</strong></h4>
<p>The softmax activation, <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},
\tag{78}\end{eqnarray}$$</div>
<p>What's so good about it,.?? <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \sum_j a^L_j &amp; = &amp; \frac{\sum_j e^{z^L_j}}{\sum_k e^{z^L_k}} = 1.
\tag{79}\end{eqnarray}$$</div>
<p>That is, we can interpret it in terms of probabilities. <br/>
Which can help us in providing a more intuitive explanation of the results. <br/>
To the mathematicians at least.</p>
<p>But, how does that help with the learning slowdown problem??   </p>
<p>Let us consider this cost function, (called <em>log-likelihood</em>) <br/>
</p>
<div class="math">$$\begin{eqnarray}
  C \equiv -\ln a^L_y.
\tag{80}\end{eqnarray}$$</div>
<p>Using this cost with softmax activation, we can derive the gradients w.r.t. weights and bias as, <br/>
</p>
<div class="math">$$\begin{eqnarray}
  \frac{\partial C}{\partial b^L_j} &amp; = &amp; a^L_j-y_j  \tag{81}\\
  \frac{\partial C}{\partial w^L_{jk}} &amp; = &amp; a^{L-1}_k (a^L_j-y_j) 
\tag{82}\end{eqnarray}$$</div>
<p>Which is strikingly similar in form to the gradients we calculated using <em>sigmoid</em> activation with <em>cross-entropy</em> loss. And hence, the same intuitions apply.   </p>
<p>So, what should we use then,.? <br/>
<em>sigmoid output layer with cross-entropy loss</em>, or <br/>
<em>softmax output layer with log0likelihood loss</em>.   </p>
<p>Actually, both work well. However, <em>softmax log-likelihood</em>is worth using when we want to interpret output activations as probabilities.   </p>
<h3 id="overfitting-and-regularization_1"><strong>Overfitting and regularization</strong></h3>
<p>Models with large number of free parameters can describe amazingly wide range of phenomena. That doesn't make it a good model. It just means that the model has enough freedom to describe any data set of a given size, without really capturing any genuine insights about the phenomena.   </p>
<p>This model may work for the existing data, however it won't be able to generalize to new situations.   </p>
<p>Our simple model for MNIST dataset described earlier has 24,000 parameters. That's a lot of parameters.   </p>
<p>Current state-of-the-art models have millions or even billions of parameters. How can we trust those results?   </p>
<p><strong>Define overfitting here...</strong> </p>
<p><strong>Talking about cross-validation,...</strong> </p>
<p><strong>Get more training data to reduce overfitting!!!!</strong> </p>
<h4 id="regularization"><strong>Regularization</strong></h4>
<p>Techniques used to reduce overfitting.   </p>
<p><em>Weight decay</em> or <em>L2 regularization</em>. <br/>
</p>
<div class="math">$$\begin{eqnarray} C = -\frac{1}{n} \sum_{xj} \left[ y_j \ln a^L_j+(1-y_j) \ln
(1-a^L_j)\right] + \frac{\lambda}{2n} \sum_w w^2.
\tag{85}\end{eqnarray}$$</div>
<p>The first term is the same old cross-entropy loss function. The second term, namely the squared sum of all weights, is the regularization that we've added. <br/>
and <span class="math">\(\lambda &gt; 0\)</span>, is called the <em>regularization parameter</em>.   </p>
<p><em>Worth noting:</em> The regularization terms <em>doesn't</em> include biases.   </p>
<p>Of course we can add regularization to other cost functions as well. <br/>
</p>
<div class="math">$$\begin{eqnarray} C = \frac{1}{2n} \sum_x \|y-a^L\|^2 +
  \frac{\lambda}{2n} \sum_w w^2.
\tag{86}\end{eqnarray}$$</div>
<p>Intuitively, the effect of regularization is to make it so the network prefers to learn small weights, all other things being equal.  </p>
<p>But first, let's see how this effects our equations for gradients and weight/bias updates. <br/>
</p>
<div class="math">$$\begin{eqnarray} 
  \frac{\partial C}{\partial w} &amp; = &amp; \frac{\partial C_0}{\partial w} + 
  \frac{\lambda}{n} w \tag{88}\\ 
  \frac{\partial C}{\partial b} &amp; = &amp; \frac{\partial C_0}{\partial b}.
\tag{89}\end{eqnarray}$$</div>
<p>And, <br/>
</p>
<div class="math">$$\begin{eqnarray}
b &amp; \rightarrow &amp; b -\eta \frac{\partial C_0}{\partial b}.
\tag{90}\end{eqnarray}$$</div>
<div class="math">$$\begin{eqnarray} 
  w &amp; \rightarrow &amp; w-\eta \frac{\partial C_0}{\partial
    w}-\frac{\eta \lambda}{n} w \tag{91}\\ 
  &amp; = &amp; \left(1-\frac{\eta \lambda}{n}\right) w -\eta \frac{\partial
    C_0}{\partial w}. 
\tag{92}\end{eqnarray}$$</div>
<p><em>A analysis of results with/without regularization on the MNIST data.</em> <br/>
<em>Conclusion:</em> Regularization helps reduce overfitting. But how??   </p>
<h4 id="why-does-regularization-help-reduce-overfitting"><strong>Why does regularization help reduce overfitting?</strong></h4>
<p>A standard story people tell to explain what's going on is along the following lines: smaller weights are, in some sense, lower complexity, and so provide a simpler and more powerful explanation for the data, and should thus be preferred. </p>
<p><em>An example with linear/polynomial regression.</em> <br/>
The idea is that, larger weights in or neural net would allow the model to gather more information, and hence would allow it to learn the noise in the input.</p>
<p>Simpler explanations can be more attractive, but that doesn't mean they are right, this intuition should be used with great caution! Further, deciding which of the explanations is <em>simpler</em> can be quite subtle and <em>subjective</em>. Finally, the true test of a model is not its simplicity, but how well it explains the unseen/new(for the model) phenomena.   </p>
<p>With that said, it is an empirical fact that regularization helps to generalize better. <br/>
Still, we don't have an entirely satisfactory understanding of what's going on. <br/>
We don't even have a good understanding of how generalization works.   </p>
<p>This is particularly galling because in everyday life we humans generalize phenomenally well.   </p>
<p>We have a system - the human brain - with huge number of free parameters. And we can generalize with very few training examples. Our brains in some sense are regularizing amazingly well. How?   </p>
<p>Back to our problem, it has been conjectures that the <em>dynamics of gradient descent learning in multilayer nets</em> has a <em>self-regularizing effect</em>. This is fortunate, but disquieting as we don't understand why.   </p>
<p>In the meantime, we would just use regularization whenever we can.   </p>
<h4 id="other-techniques-for-regularization"><strong>Other techniques for regularization</strong></h4>
<p><strong>L1 regularization:</strong> <br/>
</p>
<div class="math">$$\begin{eqnarray}  C = C_0 + \frac{\lambda}{n} \sum_w |w|.
\tag{95}\end{eqnarray}$$</div>
<p><strong>Dropout:</strong> <br/>
Here we <em>randomly</em>(and <em>temporarily</em>) delete half* the hidden neurons in the network, while leaving all the weights and other neurons untouched. <br/>
<img alt="" src="/images/nnfordl/dropout1.png"/> </p>
<p>We do this only during the training,. When we run the full network during testing, we need to halve the weights outgoing from the hidden units.   </p>
<p>How does it do regularization? It is a lot like using a committee of smaller networks. The different networks will overfit in different ways, and hopefully, the net effect would reduce overfitting.   </p>
<p>In another explanation, using dropout forces the neurons to learn more robust features, reducing co-adaptations. </p>
<p><strong>Artificially expanding the training data:</strong> </p>
<p><em>All about data augmentation</em> <br/>
<em>Very powerful. Very useful.</em> </p>
<h3 id="weight-initialization_1"><strong>Weight initialization</strong></h3>
<p><em>About using Gaussian distribution for input weights, and its pitfalls.</em> </p>
<h3 id="handwriting-recognition-revisited-the-code"><strong>Handwriting recognition revisited: the code</strong></h3>
<p>The updated <code>Network</code> class,   </p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">cost</span><span class="o">=</span><span class="n">CrossEntropyCost</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sizes</span> <span class="o">=</span> <span class="n">sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">default_weight_initializer</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cost</span><span class="o">=</span><span class="n">cost</span>
</pre></div>
<p>Weight initialization, using Gaussian random variables with zero mean and standard deviation 1 divided by the square root of number of connections      </p>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">default_weight_initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> 
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
</pre></div>
<p>Here is the old weight initializer, using only Gaussian random variables,    </p>
<div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">large_weight_initializer</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> 
                        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sizes</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
</pre></div>
<p>For our newly added <code>cost</code> attribute,   </p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CrossEntropyCost</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">a</span><span class="p">)))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">delta</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">y</span><span class="p">)</span>
</pre></div>
<p>It is implemented as a class since along with the actual cost, we also need a way to calculate the gradient. This class encapsulates both things.   </p>
<p>And, here is the <code>QuadraticCost</code> that we were using earlier, abstracted into a class,   </p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QuadraticCost</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">delta</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
<h3 id="how-to-choose-a-neural-networks-hyper-parameters"><strong>How to choose a neural network's hyper-parameters?</strong></h3>
<p><em>That's a huge space to get lost in.</em> </p>
<p><strong>Broad strategy:</strong> <br/>
Start with the problem of doing better than chance. <br/>
The <em>simplest</em>(also fastest or easiest) method that can get you started. <br/>
Just make sure to get quick feedback of how the model is doing.   </p>
<p>Then we start with parameter tuning.   </p>
<p><strong>Learning rate:</strong> </p>
<p>A sample result while training in MNIST.
<img alt="" src="/images/nnfordl/dropout2.png"/></p>
<p><em>More rules of thumb about how to choose and change the learning rate.</em></p>
<p><strong>Use early stopping to determine the number of training epochs:</strong> <br/>
<em>Again, rules of thumb on when to use this, and what to do with it.</em> </p>
<p><strong>Learning rate schedule:</strong> <br/>
<em>Its good to reduce the learning rate when accuracy starts to plateau.</em> </p>
<p><strong>The regularization parameter:<span class="math">\(\lambda\)</span></strong> <br/>
<em>Start with none.</em> <br/>
<em>Find a good <span class="math">\(\eta\)</span>.</em> <br/>
<em>Play with <span class="math">\(\lambda\)</span>.</em> <br/>
<em>Fine tune <span class="math">\(\lambda\)</span>.</em> <br/>
<em>Fine tune <span class="math">\(\eta\)</span> again.</em> </p>
<p><strong>Mini-batch size:</strong> <br/>
<em>This one depends upon the resources available too.</em> <br/>
<em>Using a larger mini-batch may make things go faster if sufficient memory is available.</em> </p>
<p><strong>Automated techniques:</strong> <br/>
<em>Cross validation: Grid search and its smarter cousins.</em> </p>
<p>The space of hyper-parameters is so large, that one never really finishes optimizing, one only abandons the network to posterity.</p>
<h3 id="other-techniques"><strong>Other techniques</strong></h3>
<h4 id="variations-on-stochastic-gradient-descent"><strong>Variations on stochastic gradient descent</strong></h4>
<p><strong>Hessian technique:</strong> <br/>
Uses <em>higher order derivatives</em>. <br/>
Can converge faster than gradient descent <br/>
Difficult to apply in practice because of the huge size of the matrix.   </p>
<p><strong>Momentum-based gradient descent:</strong> <br/>
Based on similar intuition of using higher order derivatives. <br/>
Introduces a notion of <em>velocity</em>. Gradient affects the velocity and not the <em>position</em> directly. <br/>
Introduces a kind of <em>friction</em> term, which tends to gradually reduce the velocity.   </p>
<div class="math">$$\begin{eqnarray} 
  v &amp; \rightarrow  &amp; v' = \mu v - \eta \nabla C \tag{107}\\  
  w &amp; \rightarrow &amp; w' = w+v'.
\tag{108}\end{eqnarray}$$</div>
<p><strong>Other approaches of minimizing the cost function:</strong> <br/>
<em>BFGS</em> and <em>L-BFGS</em>, and more...   </p>
<h3 id="other-models-of-artificial-neuron_1"><strong>Other models of artificial neuron</strong></h3>
<ul>
<li><em>tanh</em> instead of <em>sigmoid</em></li>
<li><em>relu</em> : <em>This is the new shit</em> </li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "left",
        indent = "0em",
        linebreak = "true";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>

            
            </div>
            <!-- /.entry-content -->
    <hr />
    <!-- AddThis Button BEGIN -->
    <div class="addthis_toolbox addthis_default_style">
            <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
            <a class="addthis_button_tweet"></a>
            <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    </div>
    <!-- AddThis Button END -->
    <hr/>
    <section class="comments" id="comments">
        <h2>Comments</h2>

        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

    </section>
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Social -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Social</span></h4>
  <ul class="list-group" id="social">
    <li class="list-group-item"><a href="https://www.facebook.com/npuri1903"><i class="fa fa-facebook-square fa-lg"></i> Facebook</a></li>
    <li class="list-group-item"><a href="https://github.com/nitishpuri"><i class="fa fa-github-square fa-lg"></i> Github</a></li>
    <li class="list-group-item"><a href="https://www.linkedin.com/in/nitishpuri/"><i class="fa fa-linkedin-square fa-lg"></i> Linkedin</a></li>
    <li class="list-group-item"><a href="https://www.instagram.com/purinitish/"><i class="fa fa-instagram fa-lg"></i> Instagram</a></li>
  </ul>
</li>
<!-- End Sidebar/Social -->

<!-- Sidebar/Categories -->
<li class="list-group-item">
  <h4><i class="fa fa-home fa-lg"></i><span class="icon-label">Categories</span></h4>
  <ul class="list-group" id="categories">
    <li class="list-group-item">
      <a href="/category/articles/"><i class="fa fa-folder-open fa-lg"></i>articles</a>
    </li>
    <li class="list-group-item">
      <a href="/category/books/"><i class="fa fa-folder-open fa-lg"></i>books</a>
    </li>
    <li class="list-group-item">
      <a href="/category/gallery/"><i class="fa fa-folder-open fa-lg"></i>gallery</a>
    </li>
    <li class="list-group-item">
      <a href="/category/machine-intelligence/"><i class="fa fa-folder-open fa-lg"></i>machine-intelligence</a>
    </li>
    <li class="list-group-item">
      <a href="/category/robotics/"><i class="fa fa-folder-open fa-lg"></i>robotics</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Categories -->

<!-- Sidebar/Tag Cloud -->
<li class="list-group-item">
  <a href="/"><h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Tags</span></h4></a>
  <ul class="list-group list-inline tagcloud" id="tags">
    <li class="list-group-item tag-1">
      <a href="/tag/algorithms/">algorithms</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/architecture/">architecture</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/blogging/">blogging</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/book/">book</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/data-science/">data-science</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/deep-dream/">deep dream</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/deep-learning/">deep-learning</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/design/">design</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/gallery/">gallery</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/game-engine/">game engine</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/generative/">generative</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/graphics/">graphics</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/image-segmentation/">image-segmentation</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/imagenet/">imagenet</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/jekyll/">jekyll</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/lisp/">lisp</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/notes/">notes</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/opengl/">opengl</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/philosophy/">philosophy</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/prgogramming/">prgogramming</a>
    </li>
    <li class="list-group-item tag-1">
      <a href="/tag/programming/">programming</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/projects/">projects</a>
    </li>
    <li class="list-group-item tag-2">
      <a href="/tag/robotics/">robotics</a>
    </li>
    <li class="list-group-item tag-4">
      <a href="/tag/scheme/">scheme</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/sketch/">sketch</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/style-transfer/">style-transfer</a>
    </li>
    <li class="list-group-item tag-3">
      <a href="/tag/udacity/">udacity</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Tag Cloud -->

<!-- Sidebar/Series -->
<li class="list-group-item">
  <h4><i class="fa fa-tags fa-lg"></i><span class="icon-label">Series</span></h4>
  <ul class="list-group">
    <li class="list-group-item">
      <h5></i>Previous article</h5>
      <a href="/posts/books/how-the-backpropogation-algorithm-works/">How the backpropogation algorithm works</a>
    </li>
    <li class="list-group-item">
      <h5>Next article</h5>
      <a href="/posts/books/a-visual-proof-that-neural-networks-can-compute-any-function/">A visual proof that neural networks can compute any function</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Series -->


<!-- Sidebar/Github -->
<li class="list-group-item">
  <h4><i class="fa fa-github fa-lg"></i><span class="icon-label">GitHub Repos</span></h4>
  <div id="gh_repos">
    <p class="list-group-item">Status updating...</p>
  </div>
</li>
<!-- End Sidebar/Github -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2018 Nitish Puri
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>                <p><small>  <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english"><img alt="Creative Commons License" style="border-width:0" src="//i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a>
    Content
  licensed under a <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.english">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>, except where indicated otherwise.
</small></p>
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="/theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="/theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="/theme/js/respond.min.js"></script>


<!-- GitHub JS Code -->
<script type="text/javascript">
$(document).ready(function () {
  if (!window.jXHR) {
    var jxhr = document.createElement('script');
    jxhr.type = 'text/javascript';
    jxhr.src = '/theme/js/jXHR.js';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(jxhr, s);
  }

  github.showRepos({
    user: 'nitishpuri',
    count: 5,
    skip_forks: true,
    target: '#gh_repos'
  });
});
</script>
<script src="/theme/js/github.js" type="text/javascript"></script>
<!-- End GitHub JS Code -->
    <!-- Disqus -->
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'nitishpuri'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function () {
            var s = document.createElement('script');
            s.async = true;
            s.type = 'text/javascript';
            s.src = '//' + disqus_shortname + '.disqus.com/count.js';
            (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
        }());
    </script>
    <!-- End Disqus Code -->
    <!-- Google Analytics -->
    <script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-103032011-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>
    <!-- End Google Analytics Code -->


    <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-59bc69ad3aa13e47"></script>
</body>
</html>