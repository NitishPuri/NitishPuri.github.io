{"pages":[{"url":"pages/peterpan/","text":"","tags":"pages","title":"PeterPan"},{"url":"pages/untitled1/","text":"","tags":"pages","title":"Untitled1"},{"url":"pages/graffiti1/","text":"","tags":"pages","title":"graffiti1"},{"url":"pages/magazines/","text":"","tags":"pages","title":"magazines"},{"url":"pages/nightclub/","text":"","tags":"pages","title":"nightClub"},{"url":"pages/portrait1_2/","text":"","tags":"pages","title":"portrait1_2"},{"url":"pages/portrait1_filter/","text":"","tags":"pages","title":"portrait1_filter"},{"url":"pages/graffiti201/","text":"","tags":"pages","title":"graffiti2~01"},{"url":"pages/graffiti3_filter01/","text":"","tags":"pages","title":"graffiti3_filter~01"},{"url":"pages/graffiti301/","text":"","tags":"pages","title":"graffiti3~01"},{"url":"pages/greeklovers01/","text":"","tags":"pages","title":"greekLovers~01"},{"url":"pages/landscape12_electromagnetic01/","text":"","tags":"pages","title":"landscape12_electromagnetic~01"},{"url":"pages/landscape12_ultraviolet01/","text":"","tags":"pages","title":"landscape12_ultraviolet~01"},{"url":"pages/landscape1201/","text":"","tags":"pages","title":"landscape12~01"},{"url":"pages/noface101/","text":"","tags":"pages","title":"noFace1~01"},{"url":"pages/portrait11_filter01/","text":"","tags":"pages","title":"portrait11_filter~01"},{"url":"pages/portrait7_negative01/","text":"","tags":"pages","title":"portrait7_negative~01"},{"url":"pages/portrait701/","text":"","tags":"pages","title":"portrait7~01"},{"url":"pages/silhouette3_electromagnetic01/","text":"","tags":"pages","title":"silhouette3_electromagnetic~01"},{"url":"pages/silhouette3_negative01/","text":"","tags":"pages","title":"silhouette3_negative~01"},{"url":"pages/splash_ufilter01/","text":"","tags":"pages","title":"splash_ufilter~01"},{"url":"pages/splash01/","text":"","tags":"pages","title":"splash~01"},{"url":"pages/woodpecker201/","text":"~01.jpg)","tags":"pages","title":"woodpecker(2)~01"},{"url":"pages/graffiti_mrdj_filter/","text":"","tags":"pages","title":"Graffiti_mrDJ_filter"},{"url":"pages/graffiti_mrdj/","text":"","tags":"pages","title":"graffiti_mrDJ"},{"url":"pages/wonderwoman_2/","text":"","tags":"pages","title":"wonderWoman_2"},{"url":"pages/69_filter01/","text":"","tags":"pages","title":"69_filter~01"},{"url":"pages/6901/","text":"","tags":"pages","title":"69~01"},{"url":"pages/batman_filter01/","text":"","tags":"pages","title":"Batman_filter~01"},{"url":"pages/batman01/","text":"","tags":"pages","title":"Batman~01"},{"url":"pages/boat1/","text":"","tags":"pages","title":"Boat1"},{"url":"pages/girl103_filtered201/","text":"","tags":"pages","title":"Girl103_filtered2~01"},{"url":"pages/girl103_filtered01/","text":"","tags":"pages","title":"Girl103_filtered~01"},{"url":"pages/ironman_filter_201/","text":"","tags":"pages","title":"IronMan_filter_2~01"},{"url":"pages/ironman_filter01/","text":"","tags":"pages","title":"IronMan_filter~01"},{"url":"pages/ironman01/","text":"","tags":"pages","title":"IronMan~01"},{"url":"pages/johnnybravo/","text":"","tags":"pages","title":"JohnnyBravo"},{"url":"pages/window_filtered01/","text":"","tags":"pages","title":"Window_filtered~01"},{"url":"pages/window01/","text":"","tags":"pages","title":"Window~01"},{"url":"pages/wonderwoman01/","text":"","tags":"pages","title":"WonderWoman~01"},{"url":"pages/animalcomposition102/","text":"","tags":"pages","title":"animalComposition1~02"},{"url":"pages/boy102/","text":"","tags":"pages","title":"boy102"},{"url":"pages/composition_10901/","text":"","tags":"pages","title":"composition_109~01"},{"url":"pages/dragonsonthewall/","text":"","tags":"pages","title":"dragonsOnTheWall"},{"url":"pages/face0101/","text":"","tags":"pages","title":"face01~01"},{"url":"pages/face0301/","text":"","tags":"pages","title":"face03~01"},{"url":"pages/feline_1/","text":"","tags":"pages","title":"feline_1"},{"url":"pages/feline_1_filtered/","text":"","tags":"pages","title":"feline_1_filtered"},{"url":"pages/fly/","text":"","tags":"pages","title":"fly"},{"url":"pages/forest_electromagnetic01/","text":"","tags":"pages","title":"forest_electromagnetic~01"},{"url":"pages/forest_negative01/","text":"","tags":"pages","title":"forest_negative~01"},{"url":"pages/forest01/","text":"","tags":"pages","title":"forest~01"},{"url":"pages/girl12_filter01/","text":"","tags":"pages","title":"girl12_filter~01"},{"url":"pages/girl1201/","text":"","tags":"pages","title":"girl12~01"},{"url":"pages/portrait1101/","text":"","tags":"pages","title":"portrait11~01"},{"url":"pages/thingfish/","text":"","tags":"pages","title":"thingFish"},{"url":"pages/wonderwoman0101/","text":"","tags":"pages","title":"WonderWoman~01~01"},{"url":"pages/portrait_angelina_edited/","text":"","tags":"pages","title":"portrait_angelina_edited"},{"url":"pages/portrait_ash/","text":"","tags":"pages","title":"Portrait_Ash"},{"url":"pages/scarlett/","text":"","tags":"pages","title":"Scarlett"},{"url":"pages/architecture_1201/","text":"","tags":"pages","title":"architecture_12~01"},{"url":"pages/mface_28/","text":"","tags":"pages","title":"mFace_28"},{"url":"pages/mface_03/","text":"","tags":"pages","title":"mFace_03"},{"url":"pages/mface_09/","text":"","tags":"pages","title":"mFace_09"},{"url":"pages/mface_16/","text":"","tags":"pages","title":"mFace_16"},{"url":"pages/mface_17/","text":"","tags":"pages","title":"mFace_17"},{"url":"pages/mface_25/","text":"","tags":"pages","title":"mFace_25"},{"url":"pages/mface_32/","text":"","tags":"pages","title":"mFace_32"},{"url":"pages/frac_01/","text":"","tags":"pages","title":"frac_01"},{"url":"pages/frac_05/","text":"","tags":"pages","title":"frac_05"},{"url":"pages/frac_08/","text":"","tags":"pages","title":"frac_08"},{"url":"pages/frac_13/","text":"","tags":"pages","title":"frac_13"},{"url":"pages/gallery/","text":"","tags":"pages","title":"Gallery"},{"url":"pages/bio/","text":"A self motivated and passionate engineer who is interested in computer graphics, simulations, machine intelligence, computer vision and everything in between. On the other side I am very much into video games(all kind), music(all kind), visual art(all kind) and (philosophy of)sciences(almost all kind). I had a very boring start when I joined Bachelors in Mechanical, IIT Roorkee. Although it is an awesome place, I found out that my passions don't lie in the gear box, (or the machining lab for that matter). Pretty soon I found myself consuming lots of games and music. Started playing with some game engines and created a very basic 3D Flight Simulator . And that was all I did in my time there(apart from the other things that engineering students do). Fortunately, I got hired at VizExperts India , an information visualization company with roots in desktop graphics. That was the time when I realized how cool programming can be!!! From there on, it has been quite a journey. We primarily worked on our in-house 3D GIS based planning and simulation engine built on an open source renderer, OpenSceneGraph which has been called with different names since then, we will just call it GeorbIS . I worked on everything from frontend UI framework to rendering and simulation engine to 3D computational filters. It really brought back all the lost love i had for classical physics in the form of Matrix transformations.Did a lot of app level features with direct involvement with the clients. I also had a small role in VizSim platform, a distributed simulation engine that provides real time sensor data fusion and visualization. There I got introduced to OpenCV and computer vision in general and implemented my first object detection and tracking pipeline. It was very cool. After that, we started working on VizGame , a gamified training platform built on top of Unreal Engine . It used our GeorbIS framework as a data backend for creating procedurally populated, highly realistic terrains with GIS data. Here, again, my role went from laying out the Game design to implementing multiplayer(Co-presence) in VR. And by this time I was also involved in mentoring other people who were like me a few years back. In the mean time I was getting more and more interested towards data science and machine intelligence. So, I decided to pursue my interests with self learning, with a vision of somehow managing to merge my experience with computer graphics and simulation to the field of computer vision and robotics. Lets see how far we can get.!!!! For a more professional looking resume, click here .","tags":"bio","title":"Bio"},{"url":"posts/research/object-detection-and-image-segmentation/","text":"Deep Learning Image Segmentation Image segmentation review Source A review of segmentation at qure.ai Rich feature hierarchies for accurate object detection and semantic segmentation Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik : Oct 2014 Source Introduces R-CNN, Regions with CNN. Bridging the gap between image classification and object detection. Object detection with R-CNN Region proposals. Uses selective search . Propose a bunch of boxes in the image and see if any of them actually correspond to an object. Feature extraction. Extracts a 4096 dimensional feature vector from each region proposal by propagating a mean subtracted 227 X 227 RGB image through five conv layers and two fully connected layers. These test time detections are highly parallel and the costs can amortized costs are hence low. Training is done by supervised pre-training followed by domain-specific fine-tuning Finally, R-CNN runs a simpler linear regression on the region proposal to generate tighter bounding box coordinates to get our final results. Fast R-CNN Ross Girshick : Sep 2015 Source Implementation 9 X faster at training time. 213 X faster at test time. Streamline the previous process by jointly learning to classify object proposals and refine their spatial locations. Advantages: Higher detection quality than R-CNN and SPPnet. Training is single stage, using a multi-task loss. Training can update all network layers. No disk storage is required for feature caching. Architecture The RoI pooling layer : Run the CNN just once per image and then find a way to share that computation across the ~2000 proposals. Initializing from pre-trained networks. with some modifications. Fine tuning for detection. Hierarchical sampling. Multi-task loss. Classification loss + boounding box regression offsets. Mini-batch Sampling. Back-propagation through RoI pooling layers. SGD hyperparameters. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun : Jan 2016 Microsoft Research Source Implementation Eliminating the bottleneck, region proposals as inputs. Introduces Region Proposal Networks (RPNs) that share convolutional layers with object detection networks. While training we alternate between the region proposal task and object detection task while keeping the proposals fixed. Mask R-CNN Kaiming He, Georgia Gkioxari, Piotr Dollár, Ross Girshick : Apr 2017 Facebook AI Research Source Implementation Extends Faster R-CNN by adding another branch that predicts the object mask along with the bounding boxes. Faster R-CNN does not provide pixel-to-pixel alignment between network inputs and outputs. To fix this a layer RoIAlign is proposed which replaces RoIPool layer used previously. The first stage is identical to Faster R-CNN. In the second stage, along with predicting the class and the box offset, Mask R-CNN also outputs a binary mask for each RoI. Multi-task loss is used : \\(L = L_{cls} + L_{bbox} + L_{mask}\\) . Masks are generated for every class without competition among classes. This decouples mask and class prediction. Different architectures are used as convolutional backbone , ResNet and ResNeXt A Review of Deep Learning Techniques Applied to Semantic Segmentation Alberto Garcia-Garcia, Sergio Orts-Escolano, Sergiu Oprea, Victor Villena-Martinez, Jose Garcia-Rodriguez : Apr 2017 Source Semantic Segmentation, Deep Learning, Scene Labeling, Object Segmentation This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. This also describes the terminology used as well as some background concepts, then some existing models are reviewed(2017). At last a set of promising future works are discussed. These techniques are not very mature as of yet, mainly because of a lack of unifying picture. CNN Architectures : AlexNet, VGG, GoogleNet, ResNet, etc.. 2D and 3D Datasets : PascalVOC , Microsoft COCO , and more,... Decoder Variants, Integrating Context Knowledge Instance Segmentation RGB-D Data and 3D Data Video Sequences DeepLab : Semantic Image Segmentation with Deep Convolution Nets, Atrous Convolution, and Fully Connected CRFs Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L. Yuille : May 2017 Source Semantic Segmentation, Atrous Convolution, Conditional Random Fields Introduces upsampled filters(Altrous Convolution) as a tool in dense prediction tasks. Allows us to control the resolution at which feature responses are computed and also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. ???? // Read this again.. U-Net: Convolution Networks for Biomedical Image Segmentation Olaf Ronneberger, Philipp Fischer, Thomas Brox : May 2015 Source Focuses on end-to-end training for segmentation tasks, relying heavily on data augmentation. Fully Convolutional Networks for Semantic Segmentation Jonathan Long, Evan Shelhamer, Trevor Darrell : Mar 2015 Source One of the first works to use Fully Connected layers to create pixel heatmap as output. Introducing Upsampling or Convolution Transpose. From Image-level to Pixel-level Labeling with Convolutional Networks Pedro O. Pinheiro, Ronan Collobert : Apr 2015 Source Weakly supervised segmentation. Put more weights to pixels with known class labels. Uses part of model trained on ImageNet and trains for segmentation on PascalVOC. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Research","title":"Object Detection and Image Segmentation"},{"url":"posts/research/neural-network-architectures/","text":"Deep Learning Architectures Self-Normalizing Neural Networks Günter Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter : Sep 2017 Source Deep learning is setting new benchmarks everyday with the help of RNNs and CNNs. However, looking at problems that are not related to vision or sequential tasks, gradient boosting, random forests, or support vector machines are winning most of the competitions(Eg. Kaggle, HIGGS Challenge ). With CNNs success, batch normalization and other stochastic regularization techniques has evolved into a standard. Both RNNs and CNNs can stabilize learning with weight sharing. However, this is not very useful with FNNs, and often leads to high variance. Self-Normalizing Neural Networks Definition 1 : A neural network is self-normalizing if it possesses a mapping \\(g : \\Omega \\mapsto\\Omega\\) for each activation \\(y\\) that maps mean and variance from one layer to the next and has a stable and attracting fixed point depending on \\((\\omega,\\tau)\\) in \\(\\Omega\\) . Furthermore, the mean and the variance remain in the domain \\(\\Omega\\) , that is \\(g(\\Omega)\\subseteq\\Omega\\) , where \\(\\Omega = \\{(\\mu,\\nu)|\\mu\\in[\\mu_{min}, \\mu{max}], \\nu\\in[\\nu_{min}, \\nu{max}]\\}\\) . When iteratively applying the mapping \\(g\\) , each point within \\(\\Omega\\) converges to this fixed point. So, SNNs keep normalization of activations when propagating them through layers of the network. Constructing SNNs : The activation function, SELU $$\\text{selu}(x) = \\lambda\\begin{cases}x, & \\text{if } x \\ge 0 \\\\ \\alpha e&#94;x - \\alpha, & \\text{if } x \\leq 0 \\end{cases}$$ This activation allows to construct a mapping \\(g\\) with properties that lead to SNNs. They cannot be derived with (scaled) ReLUs, sigmoid units, \\(tanh\\) units and leaky ReLUs. For weight initialization \\(\\omega=0\\) and \\(\\tau=1\\) for all units in higher layer is proposed. New Dropout techniques are introduced. Benchmarks compared for UCI repository datasets, outperforming FNNs with and without normalization techniques, such as batch, layer and weight normalization or specialized architectures such as ResNets. Also proved that SNNs do not face vanishing and exploding gradients problem and therefore work well for architectures with many layers. The best performaing SNNs are typically very deep in contrast to other FNNs. Understanding deep learning requires rethinking generalization Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals : Feb 2017 Source Neural networks have far more often trainable parameters than the number of samples they are trained on. Even then they exhibit small generalization errorr i.e. difference between \"training error\" and \"test error\". Effective Capacity Of Neural Networks Randomization tests. Standard architectures were trained on a copy of data where the true labels were replaced by random labels. Deep neural networks easily fit random labels i.e. the they achieve 2 test error. The test error was of course no better than random chance as there was no correlation between the training and test labels. Also replacing the true images with random pixels(Gaussian noise), we observe that CNNs continue to fit the data with zero training error. This has the following implications: The effective capacity of neural networks is sufficient for memorizing the entire dataset. Even optimization on random labels remains easy. In fact, training time increases only by a small constant factor compared with training on the true labels. Randomizing the labels is solely a data transformation, leaving all other properties of the learning problem unchanged. The role of regularization Explicit regularization may improve generalization performance, but is neither necessary not by itself sufficient for controlling generalization error. \\(l_2\\) regularization sometimes even helps optimization, illustrating its poorly understoof nature in deep learning. Finite sample expressivity Theorem 1. There exists a two-layer neural network with ReLU activations and \\(2n+d\\) weights that can represent any function on a sample of size \\(n\\) in \\(d\\) dimensions. Implicit Regularization : An Appeal To Linear Models Arguments showing that it is not necessarily easy to understand the source of generalization for linear models either. Do all global minima generalize equally well? Is there a way to determine when one global minimum will generalize whereas another will not? In case of a linear model, even the curvature of the loss function would be the same. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Research","title":"Neural Network Architectures"},{"url":"posts/research/style-transfer-part-2/","text":"Deep Learning Style Transfer Artistic style transfer for videos Manuel Ruder, Alexey Dosovitskiy, Thomas Brox : Apr 2016 Source The previously discussed techniques have been applied to videos on per frame basis. However, processing each frame of the video independently leads to flickering and false discontinuities, since the solution of the style transfer task is not stable. To regularize the transfer temporal constraints using optical flow are introduced. Notation \\(\\mathbf p&#94;{(i)}\\) is the \\(i&#94;{th}\\) frame of the original video. \\(\\mathbf a\\) is the style image. \\(\\mathbf x&#94;{(i)}\\) are the stylized frames to be generated. \\(\\mathbf {x'}&#94;{(i)}\\) is the initialization of the style optimization algorithmat frame \\(i\\) . Short-term consistency by initialization Most basic way to yield temporal consistency is to initialize the optimization for the frame \\(i+1\\) with the stylized frame \\(i\\) . Does not perform very well if there are moving objects in the scene, so we use optical flow. \\(\\mathbf {x'}&#94;{(i+1)}=\\omega_i&#94;{i+1}\\mathbf x&#94;{(i)}\\) . Here \\(\\omega_i&#94;{i+1}\\) denotes the function tha warps a given image using the optical flow field that was estimated between \\(\\mathbf p&#94;{(i)}\\) and \\(\\mathbf p&#94;{(i+1)}\\) . DeepFlow and EpicFlow , both based on Deep Matching are used for optical flow estimation. Temporal consistency loss Let \\(\\mathbf w = (u,v)\\) be the optical flow in forward direction and \\(\\mathbf {\\hat w}=(\\hat u, \\hat v)\\) the flow in backward direction. Then, \\(\\mathbf {\\tilde w}(x,y) = \\mathbf{w}((x,y) + \\mathbf{\\hat{w}}(x,y))\\) is the forward flow warped to the second image. In areas without disoclusion, this warped flow should be approximately the opposite of the backward flow. So, we can find the areas of disoclusions where \\(|\\mathbf{\\widetilde{w} + \\hat{w}}|&#94;2 > 0.01(|\\mathbf{\\widetilde{w}}|&#94;2+|\\mathbf{\\hat{w}}|&#94;2)+0.5\\) . and motion boundaries can be detected where \\(|\\Delta\\mathbf{\\hat{u}}|&#94;2+|\\Delta\\mathbf{\\hat{v}}|&#94;2>0.01|\\mathbf{\\hat{w}}|&#94;2+0.002\\) . So, temporal consistency loss function penalizes deviations from the warped image in regions where the optical flow is consistent and estimated with high confidence. $$\\mathcal{L}_{temporal}(\\mathbf{x,\\omega,c}) = \\frac1D\\sum_{k=1}&#94;Dc_k\\cdot(x_k-\\omega_k)&#94;2$$ Here, \\(\\mathbf{c}\\in [0,1]&#94;D\\) is per-pixel weighing of the loss and \\(D=W\\times{H}\\times{C}\\) is the dimensionality of the image. We define \\(\\mathbf{c}&#94;{(i-1,i)}\\) between frames \\(i-1\\) and \\(i\\) as \\(0\\) in disoccluded regions and the motion boundaries, and 1 everywhere else. So, overall loss takes the form, $$\\mathcal L_{shortterm}(\\mathbf{p}&#94;{(i)},\\mathbf{a},\\mathbf{x}&#94;{(i)}) = \\alpha\\mathcal{L}_{content}(\\mathbf{p}&#94;{(i)},\\mathbf{x}&#94;{(i)}) + \\beta\\mathcal{L}_{style}(\\mathbf{a},\\mathbf{x}&#94;{(i)}) + \\gamma\\mathcal{L}_{temporal}(\\mathbf{x}&#94;{(i)}, \\omega_{i-1}&#94;i(\\mathbf{x}&#94;{(i-1)}), \\mathbf{c}&#94;{(i-1,i)})$$ Long-term consistency The short-term model has the following limitation: when some areas are occluded in some frame and disoccluded later, these areas will likely change their appearance in the stylized video. So, we need to use a penalization for deviations from more distant frames too. \\(J\\) is the set of relative indices that each frame takes into account. So, the loss function is, $$\\mathcal L_{longterm}(\\mathbf{p}&#94;{(i)},\\mathbf{a},\\mathbf{x}&#94;{(i)}) = \\alpha\\mathcal{L}_{content}(\\mathbf{p}&#94;{(i)},\\mathbf{x}&#94;{(i)}) + \\beta\\mathcal{L}_{style}(\\mathbf{a},\\mathbf{x}&#94;{(i)}) + \\gamma\\sum_{j\\in J:i-j\\geq1}\\mathcal{L}_{temporal}(\\mathbf{x}&#94;{(i)}, \\omega_{i-j}&#94;i(\\mathbf{x}&#94;{(i-j)}), \\mathbf{c}_{long}&#94;{(i-j,i)})$$ where, \\(\\mathbf{c}_{long}&#94;{(i-j,i)}=\\text{max}(\\mathbf{c}&#94;{(i-j,i)} - \\sum_{k\\in J:i-k>i-j}\\mathbf{c}&#94;{(i-k,i)}, \\mathbf{0})\\) Multi-pass algorithm The image boundaries tend to have less contrast and less diversity than other areas. This is not a problem for mostly static videos, but with large camera motion, these effects can creep in towards the center, which leads to lower quality images over time. So, we use a multi-pass algorithm which processes the whole sequence in multiple passes and alternating directions. Each pass consists of a lower number of iterations without full convergence. The sequence is run in alternating directions with each flow and blended for some number of iterations till some convergence. The multi-pass algorithm can be combined with temporal consistency loss described above. Achieve good results if temporal loss is disabled in several initial passes and enabled in later passes after the images had stabilized. Long term motion estimate... Artifacts at image boundaries,... Implementation : https://github.com/manuelruder/artistic-videos Watch in action : https://youtu.be/vQk_Sfl7kSc Instance Normalization: The Missing Ingredient for Fast Stylization Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky : Sep 2016 Source Learn a generator network \\(g(x,z)\\) that can apply to a given input image \\(x\\) the style of another \\(x_0\\) . \\(g\\) is a convolutional neural network learned from examples \\(x_t\\) by solving $$\\text{min}_g\\frac1n\\sum_{t=1}n\\mathcal{L}(x_0, x_t, g(x_t, z_t)), \\text{where }z_t \\sim\\mathcal{N}(0,1)$$ Perceptual Losses for Real-Time Style Transfer and Super Resolution Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li : 2016 Source The system consists of two components : image transformation network \\(f_W\\) (deep resnet with encoder-decoder scheme parameterized by weights \\(W\\) ) and a loss network \\(\\phi\\) that is used to define several loss functions \\(l_i,...l_k\\) . The optimization problem becomes, $$W&#94;*=\\text{arg min}_W\\mathbf{E}_{x,\\{y_i\\}}[\\sum_{i=1}\\lambda_i l_i(f_W(x), y_i)]$$ Uses the loss network \\(\\phi\\) to define a feature reconstruction loss \\(l_{feat}&#94;{\\phi}\\) and style reconstruction loss \\(l_{style}&#94;{\\phi}\\) that measures differences in content and style between images. Simple loss functions : In addition to the perceptual losses discussed above(and described earlier), two simple loss functions that depend only on low level pixel information are used. Pixel Loss : Can only be used when ground truth is available. Total Variation Regularization : to encourage spatial smoothness. Implementation* : https://github.com/jcjohnson/fast-neural-style Stylizing Face Images via Multiple Exemplars Yibing Song, Linchao Bao, Shengfeng He, Qingxiong Yang, Ming-Hsuan Yang : Aug 2017 Source Existing methods using a single exemplar lead to inaccurate results when the exemplar does not contain sufficient stylized facial components for a given photo. Proposes a style transfer algorithm in which a Markov random field is used to incorporate patches from multiple exemplars. The proposed method enables the use of all stylization information from different exemplars. And, proposes an artifact removal methods based on an edge-preserving filter. It removes the artifacts introduced by inconsistent boundaries of local patches stylized from different exemplars. In addition to visual comparison conducted by existing methods, performs quantitative evaluations using both objective and subjective metrics to demonstrate effectiveness of the proposed method. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Research","title":"Style Transfer, Part 2"},{"url":"posts/research/style-transfer-part-1/","text":"Deep Learning Style Transfer A Neural Style Algorithm of Artistic Style Leon A. Gatys, Alexander S. Ecker, Matthias Bethge : Sep 2015 Source In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. Then we came across Deep Neural Networks. Higher layers in the network capture the high-level content in terms of objects and their arrangement in the input image. We represent these feature responses as content representation . $$\\mathcal L_{content}(\\vec p,\\vec x,l) = \\frac12\\sum_{i,j}{(F&#94;l_{ij} - P&#94;l_{ij})&#94;2}$$ For style we need to capture correlations(given by Gram matrix \\(G&#94;l \\in \\mathcal R&#94;{N_l \\times N_l}\\) where \\(G&#94;l_{ij} = \\sum_kF&#94;l_{ik}F&#94;l_{jk}\\) ) between different filter responses. This representation captures the texture information of the input, but not the global arrangement. This multi-scale representation is called style representation . $$E_l = \\frac1{4N&#94;2_lM&#94;2_l}\\sum_{ij}(G&#94;l_{ij}-A&#94;l_{ij})&#94;2$$ $$\\mathcal L_{style}(\\vec a,\\vec x) = \\sum_{l=0}&#94;Lw_lE_l$$ So, we can manipulate both content and style separately. The images are synthesised by finding an image that simultaneously matches the content representation of the photograph and the style representation of the respective piece of art. $$\\mathcal L_{total}(\\vec p,\\vec a,\\vec x) = \\alpha\\mathcal L_{content}(\\vec p,\\vec x) + \\beta\\mathcal L_{style}(\\vec a,\\vec x)$$ Gallleries Style Transfer Studies Implementations Neural Style, JC Johnson, Lua Improving the Neural Algorithm of Artistic Style Roman Novak, Yalroslav Nikulin : May 2016 Source Objectives addressed in this paper: Similar areas of the content image should be repainted in a similar way. Different areas should be painted differently. Useful Modifications, A better per-layer content/style weighting scheme. \\(w_l&#94;s = 2&#94;{D-d(l)},\\quad w_l&#94;c=2&#94;{d(l)}\\) This indicates that most important style properties come from bottom layers, while content is mostly represented by activations in the upper layers. Using more layers to capture more style properties. Used all 16 conv layers of VGG-19 for calculating Gram matrices. Using shifted activations when computing Gram matrices to eliminate sparsity and make individual entries more informative and also speed-up style transfer convergence. \\(G&#94;l=(F&#94;l+s)(F&#94;l+s)&#94;T\\) , (where \\(s=-1\\) for best results). Targeting correlations of features belonging to different layers to capture more feature interactions. \\(G&#94;{lk}=F&#94;l[up(F&#94;k)]&#94;T\\) , if \\(X_k \\leq X_l\\) This blows up the number of definitions of style( \\(G\\) ) to \\(2&#94;{16&#94;2}\\) for 16 layers of VGG-19. However, experiments also show that tieing in distant layers gives poor results. Correlation Chain Instead of considering all layer combinations, use only a \"chained\" representation, \\(\\{G&#94;{l,l-1}|l=2...16\\}.\\) So, only correlations with immediate neighbors are considered. Blurred Correlations While calculating correlations, the smaller feature layer is upsampled, but even after having the same dimensions, the feature maps may still correspond to features of different scales. To overcome this we use blurring. \\(G&#94;{lk}=F&#94;l[blur&#94;{l-k}\\circ up(F&#94;k)]&#94;T\\) This gives positive results, but it does complicate the objective function and results in slow and unreliable convergence. Some Modifications that did not work out in the end, Gradient Masking Amplifying Activations Adjacent Activations Correlations Content-aware Gram Matrices Gram Cubes Experiments Preserving Color in Neural Artistic Style Transfer Leon A. Gatys, Matthias Bethge, Aaron Hertzmann, Eli Shechtman : Jun 2016 Source The original style transfer method also copies the colors of the style image, which might be undesirable in many cases. Approach #1: Color histogram matching Transform style image \\((S)\\) to match the colors of content image \\((C)\\) . This produces a new style \\((S')\\) . The algorithm remains unchanged otherwise. We have several different options for the initial color transfer. Linear method, \\(\\mathbf x_{S'}\\leftarrow \\mathbf Ax_S+\\mathbf b\\) \\(\\mathbf b=\\mu_C- \\mathbf A\\mu_S\\) , where \\(\\mu_C\\) and \\(\\mu_S\\) are mean colors. \\(\\mathbf A\\Sigma_S \\mathbf A&#94;T=\\Sigma_C\\) , where \\(\\Sigma_C\\) and \\(\\Sigma_C\\) are pixel covariances. \\(\\mathbf A\\) can be computed using Cholesky decomposition, or by using Image Analogies. Color transfer before style transfer generally gives better results. Approach #2: Luminance-only transfer This approach is motivated by the observation that visual perception is far more sensitive to change in luminance than in color. \\(L_S\\) and \\(L_C\\) are luminance channels extracted from the style and content images. Use a YIQ color space, the color information represented by I and Q channels is combined with \\(L_T\\) to produce the final output image. \\(L_{S'}=\\frac {\\sigma_C}{\\sigma_S}(L_S - \\mu_S) + \\mu_C\\) Comparison Linear color transfer onto the style image, before style transfer. Limited by how well the color transfer from content to style works. Style transfer only in the luminance channel. Preserves the colors of content image perfectly. However, dependencies between the luminance and the color channels are lost in the output image. To Read Magenta Single Shot How transferable are features in deep neural networks? https://arxiv.org/abs/1708.08288 https://arxiv.org/abs/1603.04467 https://arxiv.org/abs/1702.00824 https://arxiv.org/abs/1709.02878 https://arxiv.org/abs/1610.02357 https://arxiv.org/pdf/1511.08630.pdf https://arxiv.org/abs/1602.07261 https://arxiv.org/pdf/1704.04861.pdf https://arxiv.org/abs/1603.01768 https://arxiv.org/abs/1505.07376 https://arxiv.org/abs/1601.04589 https://hal.inria.fr/hal-00873592 https://hal.inria.fr/hal-01142656 https://lmb.informatik.uni-freiburg.de/Publications/2010/Bro10e/ http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf https://arxiv.org/abs/1503.02531 https://arxiv.org/abs/1512.05287 http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf https://arxiv.org/abs/1603.07285v1 Deconvolutional Networks Gradient Descent Ocerview , https://arxiv.org/abs/1609.04747 (Deep Visualization Toolbox)(http://yosinski.com/deepvis) Neural Machine Translation https://arxiv.org/abs/1412.3555v1 http://www.bioinf.jku.at/publications/older/2604.pdf http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015 RNN Book, http://www.cs.toronto.edu/~graves/preprint.pdf Rectifier Nonlinearities Improve Neural Network Acoustic Models https://arxiv.org/abs/1502.01852 https://arxiv.org/abs/1511.07289v1 https://arxiv.org/abs/1402.3337 https://arxiv.org/abs/1502.03167 http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf https://arxiv.org/abs/1212.5701 https://arxiv.org/abs/1412.6980v8 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Research","title":"Style Transfer, Part 1"},{"url":"posts/summary/elements-of-statistical-learning-part-2/","text":"Chapter 7: Model Assessment and Selection Introduction Bias, Variance and Model Complexity The Bias-Variance Decomposition Example : Bias-Variance Tradeoff Optimism of the Training Error Rate Estimates of In-Sample Prediction Error The Effective Number of Parameters Also known as effective degree of freedom \\(= trace(S)\\) , where \\(\\hat y=Sy\\) . The Bayesian Approach and BIC Minimum Description Length Vapnik-Chervonenkis Dimension ☠ Cross-Validation 👍 K-Fold Cross Validation The Wrong and Right Way to Do Cross-validation Does Cross-Validation Really Work? Bootstrap Methods Conditional or Expected Test Error? ☠ Chapter 8: Model Inference and Averaging if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Summary","title":"Elements Of Statistical Learning, Part 2"},{"url":"posts/research/image-recognition-and-neural-network-architectures/","text":"Very minimal notes on some papers or articles that I recently read. Mainly for logging purposes. Deep Learning Image Recognition Very Deep Convolutional Networks For Large-Scale Image Recognition Karen Simonyan, Andrew Zisserman : Apr 2015 Source Implementation Introduces the VGG network that won ImageNet in 2014 . Deeper ConvNets. Takes input as (224 X 224) RGB and mean image subtracted as preprocessing. Two final FC hidden layers, followed by one FC layer with 1000 outputs. Number of total trainable parameters turn out to be 144 million for VGG-19. All the hidden layers use ReLU activations. Deeper networks with small filters result in more regularization and less parameters. Optimise multinomial logistic regression objective using mini-batch gradient descent with momentum. At the end introduces ensemble models by averaging softmax predictions from multiple models. Going Deeper with Convolutions Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich : Sep 2014 Google DeepMind Source Introduces \"Inception\" with improved utilization of computing resources. \"We need to go deeper\" : But deeper networks come with a cost of large number of parameters, which makes the model prone to overfitting, and dramatically increased use of computational resources. Fundamental idea : sparsely connected architectures, even inside the convolutions. However, the computing infrastructure is very inefficient when it comes to numerical calculations on sparse data structures. And non-uniform sparse structures require careful engineering! Architecture GoogLeNet 22 trainable Layers(100 total layers), low memory footprint. Auxillary classifiers are used to allow for efficient gradient propagation. These are used only at training time. Deep Residual Learning for Image Recognition Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun : Dec 2015 Microsoft Research Source Presents residual learning framework(ResNet) to ease the training of networks that are substantially deeper(152 layers!) than those used previously. How to win ImageNet in 2015. Problem with deeper networks : Vanishing Gradients : Addressed by intermediate normalization. Problem with deeper networks : Degradation , not caused by overfitting.. Introduces residual learning framework by using shortcut connections that can perform identity mapping. Using Identity mapping as precondition allows the network to easily learn the identity, if it is a desired mapping. This helps in simplifying networks. Plain Network architecture, mainly based on VGG nets. Residual Network architecture, insert shortcuts to the plain network. The model shows no optimization difficulty even with > 1000 layers..!! Finally discusses improvements for detection and localization tasks. Rethinking the Inception Architecture for Computer Vision Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna : Dec 2015 Google DeepMind Source Improving upon Inception module introduced in GoogLeNet. General guiding principles Avoid representational bottlenecks, especially early in the network. Higher dimensional representations are easier to process locally within a network. Spatial aggregation can be done over lower dimensional embeddings without much or any loss in representational power. Balance the width and depth of the network. Factorizing Convolutions with Large Filter Size Factorize into smaller convolutions. This results in reduced parameter count. Does this replacement result in any loss of expressiveness? Spatial Factorization into Asymmetric Convolutions Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi : Aug 2016 Google DeepMind Source Deep Visualization Visualizing and Understanding Convolutional Networks Matthew D Zeiler, Rob Fergus L : Nov 2013 Source Understanding why CNNs perform well on Image Classification tasks. Visualizing with a Deconvnet Feature Visualization Feature Evolution during training Feature Invariance Occlusion Sensitivity Correspondence Analysis Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks Anh Nguyen, Jason Yosinski, Jeff Clune : May 2016 Source Researchers have been using activation maximization techniques until now. This assumes that each neuron detects only one type of feature. But, we know neurons can be multifaceted . Here multifaceted feature visualization (MFV) is introduced. Systematically visualize all facets of a neuron. Improve image quality of synthesized images with natural and globally consistent colors. Center biased regularization is used so that the synthesized images dont have many repeated object fragments. This is done by first producing a blurry image, then updating the center pixels more than the edge ones, producing a final image that is sharp and has a centrally-located object. This image would have far fewer duplicated fragments. Visualizing the multifaceted nature of hidden neurons Discusses various optimization techniques to produce better images in detail : center biased regularization, mean image initialization.","tags":"Research","title":"Image Recognition and Neural Network Architectures"},{"url":"posts/research/research-notes-robotics/","text":"Very minimal notes on some papers or articles that I recently read. Mainly for logging purposes. Robotics Robot Grasping Robotic Grasping and Contact: A Review Source Survey of work done in last two decades. Functions of Human hand Explore : haptics Restrain : fixturing Manipulation : dexterous manipulation Closure properties of grasps Contact modelling the grap Force Analysis Contact model Kinematics of contact Contact compliance Measures of grasp performance Grasping and the kinematics of the hand Dynamics Mobile Robots Modular and Reconfigurable Mobile Robots Source Classification Modular robots with mobile configuration change(MCC) S-bots Uni-Rovers JL-I and JL-II Millibots AMOEBA Modular robots with whole body locomotion(WBL) Whole body locomotion in chain architecture CONRO/PolyBot GZ-I CKBot Whole body locomotion in a lattice architecture Macro robots in a lattice architecture Crystalline Odin I-Cubes Catoms Mini robots in a lattice archtecture Reconfigurable mechanisms in a lattice architecture Whole body locomotion in a hybrid architecture M-TRAN/iMobot Molecubes ATRON YaMOR SuperBot Bio Robots OpenRatSLAM : on open source brain-based SLAM system Feb 2013 Source RatSLAM, OpenRatSLAM, SLAM, Navigation, Mapping, Brain-based, Appearance-based, ROS, Open-source, Hippocampus RatSLAM is a navigation system based on the neural processes underlying navigation in the rodent brain, capable of operating with low resolution monocular image data. This paper describes OpenRatSLAM, an open source version of RatSLAM with bindings to ROS SLAM(Simultaneous Localization and Mapping) , at the core based on SIFT or SURF features. This implementation is based on RatSLAM, leveraging tools like OpenCV and ROS. Modular, detailed, integrated with ROS and rviz , works online and offline. RatSLAM Pose Cells, Local View Cells and Experience Map OpenRatSLAM, code Pose Cell Network : represents pose in response to odometric and local view connections. This also makes decisions about the experience map node and link creation. Local View Cells : determines whether a scene is novel or familiar by image comparison techniques. Mostly based on template matching. Experience map : manages graph building, graph relaxation and path planning. Visual Odometry : For image only datasets, provides an odometric estimate based on changes in the visual scene. OpenRatSLAM parameters and tuning Iterative tuning by minimizing loss. Using OpenRatSLAM Examples of datasets this is used with, and some results. Future work Watch In Action Biologically Inspired App roaches to Robotics March 1997 Source Big gap between fantasy and reality in terms of Autonomous Robots. Inspirations from insects : agility, adaptability, simplicity Focuses on walking like an insect. From Biology to Robotics Studies done at various levels of integration and inspiration. Distributed Gait Control A Distributed Neural Network Controller A Stick Insect Controller Evolved Locomotion Controllers Use genetic algorithms to evolve the neural networks for controlling the locomotion. Rough Terrain Locomotion The First Takeoff of a Biologically Inspired At-Scale Robotic Insect April 2008 Source Actuators, aerial robotics, biologically inspired robotics, microrobotics Goal is to create an insect-sized, truly micro air vehicle. Harvard Microrobotic Fly Fig. (a) Conceptual drawing highlighting the four primary mechanical and aero-mechanical components. Fig. (b) First insect-scale flying robot able to takeoff. INSECT-FLIGHT Dipteran thoracic mechanics is discussed. Creation of a Robotic Insect Actuation Using peizoceramic materials. Transmission Airfoils Watch In Action Towards Dynamic Trot Gait Locomotion—Design, Control, and Experiments with Cheetah-cub, a Compliant Quadruped Robot July 2013 Source Watch In Action","tags":"Research","title":"Research Notes - Robotics"},{"url":"posts/summary/algorithms-part-3/","text":"Chapter 3: Decomposition of Graphs Why Graphs The range of problems that can be solved by representing your problem in Graphs. Graph representations, Adjacency Matrix $$ a_{ij} = \\begin{cases} 1 \\text{ if there is an edge from } v_i \\text{ to } v_j \\\\ 0 \\text{ otherwise} \\end{cases}$$ Or, Adjacency List , \\(|V|\\) linked lists, one per vertex. The list for u holds the names of vertices to which u , has an outgoing edge. Depth first search in undirected Graphs Exploring mazes So, we need to simulate a piece of chalk(to check whether the node has been visited), and a string(to retrace our steps back home). The analogs that we have are, a boolean flag as a chalk, and a stack as a string(in this case its the recursive system stack). Running time, \\(O(|V| + |E|)\\) Connectivity in undirected graphs Identify and assign different integers to the different connected components in a undirected Graph. Previsit and Postvisit orderings Property : For any node \\(u\\) and \\(v\\) , the two intervals \\([pre(u), post(u)]\\) and \\([pre(v), post(v)]\\) are either disjoint or one is contained within the other. Depth-First search in Directed Graphs Types of edges Directed Acyclic Graphs Property : A directed graph has a cycle if and only if its depth-first search reveals a back edge. Property : In a dag, every edge leads to a vertex with a lower \\(post\\) number. Property : Every dag has at least one source and at least one sink. Strongly Connected Components Defining connectivity for directed graphs Two nodes u and v of a directed graph are connected if there is ap path from u to v and a path from v to u . This relation partitions V into disjoint sets that we call strongly connected components .The graph above has five of them. Property : Every directed graph is a dag of its strongly connected components. An efficient algorithm Property 1 : If the \\(explore\\) subroutine is started in a node \\(u\\) , then it will terminate precisely when all nodes reachable from \\(u\\) have been visited. Therefore, if we call explore on a node that lies somewhere in a sink strongly connected component(a strongly connected component that is a sink in the meta-graph), then we will retrieve exactly that component. But, how to find a node that we know for sure lies in a sink strongly connected component. How do we continue once the first component has been discovered. Property 2: The node that receives the highest \\(post\\) number in a depth-first search must lie in a source strongly connected component. which directly follows from, Property 3: If \\(C\\) and \\(C'\\) are strongly connected components, and there is an edge from a node in \\(C\\) to a node in \\(C'\\) , then the highest \\(post\\) number in \\(C\\) is bigger than the highest \\(post\\) number in \\(C'\\) . So, now we can determine whether a particular node lies in a source component of the meta graph. The opposite of what we need. Now, consider the reverse graph. It will have exactly the same strongly connected components as G. So if we find a part of source in the reverse graph, this node will be a part of a sink component in the original graph. Once we find the first strongly connected component a d deleted it from the graph, the next node with the highest post number will be a part of another sink component in G. The resulting algorithm, 1. Run depth-first search on \\(G&#94;R\\) . 2. Run the undirected connected components algorithm on \\(G\\) and during the depth-first search, process the vertices in decreasing order of their post numbers from step 1. Chapter 4: Paths in Graphs Distances The distance between two nodes is the length of the shortest path between them. Breadth first search Dijkstra's algorithm An adaptation of breadth-first search For any edge \\(e = (u,v) \\text{ of } E\\) , replace it by \\(l_e\\) edges of length 1, by adding \\(l_e - 1\\) dummy nodes between \\(u\\) and \\(v\\) . Therefore, we can compute distances in graph by running BFS on the new graph. We can do this by setting an estimated time of arrival for each new node in the frontier. The nodes are then being processed on the basis of earliest time. The right data structure to do this is a priority queue (usually implemented by a heap ). Running time \\(O(|V| + |E|)\\log|V|\\) Which heap is best ? Priority Queue Implementations Array Simplest implementation of a priority queue is as an unordered array of key values for all potential elements. Binary Heap Here elements are stored in a complete binary tree. In addition, a special ordering constraint is enforced: the key of any node of the tree is less than or equal to that of its children , i.e. the root always contains the smallest element. d-ary heap Identical to a binary heap, except that the nodes have d children instead of just two. Shortest paths in the presence of negative edges Note : The presence of a negative cycle means we cannot answer the question of shortest paths in the given graph. Shortest paths in dags if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Summary","title":"Algorithms, Part 3"},{"url":"posts/summary/algorithms-part-4/","text":"Chapter 5: Greedy Algorithms !Thinking Ahead. Minimum spanning trees Property 1 Removing a cycle edge cannot disconnect a graph. The tree with minimum total weight is then known as minimum spanning tree . Formally, Input : An undirected graph \\(G = (V, E)\\) ; edge weights \\(w_e\\) . Output : A tree \\(T = (V, E')\\) , with \\(E' \\subseteq E\\) , that minimizes \\(weight(T) = \\sum_{e \\in E'}w_e\\) . A greedy approach Kruskal's algorithm Repeatedly add the next lightest edge that doesn't produce a cycle. The cut property Cut Property Suppose edges \\(X\\) are part of a minimum spanning tree of \\(G = (V, E)\\) . Pick any subset of nodes S for which \\(X\\) does not cross between \\(S\\) and \\(V-S\\) , and let \\(e\\) be the lightest edge across this partition. Then \\(X \\cup \\{e\\}\\) is part of some \\(MST\\) . Kruskal's algorithm We need to use a data structure for representing disjoint sets, supporting the following operations, makeset(x) : create a singelton set containing just x . find(x) : to which set does x belong. union(x,y) : merge the sets containing x and y . A data structure for disjoint sets Union by rank $$\\underline{procedure\\, makeset(x)}\\\\ \\pi(x) = x \\\\ rank(x) = 0 \\\\ \\, \\\\ \\underline{function\\, find(x)} \\\\ while\\quad x \\neq \\pi(x): x = \\pi(x) \\\\ return\\; x \\\\ \\, \\\\ \\underline{procedure\\, union(x,y)} \\\\ r_x = find(x) \\\\ r_y = find(y) \\\\ if\\quad r_x = r_y:\\, return \\\\ if\\quad rank(r_x) > rank(r_y) : \\\\ \\quad \\pi(r_y) = r_x \\\\ else: \\\\ \\quad \\pi(r_x) = r_y \\\\ \\quad if\\quad rank(r_x) = rank(r_y): rank(r_y) = rank(r_y) + 1 $$ The given structure has the following properties. Property 1 For any \\(x, rank(x) < rank(\\pi(x))\\) . Property 2 Any root node of rank \\(k\\) has at least \\(2&#94;k\\) nodes in its tree. Property 3 If there are \\(n\\) elements overall, there can be at most \\(n/2&#94;k\\) nodes of rank \\(k\\) . Path compression $$ \\\\ \\underline{function\\, find(x)} \\\\ if\\quad x \\neq \\pi(x): \\pi(x) = find(\\pi(x)) \\\\ return\\; \\pi(x) \\\\ $$ During each find , when a series of parent pointers are followed up to the root, we will change all these pointers so that they point directly to the root. This simple alteration results in doing slightly more work per find operation. However, the amortized cost of each operation turns out to be just barely more than \\(O(1)\\) . Prim's algorithm In most general terms, any algorithm working on the following general schema is guaranteed to work. $$X = \\{\\,\\}\\text{ edges picked so far} \\\\ repeat\\; until\\quad |X| = |V|-1 :\\\\ \\quad \\text{pick a set} S \\subset V \\text{ for which X has no edges between S and V - S} \\\\ \\quad let\\; e \\in E \\text{ be the minimum-weight edge between S and V - S} \\\\ X = X \\cup \\{e\\} $$ Alternative to Kruskal's algorithmfor finding Minimum Spanning Trees. This is very similar to Dijkstra, only that the priorities are decided differently. Huffman coding Variable length encoding of symbols, depending on frequency of the particular symbol. Should be prefix-free , i.e. no codeword can be a prefix of another codeword. Any prefix-free encoding can be represented by a full binary tree. The two symbols with the smallest frequencies must be the bottom of the optimal tree. $$\\underline{procedure\\; \\text{Huffman}(f)} \\\\ Input:\\quad \\text{An array $f[1...n]$ of frequencies} \\\\ Output:\\quad \\text{An encoding tree with n leaves} \\\\ \\, \\\\ \\text{let H be a priority queue of integers, ordered by } f \\\\ for\\; i = 1\\, to\\, n:\\; insert(H, i) \\\\ for\\; k=n+1\\, to\\, 2n-1: \\\\ \\quad i = deletemin(H), j = deletemin(H) \\\\ \\quad \\text{create a node numbered k with children i,j} \\\\ \\quad f[k] = f[i] + f[j] \\\\ \\quad insert(H,k) \\\\ $$ Horn formulas Horn formulas are a framework for performing logical reasoning, expressing logical facts and deriving conclusions. Knowledge about variables is represented by two kinds of clauses: 1. Implications, whose left-hand side is an AND of any number of positive literals and whose right-hand side is a single positive literal. These express statements of the form \"if the conditions on the left hold, then the one on the right must also be true.\". For instance, \\((z\\land w)\\implies u\\) might mean \"if the colonel was asleep at 8 pm and the murder took place at 8 pm then the colonel is innocent.\" A degenerate type of implication is the singleton \" \\(\\implies x\\) ,\" meaning simply that x is true : \"the murder definitely occurred in the kitchen\". 2. Pure negative clauses , consisting of an OR of any number of negative literals, as in \\((\\bar u\\lor \\bar v \\lor \\bar y)\\) Given a set of clauses, we need to assign true/false values to the variables that satisfies all the clauses. The is called a satisfying assignment . So we have this given formula for finding a satisfying assignment. $$ \\\\ Input:\\quad \\text{a Horn formula} \\\\ Output:\\quad \\text{a satisfying assignment, if one exists} \\\\ \\, \\\\ \\text{set all variables to false} \\\\ \\, \\\\ \\text{while there is an implication that is not satisfied:} \\\\ \\quad \\text{set the right-hand variable of the implication to true} \\\\ \\, \\\\ \\text{if all pure negative clauses are satisfied: return the assignment} \\\\ \\text{else: return \"formula is not satisfiable\"} \\\\ $$ Set Cover $$ \\text{SET COVER} \\\\ Input: \\text{A set of elements $B$; sets } S_1,...S_m \\subseteq B. \\\\ Output: \\text{A selection of the $S_i$ whose union is $B$} \\\\ Cost: \\text{Number of sets picked.} \\\\ $$ The greedy algorithm will not be optimal, but it will be pretty close to it. Claim , Supppose \\(B\\) contains \\(n\\) elements and that the optimal cover consists of \\(k\\) sets. Then the greedy algorithm will use at most \\(k\\ln n\\) sets. The ratio between the greedy algorithm's solution and the optimal solution varies from input to input but is always less than \\(\\ln n\\) . There are certain inputs for which the ratio is very close to \\(\\ln n\\) . We call this maximum ratio the approximation factor of the greedy algorithm. Chapter 6: Dynamic Programming The sledgehammers of the algorithms class: Dynamic Programming and Linear Programming Shortest paths in dags, revisited The following routine can be used to calculate the shortest path in DAG. $$ \\\\ \\text{initialize all $dist(.)$ values to $\\inf$} \\\\ dist(s) = 0 \\\\ \\text{for each}\\quad v \\in V\\setminus\\{s\\}, \\text{in linearized order}: \\\\ \\quad dist(v) = min{(u,v)\\in E}\\{dist(u) + l(u,v)\\} $$ Dynamic programming is a very powerful algorithmic paradigm in which a problem is solved by identifying a collection of subproblems and tackling them one by one, smallest first, using the answers to subproblems to figure out larger ones, until the whole lot of them is solved. In dynamic programming we are not given a dag, it is implicit . Its nodes are the subproblems we define, and its edges are the dependencies between the subproblems. Longest increasing subsequence The problem can be represented as a DAG, containing all possible transitions; establish a node \\(i\\) for each element \\(a_i\\) , and add directed edges \\((i,j)\\) whenever it is possible for \\(a_i\\) and \\(a_j\\) to be consecutive elements in an increasing subsequence, that is, whenever \\(i < j\\) and \\(a_i < a_j\\) . So, for finding the longest subsequence, our goal is simply to find the longest path in the dag. $$\\\\ for\\quad j = 1, 2,... n: \\\\ \\quad L(j) = 1 + max\\{L(i): (i, j) \\in E\\} \\\\ return\\; max_jL(j) \\\\ $$ , where \\(L(j)\\) is the length of the longest path(the longest increasing subsequence) ending in \\(j\\) . The actual subsequence of nodes can also be determined by some bookkeeping(ex. note down \\(prev(j)\\) ), the next-to-last node on the longest path to j.) Edit distance The minimum number of edits required to convert a string to another string. A dynamic programming solution \\(E(i,j)\\) represents the subproblems of finding match between string \\(x[1...i]\\) and \\(y[1...j]\\) . Our final objective is to compute \\(E(m,n)\\) . For that we need to express \\(E(i,j)\\) in terms of smaller subproblems. Now, \\(E(i,j) = min\\{1 + E(i-1, j), 1 + E(i, j-1), diff(i,j) + E(i-1, j-1)\\}\\) where, \\(diff(i,j)\\) is 0 if \\(x[i] = y[i]\\) , and \\(1\\) otherwise. The above relation forms a table, which can easily be computed in time \\(O(mn)\\) . Knapsack A burglar wants to find out which items to carry, considering the weight and cost where he can carry up to a fixed amount of weight, maximizing the cost of items. Knapsack with repetition \\(K(w) =\\) maximum value achievable with a knapsack of capacity \\(w\\) . \\(K(w) = max_{i:w_i \\leq w}\\{K(w - w_i)+v_i\\}\\) , Algorithm: $$\\\\ K(0) = 0 \\\\ for\\quad w = 1 to\\; W : \\\\ \\quad K(w) = max\\{K(w-w_i) + v_i : w_i < w\\} \\\\ return\\; K(W) \\\\ $$ Knapsack without repetition \\(K(w,j) =\\) maximum value achievable using knapsack of capacity \\(w\\) and items \\(1...j\\) . \\(K(w,j) = max\\{K(w-w_j,j-1) + v_j, K(w, j-1)\\}\\) $$\\\\ \\text{initialize all } K(0,j) = 0 \\text{ and all } K(w,0) = 0 \\\\ for\\quad j=1\\; to\\; n: \\\\ \\quad for\\quad w=1\\; to\\; W : \\\\ \\quad \\quad if\\; w_j > w: K(w,j) = K(w,j-1) \\\\ \\quad \\quad else: K(w,j) = max\\{K(w,j-1), K(w-w_j,j-1)+v_j\\} \\\\ return\\; K(W,n) \\\\ $$ Chain Matrix Multiplication Matrix multiplication is not commutative, but is associative. Which means, \\(A \\times (B \\times C) = (A \\times B) \\times C\\) . Thus, we can compute product of four different matrices in many different ways, Some of the ways are much better(computationally) than others. Shortest Paths Shortest reliable paths Suppose we want the shortest path from \\(s\\) to \\(t\\) that uses at most \\(k\\) edges. Is there a quick way to adapt Dijkstra's algorithm to this new task? Not quite; since the algorithm focuses on the length of each shortest path without remembering the number of hops in the path, which is now a crucial information. In dynamic programming, we can now define, for each vertex \\(v\\) and each integer \\(i < k, dist(v,i)\\) to be the length of the shortest path from \\(s\\) to \\(v\\) that uses \\(i\\) edges. \\(dist(v,i) = min_(u,v)\\in E{dist(u, i-1) + l(u,v)}\\) . All pairs shortest paths Floyd-Warshall algorithm : We want to find the shortest paths between all pairs of vertices. Is there a good subproblem for solving this problem. Yes. We can start with just two starting nodes, and gradually expand the set of possible intermediate nodes. More concretely, number the vertices in \\(V\\) as \\(\\{1...n\\}\\) and let, \\(dist(1,j,k)\\) denote the length of shortest path from \\(i\\) to \\(j\\) in which only nodes \\(\\{1,2...k\\}\\) can be used as intermediates. Initially, \\(dist(1,j,0)\\) is the length of the direct edges between \\(i\\) and \\(j\\) if one exists, and is \\(\\inf\\) otherwise. This, using \\(k\\) gives us a shorter path from \\(i\\) to \\(j\\) if and only if \\(dist(i,k,k-1) + dist(k,j,k-1) < dist(i,j,k-1)\\) , in which case \\(dist(i,j,k)\\) should be updated accordingly. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Summary","title":"Algorithms, Part 4"},{"url":"posts/summary/algorithms-part-1/","text":"Chapter 0: Prologue Books and algorithms Ideas that changed the world. Widespread use of decimal system. Enter Fibonacci 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...., Also, \\(F_n = F_{n-1} + F_{n-2}\\) And, \\(F_n ≈ 2&#94;{0.694n}\\) A naive implementation , with no caching of values.., fib1 Runtime --> \\(T(n) >= F_n\\) , exponential in n Can We Do Better,....? A Polynomial algorithm..., fib2 A loop based algorithm that remembers previous values in an array. Polynomial running time. More Careful Analysis What about addition of numbers Fibonacci values for large n. Adding two n-bit integers take time ~ n. So, \\(fib1 ≈ n.Fn\\) And, \\(fib2 ≈ n&#94;2\\) Big-O notation Right simplification of the analysis. Leave behind the lower order terms. General rules : Multiplicative constants can be omitted. \\(n&#94;a\\) dominates \\(n&#94;b, if a > b\\) Any exponential dominates any polynomial. \\(a&#94;n\\) dominates \\(n&#94;b\\) Likewise, any polynomial dominates any logarithm. \\(n\\) dominates \\(log(n)\\) Exercise 0.4 So now \\(F_n\\) can be computed by calculating \\(X&#94;n\\) where \\(X\\) is the square matrix. So, \\(Fn = O(log n)\\) But then, with careful analysis, Multiplication of large n-bit numbers \\(≈ O(n&#94;2)\\) Chapter 1: Algorithms with Numbers Two ancient problems: Factoring : Given a number N, express it as a product of its prime factors. Hard Primality : Given a number N, determine whether it is a prime. Easy Basic Arithmetic Addition The sum of any three single-digit numbers is at most two digits long. Given two binary numbers x and y, how does our algorithm take to add them? Depends on size of input, number of bits in x and y. Running time \\(= O(n), n =\\) number of bits in the numbers. Is there anything faster? No.. About word length and large numbers Multiplication and Division To multiply x and y , create an array of intermediate sums, each representing the product of x by a single digit if y . These values are appropriately left shifted and added up. Running time = Addition of n numbers of n-bits length = \\((n-1).O(n)\\) = \\(O(n&#94;2)\\) Another fascinating algorithm for multiplication: However, running time = \\(O(n&#94;2)\\) , n = number of bits. Modular Arithmetic $$x \\equiv y \\pmod N \\Leftrightarrow N divides (x - y)$$ Modular arithmetic is a system for dealing with restricted ranges of integers. Another interpretation is that modular arithmetic deals with all the integers, but divides into N equivalence classes, each of the form \\(\\{i+kN:k\\in \\mathbb Z\\}, i \\in [0, N-1]\\) . Modular addition and multiplication To add two numbers x and y modulo N , we start with regular addition. Since x and y are both in the range 0 to N - 1 , their sum is in the range 0 to 2(N-1) . If the sum exceeds N-1 , we merely need to subtract off N to bring it back to required range. So, running time \\(= O(n), n = log N\\) . To multiply two mod N numbers x and y, do regular multiplication, reduce the answer to modulo N. The product can be as large as \\((N-1)&#94;2\\) , at most 2n bits large. Need to compute the remainder using quadratic time division algorithm. Multiplication thus remains quadratic. Division , however is tricky, whenever legal, it can be managed in quadratic time. Modular exponentiation We want to compute \\(x&#94;y \\pmod N\\) . Let n be the size(bits) of x, y and N . As with multiplication, the algorithm will halt after at most n recursive calls, and during each call it multiplies n-bit numbers, for a total running time of \\(O(n&#94;3)\\) . Euclid's algorithm for G.C.D Eculid's rule : If x and y are positive integers with \\(x >= y\\) , then \\(gcd(x, y) = gcd(x \\pmod y, y)\\) . Also, if \\(a > b\\) , then \\(a \\pmod b < a/2\\) . This means after any two consecutive iterations, both arguments are at the very least reduced to half.If they are initially n-bit, base case will be reached in at most 2n recursive calls. And each call involves a quadratic-time division. Running time \\(= O(n&#94;3)\\) . An extension to Euclid's algorithm A small extension to Euclid's algorithm is the key to dividing in the modular world. Modular division x is the multiplicative inverse of a modulo N , if \\(ax == 1 \\pmod N\\) If \\(gcd(a,N) > 1 , \\Rightarrow ax \\neq 1 \\pmod N \\forall x\\) , and therefor a cannot have a multiplicative inverse modulo N . When \\(gcd(a, N) = 1\\) , we say a and N are relatively prime . The extended Euclid's algorithm gives us integers x and y such that \\(ax + Ny = 1\\) , which means \\(ax \\equiv 1 (mod N)\\) . Thus x is a's sought inverse . Modular Division Theorem , a has multiplicative inverse modulo N , if and only if they are relatively prime, and it can be found by running extended Euclid theorem in time \\(O(n&#94;3)\\) . Primality Testing Here the theorem does not say anything about what happens if number is not prime. In fact, for some composite numbers, Pr(Algorithm returns yes when N is not prime) <= 1/2 We can thus choose k different random integers to test for primality testing, reducing Pr to \\(1/2&#94;k\\) . Generating random primes Due to such abundance of prime numbers, prime number generation is easy. Generate a random n-bit number. Check for primality. If not prime, repeat the process. Cryptography Rivest-Shamir-Adelman(RSA) : Blabber about private key and public key systems. Private-key schemes: one time pad and AES RSA Public key cryptography Based heavily upon number theory. Universal Hashing Hash Tables Give a key to any value . Hash function : How to define the mapping between key and value . Families of Hash Functions A family of hash functions with this property is called universal . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Summary","title":"Algorithms, Part 1"},{"url":"posts/summary/algorithms-part-2/","text":"Chapter 2: Divide-and-Conquer Algorithms The divide and conquer strategy solves a problem by 1. Breaking it into sub-problems that are themselves smaller instances of the same type of problem. 2. Recursively solving these problems. 3. Appropriately combining their results. Multiplication For multiplying two n-bit integers x and y. \\(x = \\bbox[5px, border:2px solid black]{ x_L } \\quad \\bbox[5px, border:2px solid black]{ x_R } = 2&#94;{n/2}x_L + x_R\\) \\(y = \\bbox[5px, border:2px solid black]{ y_L } \\quad \\bbox[5px, border:2px solid black]{ y_R } = 2&#94;{n/2}y_L + y_R\\) then, \\(xy = (2&#94;{n/2}x_L + x_R)(2&#94;{n/2}y_L + y_R) = 2&#94;nx_Ly_L + 2&#94;{n/2}(x_Ly_R + x_Ry_L) + x_Ry_R\\) Now, we can compute xy by evaluating the RHS. Addition and multiplication by \\(2&#94;n\\) are linear time. Rest of the 4 multiplications can be done by recursively applying this algorithm. So, \\(T(n) = 4T(n/2) + O(n)\\) . Which results in \\(O(n&#94;2)\\) . By expanding the middle term, we can do with just three calculations, \\(x_Ly_L , x_Ry_R, (x_L+x_R)(y_L+y_R)\\) . since, \\(x_Ly_R + x_Ry_L = (x_L + x_R ) (y_L + y_R ) - x_Ly_L – x_Ry_R\\) . Resulting algorithm would then be \\(T(n) = 3T(n/2) + O(n)\\) , which is \\(O(n&#94;{1.59})\\) Height of the tree \\(= \\log_2 n\\) , since the length of the sub-problem gets halved at every level. Branching factor = 3. So, at any level k we will have \\(3&#94;k\\) to solve each of size \\(n/2&#94;k\\) . Therefore, time spent at level k is, \\(3&#94;k \\times O(\\frac n {2&#94;k}) = (\\frac 3 2)&#94;k \\times O(n)\\) Which is a geometric series. So, the sum then approximates to the last term of the series. That is \\(O(3&#94;{\\log_2 n})\\) , which can be written as \\(O(n&#94;{\\log_2 3})\\) , which is about \\(O(n&#94;{1.59})\\) . Can we do better ??? using Fast Fourier Transforms discussed later. Recurrence relations Consider this recurrence tree. Master's Theorem : If \\(T(n) = aT(\\lceil n/b \\rceil) + O(n&#94;d)\\) for some constants \\(a > 0, b > 1\\) , and \\(d \\ge 0\\) , then $$ T(n) = \\begin{cases} O(n&#94;d), \\text{ if } d > \\log_b a \\\\ O(n&#94;d\\log n), \\text{ if } d = \\log_b a \\\\ O(n&#94;{\\log_b a}), \\text{ if } d < \\log_b a \\end{cases}$$ Mergesort Sort an array by recursively sorting each half and merging the results. Since merge does constant amount of work per recursive call, overall time is $$ T(n) = 2T(n/2) + O(n), \\text{ or } O(n\\log n)$$ Here is an iterative version using a Queue. Medians Median = 50th percentile of a list of numbers. A randomized divide-and-conquer algorithm for selection For any number v , Split S into three categories: elements smaller than v \\((S_L)\\) , equal to v \\((S_v)\\) , greater than v \\((S_R)\\) , then $$ selection(S, k) = \\begin{cases} selection(S_L, k), \\quad\\quad\\quad\\quad\\quad\\quad \\text{if } k \\le |S_L| \\\\ v , \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\text{ if } |S_L| < k \\le |S_L| + |S_v| \\\\ selection(S_R, k - |S_L| - |S_v|), \\text{ if } k > |S_L| + |S_v| \\end{cases}$$ The three sub-lists can be computed in linear time, even in place. How to pick v ? Randomly from S . Matrix multiplication Symbolically, $$ Z_{ij} = \\sum_{k=1}&#94;n X_{ik}Y_{kj}$$ This implies the algorithm to be \\(O(n&#94;3)\\) Enter divide-and-conquer. Divide the matrices X and Y into 4 blocks. $$ X = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}, Y = \\begin{bmatrix}E & F \\\\ G & H\\end{bmatrix} $$ , Then, their product can be expressed as, $$ XY = \\begin{bmatrix}A & B \\\\ C & D\\end{bmatrix} = \\begin{bmatrix}AE+BG & AF+BH \\\\ CE+DG & CF+DH \\end{bmatrix} $$ Total running time can be described as, \\(T(n) = 8T(n/2) + O(n&#94;2), \\text{ which is again } O(n&#94;3)\\) . Turns out you can do this, $$ XY = \\begin{bmatrix}P_5+P_4-P_2+P_6 & P_1+P_2 \\\\ P_3+P_4 & P_1+P_5-P_3-P_7\\end{bmatrix} $$ where, \\(P_1 = A(F-H) \\quad\\quad P_5 = (A+D)(E+H)\\) \\(P_2 = (A+B)H \\quad\\quad P_6 = (B-D)(G+H)\\) \\(P_3 = (C+D)E \\quad\\quad P_7 = (A-C)(E+F)\\) \\(P_4 = D(G-E)\\) The new running time is, \\(T(n) = 7T(n/2) + O(n&#94;2), \\text{which is } O(n&#94;{\\log_2 7}) \\approx O(n&#94;{2.81})\\) Fast Fourier transform Next target, Polynomials . The general solution for polynomial multiplication works in \\(\\theta(d&#94;2)\\) time, where \\(d\\) is the degree of polynomials. An alternative representation of polynomials. Fact : A degree-d polynomial is uniquely characterized by its values at any \\(d+1\\) distinct points . So, we can specify a degree-d polynomial $$A(x) = a_0+a_1x+...+a_dx&#94;d$$ by any one of the following, 1. Its coefficients, \\(a_0, a_1,..., a_d\\) 2. The values \\(A(x_0), A(x_1), ..., A(x_d)\\) Now, taking in consideration the second form, the product has a degree \\(2d\\) , and is completely determined by its values at \\(2d+1\\) points. Since, those values are just products of the two polynomials at the given point. Thus, polynomial multiplication takes linear time in the value representation. Evaluation by divide-and-conquer The Trick : Choose the n points to be selected as positive-negative pairs, then the computations needed to be done overlap a lot. Specifically, if we could then recurse, we would have, $$T(n) = 2T(n/2) + O(n), \\text{which is } O(n\\log n)$$ But, recursing it at the second level and beyond seems impossible. Unless, of-course we use complex numbers. To get positive-negative pairs at subsequent levels, we can use the roots of \\(z&#94;n = 1\\) Which are, \\(1, \\omega, \\omega&#94;2, ... \\omega&#94;{n-1}, \\text{ where, } \\omega = e&#94;{2\\pi i/n}\\) So, if we choose these numbers, at every successive level of recursion we have pairs of positive-negative numbers Interpolation $$\\langle \\text{values} \\rangle = FFT(\\langle \\text{coefficients} \\rangle, \\omega)$$ , and $$\\langle \\text{coefficients} \\rangle = \\frac 1 n FFT(\\langle \\text{values} \\rangle, \\omega&#94;{-1}) $$ Details left,, See Fourier basis A closer look at the fast Fourier Transform The FFT takes as input a vector \\(a = (a_0, ... a_{n-1})\\) and a complex number \\(\\omega\\) whose powers are the complex root of unity. It multiplies this vector with the Matrix \\(M_n(\\omega)\\) , which has \\((j, k)_{th}\\) entry (starting row and column count at zero) \\(\\omega&#94;{jk}\\) . if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Summary","title":"Algorithms, Part 2"},{"url":"posts/summary/elements-of-statistical-learning-part-1/","text":"Chapter 1: Introduction Motivation towards statistical learning and belief in data. What's next. Chapter 2: Overview of Supervised Learning Variable types and terminology Quantitative vs Qualitative output. Regression and Classification Simple approaches : Least Squares and Nearest Neighbors Linear Models and Least Squares \\(\\hat Y = \\hat \\beta_0 + \\sum_{j=1}&#94;pX_j\\hat\\beta_j\\) Least squares by solving normal equations. Nearest Neighbor Methods Voronoi tessellation From Least Squares to Nearest Neighbors Statistical Decision Theory Local Methods in High Dimensions The curse of Dimensionality, Bellman Statistical Models, Supervised Learning and Function Approximation A Statistical Model for the Joint Distribution Pr(X, Y ) Supervised Learning Function Approximation Structured Regression Models Difficulty of the Problem Classes of Restricted Estimators Roughness Penalty and Bayesian Methods regularization Kernel Methods and Local Regression Basis Functions and Dictionary Methods Model Selection and the Bias–Variance Tradeoff Chapter 3: Linear Methods Of Regression Introduction Linear Regression Models and Least Squares Solution from normal form F statistic Example : prostrate cancer The Gauss-Markov Theorem Proof that the Least Squares estimate for the parameters, \\(\\beta\\) has the least variance. Multiple Regression from Simple Univariate Regression Multiple Outputs Subset Selection Best-Subset Selection Forward and Backward-Stepwise Selection Forward-Stagewise Selection Example : Prostrate Cancer (Continued) Shrinkage Methods Ridge Regression : L2 regularization The Lasso : L1 regularization Discussion : Subset Selection, Ridge Regression and the Lasso Least Angle Regression Methods Using Derived Input Directions Principal Components Regression Partial Least Squares Discussion : A Comparison of Selection and Shrinkage Methods Multiple Outcomes Shrinkage and Selection ☠ More on Lasso and Related Path Algorithms ☠ Incremental Forward Stagewise Regression Piecewise-Linear Path Algorithms The Dantzig selector The Grouped Lasso Further Properties of Lasso Pathwise Coordinate Optimization Computational Considerations Fitting is usually done using Cholesky decomposition of matrix \\(X&#94;TX\\) . Chapter 4: Linear Methods of Classification Introduction Linear Regression of an Indicator Matrix Linear Discriminant Analysis Regularized Discriminant Analysis Computations for LDA Reduced-Rank Linear Discriminant Analysis Logistic Regression Fitting Logistic Regression Models Example : South African Heart Disease Quadratic Approximations and Inference \\(L_1\\) Regularized Logistic Regression Logistic Regression or LDA ? Separating Hyperplanes Rosenblatt's Perceptron Learning Algorithm Optimal Separating Hyperplanes ☠ Chapter 5: Basis Expansions and Regularization Introduction Piecewise Polynomials and Splines Natural Cubic Splines Example: South African Heart Disease (Continued) Example: Phoneme Recognition Filtering and Feature Extraction Smoothing Splines Degrees of Freedom and Smoother Matrices Automatic Selection of the Smoothing Parameters Fixing the Degrees of Freedom The Bias–Variance Tradeoff Nonparametric Logistic Regression Multidimensional Splines Regularization and Reproducing Kernel Hilbert Spaces ☠ Spaces of Functions Generated by Kernels Examples of RKHS Penalized Polynomial Regression Gaussian Radial Basis Functions Support Vector Classifiers Wavelet Smoothing ☠ Wavelet Smoothing and the Wavelet Transform Adaptive Wavelet Filtering Chapter 6: Kernel Smoothing Methods One-Dimensional Kernel Smoothers Local Linear Regression Local Polynomial Regression Selecting the Width of the Kernel Local Regression in \\({\\mathbb R}&#94;p\\) Structured Local Regression Models in \\({\\mathbb R}&#94;p\\) Structured Kernels Structured Regression Functions Kernel Density Estimation and Classification Kernel Density Estimation Kernel Density Classification The Naive Bayes Classifier Radial Basis Functions and Kernels Mixture Models for Density Estimation and Classification Computational Considerations if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Summary","title":"Elements Of Statistical Learning, Part 1"},{"url":"posts/summary/introduction-to-philosophy/","text":"These are just some bullet notes from the Coursera course Introduction to philosophy What Is Philosophy Working out the best way to thinking about things. The philosophical question. Philosophy: Difficult, Important and Everywhere Is it 'Fundamental'? No,.. Yes,..? Is it 'Important'? No,.. Yes,.? How Do We Do It? Arguments to your questions Do we have free will? Is there a right way to think about things? How do we know we can get to the right way just by thinking,.? What Is Knowledge? And Do We Have Any? Basic Constituents of Knowledge Propositional vs Ability knowledge Conditions for propositional knowledge Truth Belief Knowledge : Something more than just getting it right. Intuitions About Knowledge The Anti-Luck Intuition The Ability Intuition The Classical Account of Knowledge and the Gettier Problem Classical definition od knowledge Justified, True, Belief Gettier CounterExamples Knowledge cannot be merely justified true belief It might just be a matter of luck that your belief is true, irreverent of your justification. Examples The Stopped Clock!! A Sheep Behind a Sheep Shaped Object. Formula For Creating Gettier Cases - is easy!! Additional Conditions !! @@ NO FALSE LEMMAS : Farley complicated and boring Do We have any knowledge? Radical Skepticism, we don't know nearly as much as we think we do. We have as much knowledge as we take ourselves to have. Brain-in-a-vat Hypothesis : The Matrix How do we know that its not true.? Quite far fetched. Lots of arguments. And similar problems... Still not answerable to a simple solution. Minds, Brains and Computers What is it to have a mind? Theories of Mind Cartesian (or Substance) Dualism, by Descartes Mind is made of fundamentally different substance to the body. The mind is made of immaterial stuff and the body is made of material stuff. Princess Elizabeth of Bohemia & other problems of causation Physical things can only be affected/changed by interaction with other physical things Physicalism , all that exists is physical stuff. Identity Theory of physicalism having a mental state consists in being a particular physical state, mental and physical states are identical. Type identity and Token Identity Type identity offers a strong research prograam, it says that type of physical states are identical to type of mental states. A problem of type identity theory Hilary Putnam, claimed that type-identity would find the identical physical state with mental state of being in pain, for humans It might be something entirely different physical state for octopus for example, but the same mental state of being in pain. Mental states are multiply realizable. Functionalism Chairs can be made of many different things, shapes, sizes, look completely different But what makes them identifiable as chairs is the job that they do. Mind as a Computer With Functionalism, it has become very popular to think about the mind as analogous to a computer. It tries to argue that, like computers, minds are information processing units that take information of some kind and turn it into information of other kind. Turing Machines Identify the between machine and man. When the machine is able to fool the interrogator, then the computer has reached the level of functional complexity required to have a mind. Objections , A machine with a huge database with answers to all the questions. Would we call it a 'thinking' machine.? There can be beings that cannot persuade the questioner that they are human, but who we nevertheless want to count as minded. The Turing test relies on language, and a very narrow criteria for minds. John Searle's Chinese Room You get a symbol that you dont understand, you have a code book that tells what other symbol to respond with when you see a symbol. On the other hand, there is a person who is conversing with you in Chinese. But you don't even know that you are in a communicative act. Computers work by processing symbols. Symbols have syntactic and semantic properties. Computers are manipulating machines, more importantly, they are only aware of the syntactic properties of the symbol. We program the computer to operate on the syntactic structure of the symbol. The problem is that the computer does not 'know' the semantic content about the symbols that it is manipulating. If our mind is indeed a machine, where is the 'programmer' who deciphers meanings of symbols that our mind process? Representation is a three way relation. X represents Y to Z; the beer-mug represents my position on field to my friends. However, in case of the mind this neural activity represents a dog to ??? Morality: Objective, Relative or Emotive The status of morality Here we ask the question, what are we doing when we make moral judgments. The Questions Are they sorts of judgments that can be true or false - or are they mere opinions? If they are true /false, what makes them true /false? If they are true, are they objectively true? Objectivism Our moral judgments are the sorts of things that can be true or false, and what makes them true or false are facts that are generally independent of what are or what cultural groups we belong to - they are objective moral facts. Objection : How do you account for moral disputes between two people then? Relativism Our moral judgments are indeed true or false, but they're only true or false relative to something that can vary between people. Objections : How do you explain or keep track of moral progress then? Subjectivism , a form of relativism Out moral judgments are indeed true or false, but they're only true or false relative to the subjective feelings of the person who makes them. \"X is bad\" = \"I dislike X\" Cultural Relativism , a form of relativism Our moral judgments are indeed true or false, but they are only true or false relative to the culture of the person who makes them. \"X is bad\" = \"X is disapproved of in my culture\" Emotivism Moral judgments are neither objectively true/false or relatively true/false. They're direct expressions of our emotive reactions. Objections : How do you account for the moral decisions that we make and conclusions we arrive at based on our cognitive abilities. Should you believe what you hear Testimony and Miracles The Enlightenment : intellectual autonomy Hume : naturalistic philosophy Never believe that a miracle has occurred, on the basis of a testimony What is testimony? Any situation in which you believe something on the basis of what someone else asserts, either verbally or in writing. A lot of what we believe about the world is based on the testimony of other people. Hume assumes that you should only trust testimony when you have evidence that the testifier is likely to be right. Evidentialism : A wise man,... proportions his belief to the evidence. On Miracle, A miracle is a violation of the laws of nature. Thomas Ried argued, that trusting testimony is analogous to trusting your senses. we don't only trust our senses when we have evidence that they're likely to be right. Kant, Enlightenment is man's emergence from his self-incurred immaturity. Immaturity is the inability to use one's own understanding without the guidance of another. The motto of enlightenment is therefore : Have courage to use your own understanding. Intellectual autonomy : Think for yourself. Is your beliefs are based merely on testimony, they will not amount to knowledge. Are Scientific Theories true Time Travel and Philosophy Reading","tags":"Summary","title":"Introduction To Philosophy"},{"url":"posts/summary/notes-on-opengl/","text":"Transformations Translation : (x, y, z) $$ \\begin{bmatrix} 1 & 0 & 0 & x \\\\ 0 & 1 & 0 & y \\\\ 0 & 0 & 1 & z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$ Scale : (x, y, z) $$ \\begin{bmatrix} x & 0 & 0 & 0 \\\\ 0 & y & 0 & 0 \\\\ 0 & 0 & z & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$ Rotation x : $$ \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & cos(\\theta) & -sin(\\theta) & 0 \\\\ 0 & sin(theta) & cos(theta) & z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$ Rotation y : $$ \\begin{bmatrix} cos(\\theta) & 0 & sin(\\theta) & 0 \\\\ 0 & 1 & 0 & 0 \\\\ -sin(\\theta) & 0 & cos(theta) & z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$ Rotation z : $$ \\begin{bmatrix} cos(\\theta) & -sin(\\theta) & 0 & 0 \\\\ sin(theta) & cos(theta) & 0 & 0 \\\\ 0 & 0 & 1 & z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} $$ Vertex Specifications Specify vertex data. Create buffer Data: const float vertexPositions[] = {...} Initialize vertex Buffer: glGenBuffers(1, &posiionBufferObject) glbindbuffer(GL_ARRAY_BUFFER, positionbufferObject) glbufferData(GL_ARRAY_BUFFER, sizeof(vertexpositions), vertexpositions, BufferobjectUsageHint) BufferobjectUsagehint: GL__STATIC_DRAW : Static data, only intend to set it once GL_STREAM_DRAW : Dynamic data, intend to update it constantly, generally once per frame glbindbuffer(0) Specify vertex data: glbindbuffer(GL_ARRAY_BUFFER, positionbufferobject) glenablevertexattribarray(0) glvertexattribarray(0, 4, GL_FLOAT, GL_FALSE, 0, 0) Display: gldrawarrays(GL_TRIANGLES, 0, 3) glbufferSubData() : Change buffer data value Vertex Array Objects: OpenGL Objects that store all of the state needed to make one or more draw calls. This includes attribute array setup information from glVertexAttribArray , buffer objects used for attribute arrays, and GL_ELEMENT_ARRAY_BUFFER binding, which is a buffer object that stores the index arrays, if needed Pipeline Vertex Specification Vertex Proocessing and Vertex Shader Culling Rasterization Fragment Processing if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"left\", indent = \"0em\", linebreak = \"true\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Summary","title":"Notes On Opengl"},{"url":"posts/robotics/kuka-kinematics/","text":"https://github.com/HbirdJ/RoboND-Kinematics-Project https://github.com/NitishPuri/RoboND-Kinematics-Project/blob/master/writeup.md","tags":"robotics","title":"Kuka Kinematics"},{"url":"posts/gallery/sketch_2/","text":"Some recent sketches, Part 2","tags":"gallery","title":"sketch_2"},{"url":"posts/meta/blogging-like-a-hacker/","text":"Edit: After the blog post I have moved to Pelican , mostly because of my affinity for python. ❤️ So, finally I was able to settle up for a blog after months of procrastination with multiple frameworks and platforms. I had earlier tried to use Tumblr . But, I wanted to be able to explore different areas, like sharing my views on technology that I am currently interested in, music that I am currently digging in, some random art that I create and so on. Tumblr turned out to be a great platform for sharing my art, but for the same reasons did not look like a place where I could write about deep neural networks, natural language and computer vision. Though there are some blogs there that do just that . Then I tried Wordpress, but was never able to settle down on something satisfying and never actually pushed anything. It just had too much to fiddle with, so many options, so many plugins. :confused: This was very difficult for someone like me who opens 20 new browser tabs reading a single post and then bookmarks them all for reading later.:innocent: Then I spend two weeks doing Django tutorials . It was an interesting experience as this was my first time dealing with Python and web frameworks as well. but, in the end, I realized that these are just as many knobs, even if I keep the features minimal, it was too much work for me as I did not wanted to spend so much time building web technology(i would rather complete Skyrim). :grin: Then, after abandoning my quest for several months, I came across this awesome article about writing programming blogs, and how it is important for being a part of the technical community today. And, naturally, I had another 25 tabs open looking for advice on starting a tech blog and most feasible platforms to do so today. There I found this post. It talks about using jekyll with github pages as a blogging platform. Though I already \"knew\" about it, but I only thought of as a cleaner way of viewing your README files written in markdown. As I read through that article, it was clear to me that I would be able to settle in with this. It was simple to setup, free and simple to host, works with markdown which everybody uses, no necessary knobs to fiddle with databases and hosting providers. It was a great resource as it also listed out other links that I could follow to customize as much as I wanted. So, I opened up another browser window and started searching for some good themes, which brought me back to the themes originally suggested here . So, this website was created using the theme lanyon , which derives from poole . Jekyll, Poole and Lanyon are all characters from The Strange Case of Dr. Jekyll and Mr. Hyde . I started with forking the theme to my repo, following this article. However, before actually posting anything I had to setup up a local environment for jekyll also discussed in the article. Customizations Added support for Disqus comments. Added Google Analytics support. Changed some icons. Added sections for Timeline and resume Added sections for Art gallery.","tags":"Meta","title":"Blogging like a hacker"},{"url":"posts/gallery/fractals_1/","text":"Some fun with fractals","tags":"gallery","title":"fractals_1"},{"url":"posts/gallery/sketch_1/","text":"Some recent sketches, Part 1","tags":"gallery","title":"sketch_1"},{"url":"posts/gallery/misc/","text":"Some misc. cutouts and experiments.","tags":"gallery","title":"misc"},{"url":"posts/gallery/monochrome/","text":"Some black and white stuff, with colors","tags":"gallery","title":"monochrome"},{"url":"posts/gallery/in_making_3/","text":"Some early creations, Part 3","tags":"gallery","title":"in_making_3"},{"url":"posts/gallery/in_making_2/","text":"Some early creations, Part 2","tags":"gallery","title":"in_making_2"},{"url":"posts/gallery/in_making_1/","text":"Some early creations, Part 1","tags":"gallery","title":"in_making_1"}]}